{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/cactus/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":1,"renderable":1},{"_id":"themes/cactus/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/cactus/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":1,"renderable":1},{"_id":"themes/cactus/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/cactus/source/css/rtl.styl","path":"css/rtl.styl","modified":1,"renderable":1},{"_id":"themes/cactus/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/cactus/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","path":"lib/vazir-font/Vazir-Black.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","path":"lib/vazir-font/Vazir-Black.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","path":"lib/vazir-font/Vazir-Bold.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","path":"lib/vazir-font/Vazir-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","path":"lib/vazir-font/Vazir-Light.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","path":"lib/vazir-font/Vazir-Light.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","path":"lib/vazir-font/Vazir-Medium.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","path":"lib/vazir-font/Vazir-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff","path":"lib/vazir-font/Vazir.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","path":"lib/vazir-font/Vazir-Thin.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","path":"lib/vazir-font/Vazir-Thin.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","path":"lib/vazir-font/font-face.css","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff2","path":"lib/vazir-font/Vazir.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/images/logo.png","path":"images/logo.png","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","path":"lib/vazir-font/Vazir-Black.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","path":"lib/vazir-font/Vazir-Bold.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","path":"lib/vazir-font/Vazir-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","path":"lib/vazir-font/Vazir-Black.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","path":"lib/vazir-font/Vazir-Light.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","path":"lib/vazir-font/Vazir-Medium.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","path":"lib/vazir-font/Vazir-Light.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","path":"lib/vazir-font/Vazir-Medium.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","path":"lib/vazir-font/Vazir-Thin.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.eot","path":"lib/vazir-font/Vazir.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","path":"lib/vazir-font/Vazir-Thin.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.ttf","path":"lib/vazir-font/Vazir.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.eot","path":"lib/font-awesome/webfonts/fa-regular-400.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff","path":"lib/font-awesome/webfonts/fa-regular-400.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","path":"lib/font-awesome/webfonts/fa-regular-400.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","path":"lib/justified-gallery/css/justifiedGallery.min.css","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/js/jquery.justifiedGallery.min.js","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff","path":"lib/font-awesome/webfonts/fa-brands-400.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.eot","path":"lib/font-awesome/webfonts/fa-brands-400.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","path":"lib/font-awesome/webfonts/fa-brands-400.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff","path":"lib/font-awesome/webfonts/fa-solid-900.woff","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.svg","path":"lib/font-awesome/webfonts/fa-regular-400.svg","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.eot","path":"lib/font-awesome/webfonts/fa-solid-900.eot","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGM-BoldItalic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","path":"lib/font-awesome/webfonts/fa-solid-900.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGL-BoldItalic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGS-BoldItalic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","path":"lib/meslo-LG/MesloLGM-Italic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","path":"lib/meslo-LG/MesloLGS-Italic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","path":"lib/meslo-LG/MesloLGS-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","path":"lib/meslo-LG/MesloLGL-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","path":"lib/meslo-LG/MesloLGL-Italic.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","path":"lib/meslo-LG/MesloLGM-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","path":"lib/meslo-LG/MesloLGM-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","path":"lib/meslo-LG/MesloLGS-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.svg","path":"lib/font-awesome/webfonts/fa-solid-900.svg","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","path":"lib/meslo-LG/MesloLGL-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.svg","path":"lib/font-awesome/webfonts/fa-brands-400.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/cactus/.gitignore","hash":"8d1ea5895c15244978ee32f4ef280124512d995a","modified":1559614447414},{"_id":"themes/cactus/.jshintrc","hash":"d6ee46102ed2ff00acb555557f47241b701e99a4","modified":1559614447415},{"_id":"themes/cactus/.stylintrc","hash":"8bbbee5eae70fbe7dd53c9ce43d244b47d58385b","modified":1559614447415},{"_id":"themes/cactus/LICENSE","hash":"06806c75801c9ae870a1b47d588ef8e00d3c7c94","modified":1559614447415},{"_id":"themes/cactus/README.md","hash":"9d2b515c089758f81db80c062cf53b8091006358","modified":1559614447416},{"_id":"themes/cactus/_config.yml","hash":"d29d4d7aed1b6333f012f4798b07a53fd63898b1","modified":1559823798521},{"_id":"themes/cactus/package.json","hash":"645ae67fde94dd1b78c4f6fdf3e91f3baf7b155c","modified":1559614447439},{"_id":"themes/cactus/gulpfile.js","hash":"367c889c599c6595b9a2df0bbdcc1f8b1181b992","modified":1559614447417},{"_id":"source/_data/projects.json","hash":"d36878049cbf7ea240019001582ae248d9972757","modified":1559610378973},{"_id":"source/_posts/JavaUtilsAPI.md","hash":"fc9d3174f4b69a97b6a3b1e606ca851b6f1b35cc","modified":1563007705886},{"_id":"source/_posts/FluemToKafkaBaseOnDifferntTopic.md","hash":"f00bc085048777f75ed7a7fa48b697b6a217a45f","modified":1559610378973},{"_id":"source/_posts/KMP.md","hash":"e85aeaee15d139cdce9be4486544f81b429e2fab","modified":1559610378974},{"_id":"source/_posts/Graduation-sentiment.md","hash":"1cf7936c92db6eebca672e9b5243f4d9d9968fb8","modified":1559823378566},{"_id":"source/_posts/Minions-Visualization-first.md","hash":"8bd6d400a981df3ac94d6aa379a516c979430355","modified":1559610378975},{"_id":"source/_posts/Kadane's-Algorithm.md","hash":"ef394d643a48735ab0d6d2a6c2fa599743dd41ff","modified":1559610378975},{"_id":"source/_posts/StudyHBaseJavaAPI.md","hash":"37ba6bbebb154575a9234ab3d1748bfe45cabb04","modified":1559610378976},{"_id":"source/_posts/axioscorsquestions.md","hash":"060f6275f03c6761d05946c12294c45f0232338e","modified":1559610378976},{"_id":"source/_posts/MyLeetCodeSummarize.md","hash":"ac623ccb4e7edc95d7ff032e024edd0ce1266c81","modified":1559610378976},{"_id":"source/_posts/graduation-letter-from-Hushi.md","hash":"99688a9681319bbda50ca874ee497d93515c1a25","modified":1561689024930},{"_id":"source/_posts/frontendstudy.md","hash":"e3792c7d25d8e1c58681b7618069362127aca1ac","modified":1559610378977},{"_id":"source/_posts/graduationDesignReferences.md","hash":"6f3b63b32c502e70bb5cb782d873427323d0dd06","modified":1559610378979},{"_id":"source/_posts/minions-frontend-overview.md","hash":"f4f24a9a3a3ca36cf1445e81652fdffe53b441f9","modified":1559610378979},{"_id":"source/_posts/minionsUsage.md","hash":"028c4f7f8aea90498fa3da3a93c73b6db1c1e6b4","modified":1559610378980},{"_id":"source/_posts/mock-accesslog-python.md","hash":"abaa7f456123c350b9bd4f774a86da2caca9f60e","modified":1559610378980},{"_id":"source/_posts/websitesafari.md","hash":"b1e27b57c5defbce190c248de471f83f31b88f99","modified":1559610378985},{"_id":"source/_posts/tanslate-Resilient-Distributed-Datasets-A-Fault-Tolerant-Abstraction–for-In-Memory-Cluster-Computing.md","hash":"6f1a187929d1c3034f1d89f778382c20ef986472","modified":1559610378981},{"_id":"source/_posts/user-action-logger-design.md","hash":"da569c4ea83f0976d1b51f9f9bcfaabfc13949ee","modified":1559610378984},{"_id":"source/_posts/transfer-scaling-webapps-for-newbs-and-non-techies.md","hash":"882a02dcecdb9a14e5577c350740d89b71f9d8e6","modified":1559610378983},{"_id":"source/_posts/weekly-report-190414.md","hash":"826ba4324a1ff153414b1265bf8cb4e604db335f","modified":1559610378985},{"_id":"source/_posts/weeklyReport190330.md","hash":"e8d1e64267c363bbff21a7908d79412ae70cec12","modified":1559610378987},{"_id":"source/about/index.md","hash":"597ad2a202be7cd33cf1cd48320250484ed33819","modified":1559610378988},{"_id":"source/_posts/weekly-report-190420.md","hash":"9f169d4f57ab16cd50b1184a61ab6be62f805f82","modified":1559610378985},{"_id":"source/categories/index.md","hash":"72b3399bc1d24e9d945eb449ba978c86d9095320","modified":1559610378989},{"_id":"source/_posts/weeklyReport190316.md","hash":"f7407e5831e481299b4091bfe527c73d2cd6284f","modified":1559610378986},{"_id":"source/_posts/微信小程序学习意见.md","hash":"26a8b899af5af603f8dfb5202cc167f6f2a57aee","modified":1559610378988},{"_id":"source/_posts/weeklyReport190323.md","hash":"82bab43ab0c4e322ddd31d6b7494c173f2948e27","modified":1559610378986},{"_id":"source/tags/index.md","hash":"01694777d3198c16f19165bea6ab04a9366e0a56","modified":1559610378990},{"_id":"themes/cactus/.git/FETCH_HEAD","hash":"bbf03267a91b13972def4fc8bdbf5fdd4465b6f2","modified":1559618654574},{"_id":"themes/cactus/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1559614447391},{"_id":"source/search/index.md","hash":"4c1d5a2459304f344cb613fbe665dd3deb8523ac","modified":1559610378989},{"_id":"themes/cactus/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1559614434161},{"_id":"themes/cactus/.git/config","hash":"bbfb2d2962522ab703a9b119c7306ce589ec6438","modified":1559614447400},{"_id":"themes/cactus/.git/index","hash":"1c4fad86e44a3ed595594ddb7a3dfe4b2862a079","modified":1559622274418},{"_id":"themes/cactus/.git/packed-refs","hash":"a8a966a850f6a0c813f222ac67730e7f0977336f","modified":1559614447384},{"_id":"themes/cactus/scripts/merge-configs.js","hash":"98b79403945fa7f8dc177e6da283681677d02567","modified":1559614447439},{"_id":"themes/cactus/scripts/meta.js","hash":"6b05bc75ab2478a1f01deab46919164895eb1f6d","modified":1559614447440},{"_id":"themes/cactus/scripts/thumbnail.js","hash":"3d841aa267026703a5cc29f4ffbb613f0a2bbd45","modified":1559614447441},{"_id":"themes/cactus/scripts/page_title.js","hash":"03b5c8b6215f0b305327e8d9b5d49add83b4b95f","modified":1559614447440},{"_id":"themes/cactus/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1559614447418},{"_id":"themes/cactus/languages/ca.yml","hash":"6f9897fe8c7b7bf23be7fa58c530602217aea898","modified":1559614447418},{"_id":"themes/cactus/languages/es.yml","hash":"56ddfd9081641c45272bdc2df750e41ab8b600bf","modified":1559614447420},{"_id":"themes/cactus/languages/fr.yml","hash":"7e8269d0c2ca031b7c037168c9c3d45ec83b1d0e","modified":1559614447421},{"_id":"themes/cactus/languages/fa.yml","hash":"05fb8dad765af2b9eec32582f5d277eb2735ffc3","modified":1559614447421},{"_id":"themes/cactus/languages/en.yml","hash":"c723e2e5bf39a009d5553c50986cc1555d22bc49","modified":1559614447419},{"_id":"themes/cactus/languages/it.yml","hash":"501bfc9aed86964ee75354a805378740a02af799","modified":1559614447422},{"_id":"themes/cactus/languages/pt-br.yml","hash":"417d25e3dae260a2503ec6a8f1b590fa00534b14","modified":1559614447423},{"_id":"themes/cactus/languages/nl.yml","hash":"4c36921322e782775ccff9aec78d4261153cf5e5","modified":1559614447422},{"_id":"themes/cactus/languages/pl.yml","hash":"3677b782edd98986bb82568c6ff6c8b77140b75a","modified":1559614447423},{"_id":"themes/cactus/languages/vi.yml","hash":"1a1dfc60e952c3429c038f69a9e0ca2057784fca","modified":1559614447425},{"_id":"themes/cactus/languages/ru.yml","hash":"47fb1672f36d5adadf2f6c3a5c4d1714f5bd563b","modified":1559614447424},{"_id":"themes/cactus/languages/zh-CN.yml","hash":"353e02417dcc22d0574de47a7bb13a84d0375652","modified":1559614447427},{"_id":"themes/cactus/languages/zh-TW.yml","hash":"d02ac645f87c5841ff35499818654df85bd872f5","modified":1559614447427},{"_id":"themes/cactus/languages/tr.yml","hash":"7971cbff7f90ffe1dc40402be6332890be1ed2c7","modified":1559614447425},{"_id":"themes/cactus/layout/index.ejs","hash":"2dc13298bb00497c575b1b819c5024f2eaf5923c","modified":1559614447437},{"_id":"themes/cactus/layout/archive.ejs","hash":"3009339c4258c63943a14a61481dcb358b63e98f","modified":1559614447436},{"_id":"themes/cactus/layout/layout.ejs","hash":"e14f83a23128ce0d4208f04f419363fd7d0cab18","modified":1559614447437},{"_id":"source/_posts/graduation-design-paper.md","hash":"0986c0da8f450b540d70913d1dba08b801eef22d","modified":1559610378978},{"_id":"themes/cactus/layout/post.ejs","hash":"0d6d97deb5c0820fdc82a0b4caed4f64015f1fd8","modified":1559614447439},{"_id":"themes/cactus/layout/page.ejs","hash":"1700d4f434170e1eeb0e2a9470fc3c0732b2ba7e","modified":1559614447438},{"_id":"themes/cactus/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1559614434163},{"_id":"themes/cactus/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1559614434177},{"_id":"themes/cactus/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1559614434164},{"_id":"themes/cactus/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1559614434166},{"_id":"themes/cactus/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1559614434165},{"_id":"themes/cactus/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1559614434170},{"_id":"themes/cactus/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1559614434167},{"_id":"themes/cactus/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1559614434169},{"_id":"themes/cactus/.git/logs/HEAD","hash":"7ec96918e0c3c8fa69f92e5c8e97036cdf53b0c1","modified":1559614447395},{"_id":"themes/cactus/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1559614434173},{"_id":"themes/cactus/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1559614434172},{"_id":"themes/cactus/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1559614434175},{"_id":"themes/cactus/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1559614434174},{"_id":"themes/cactus/source/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1559614447489},{"_id":"themes/cactus/source/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1559614447490},{"_id":"themes/cactus/source/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1559614447489},{"_id":"themes/cactus/layout/_partial/comments.ejs","hash":"fc4bc4421f2ed388b30fd3e63cba30f66c0df331","modified":1559614447427},{"_id":"themes/cactus/layout/_partial/pagination.ejs","hash":"f561e68e9dd10ae1cf2e79e8425f7e9b86ed723e","modified":1559614447430},{"_id":"themes/cactus/layout/_partial/head.ejs","hash":"63ebdd4d96e96e48cdde724b1099710f4ed2af79","modified":1559614447428},{"_id":"themes/cactus/layout/_partial/scripts.ejs","hash":"df66c98462f3de9691c820ecc442851ad121c52e","modified":1559614447435},{"_id":"themes/cactus/layout/_partial/header.ejs","hash":"4ebbfc58eac02acd2940390e7b4e52bda4e1d402","modified":1559614447430},{"_id":"themes/cactus/layout/_partial/footer.ejs","hash":"c5e082095facfb4de2944cdb11f44c8a4f68f2e5","modified":1559614447428},{"_id":"themes/cactus/layout/_partial/styles.ejs","hash":"b7cf29a0cbe44d93766f89113ba3cfd832e75dfb","modified":1559614447435},{"_id":"themes/cactus/layout/_partial/search.ejs","hash":"12dba799ae5736dfbf1548e1923bbb7447de8a6d","modified":1559614447435},{"_id":"themes/cactus/source/css/_fonts.styl","hash":"5c1727836f5d6076a65cf91dbbeaba8d16b5132b","modified":1559614447445},{"_id":"themes/cactus/source/css/_mixins.styl","hash":"37093d2a554283ab652b59e5e0ff9b542edced0f","modified":1559614447480},{"_id":"themes/cactus/source/css/_extend.styl","hash":"739b6942438fec016bf489d9729382735927e3f0","modified":1559614447444},{"_id":"themes/cactus/source/css/style.styl","hash":"e60336c0af59b0f754746d9d20e0956c14f141c3","modified":1559614447488},{"_id":"themes/cactus/source/css/_variables.styl","hash":"18f57fe9dada74c458ab3e3e7ba829ea535e75e7","modified":1559614447487},{"_id":"themes/cactus/source/css/_util.styl","hash":"1c7bed7376f04d7b77d817e132aee08cf65e6395","modified":1559614447486},{"_id":"themes/cactus/source/css/rtl.styl","hash":"ff61ec57dada4914e8cfba84ddecd04a1b333638","modified":1559614447487},{"_id":"themes/cactus/source/js/main.js","hash":"bad144490654ac2518d15e82f426ecbd66b22fd5","modified":1559614447494},{"_id":"themes/cactus/source/js/search.js","hash":"e17274ed3347055bf6432a712d880ec68411944f","modified":1559614447494},{"_id":"themes/cactus/.git/refs/heads/master","hash":"f1f9cd45fa532645d329694006bf951a8d289f82","modified":1559614447394},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","hash":"37443d0040f0d7af381c955e4c15919a15d0349e","modified":1559614447657},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","hash":"0a257c8b60e0f20802c1dc8daeed2d3cb0d44f17","modified":1559614447659},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","hash":"df15fd1e74b6f4a50bea57e2b44d9627f38495b5","modified":1559614447667},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","hash":"62447a951d48b21c4696ae72df4bc4adef636e26","modified":1559614447668},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","hash":"ef07a250766fea840c1049e67c0405d9216ee0a8","modified":1559614447682},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","hash":"32ae5c0d1d5943c8bb8e0f6ab07c3269c6f8b8a8","modified":1559614447680},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","hash":"f5653059b2a5929516e4aab02329a978600b9b67","modified":1559614447692},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","hash":"668400ae92700965f03f2371faaee0ab8c8347c3","modified":1559614447693},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff","hash":"bbee70033f0f5882e9869e417b69c6a38f56f187","modified":1559614447716},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","hash":"ad4d46a99a1daf6353c86c79ac3a2b030213859c","modified":1559614447703},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","hash":"c3be79b553ec394db71268d604b1d29183b867dc","modified":1559614447705},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","hash":"b75063e015641d22130bb8be3f176d7c20533aad","modified":1559614447719},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff2","hash":"30ce165216db078951a690a6ad665b9b78f5dd81","modified":1559614447717},{"_id":"themes/cactus/layout/_partial/post/category.ejs","hash":"75567a2de037857cb3e4c9fc5fc87094a23fb119","modified":1559614447431},{"_id":"themes/cactus/layout/_partial/post/actions_mobile.ejs","hash":"c8d06799755147a64cdd0cb2728d3ce46a044dae","modified":1559614447431},{"_id":"themes/cactus/layout/_partial/post/actions_desktop.ejs","hash":"684ab77be954f73853ee9407567b4c5c0ef6af74","modified":1559614447431},{"_id":"themes/cactus/layout/_partial/post/title.ejs","hash":"67da1757316408a72393e6d2aab4ec635e1d30c1","modified":1559614447434},{"_id":"themes/cactus/layout/_partial/post/tag.ejs","hash":"98fe03d5a8835dbc22869d519cc5b5f48808b1e5","modified":1559614447433},{"_id":"themes/cactus/layout/_partial/post/gallery.ejs","hash":"725ab823c2fb4a5e444eca3afa617d88f305ef71","modified":1559614447432},{"_id":"themes/cactus/layout/_partial/post/date.ejs","hash":"bb624f77d29594b8d3a7dd82f1aee6eead6b9b61","modified":1559614447432},{"_id":"themes/cactus/source/css/_colors/classic.styl","hash":"750cae11fd67684c0b9f73892e9b4dc0b18e6e59","modified":1559614447442},{"_id":"themes/cactus/layout/_partial/post/share.ejs","hash":"062bdcea5efc37cd7cd3478e4ff0a97db6be3fb1","modified":1559614447433},{"_id":"themes/cactus/source/css/_colors/light.styl","hash":"9d6e813e1612dd51f9e41038376348c561bd33bd","modified":1559614447443},{"_id":"themes/cactus/source/css/_colors/dark.styl","hash":"2c949ae926ae73ef5e5e7b7ebe8ba173d6afd234","modified":1559614447442},{"_id":"themes/cactus/source/css/_highlight/androidstudio.styl","hash":"4d67bdab6cc9c614486ca42f98199a04d053e7f0","modified":1559614447446},{"_id":"themes/cactus/source/css/_colors/white.styl","hash":"6aef2765112d3daa32fb770dc7c5ea478c7baf3d","modified":1559614447444},{"_id":"themes/cactus/source/css/_highlight/agate.styl","hash":"fc289ba8f47ead6331ec3a51533cfa93251c5634","modified":1559614447445},{"_id":"themes/cactus/source/css/_highlight/arduino-light.styl","hash":"591962bfc758a521b4cb907750c19a1a2423b4d5","modified":1559614447446},{"_id":"themes/cactus/source/css/_highlight/ascetic.styl","hash":"ca087a3c70998c7ac6b0b42d5cf7a653b8707591","modified":1559614447448},{"_id":"themes/cactus/source/css/_highlight/arta.styl","hash":"262167aaebcf28de7f85af7ac77a76fa1fa284f7","modified":1559614447447},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-dark.styl","hash":"7e83c7f2acaaaa98864660afe2794745c36c8e51","modified":1559614447448},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-dark.styl","hash":"68584ed0e99c7d0e49ef8a2e67cd4dcdad359de4","modified":1559614447449},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-light.styl","hash":"f47de0b9d66617728f68096ed48371dd6bb9e67a","modified":1559614447449},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-light.styl","hash":"657fe215931fd06e21b56374df699a94890f7ab4","modified":1559614447450},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-light.styl","hash":"2b416a0567a53aa0fa8898b196ddd44315c1a5f3","modified":1559614447451},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-dark.styl","hash":"1cecd13e0d6b24042ff86372f0596c1441bb834a","modified":1559614447450},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-dark.styl","hash":"a741eba35cdfe2cfd67dfbf109655f253d6b4795","modified":1559614447451},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-dark.styl","hash":"f186b357dcebded89b7bcc77389b2cff76533d72","modified":1559614447452},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-light.styl","hash":"8d7c7242974aa2454fa792c5d7a47c5f9632355a","modified":1559614447452},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-light.styl","hash":"c1db353e8613607580d40b12ddc162d029560576","modified":1559614447453},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-dark.styl","hash":"802979cea895a0a384645cb30a43de9572cb0e3f","modified":1559614447453},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-light.styl","hash":"96181544eeadc5b0749229f11607e7c01f81e078","modified":1559614447455},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-light.styl","hash":"8659eaae6a0c2e00b4b9199803e50adf4ff0128d","modified":1559614447454},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-dark.styl","hash":"0d51ddc580ccb0a291271fa9632bc91dab632df6","modified":1559614447454},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-dark.styl","hash":"2f008271299042f2443bca98c9bcadbc8c45e837","modified":1559614447456},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-light.styl","hash":"8a5207a0c30262a0bf5e1a41411a306f7a89a7e7","modified":1559614447456},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-light.styl","hash":"08e2df313c272d5c70c93e713639663c168180d0","modified":1559614447457},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-dark.styl","hash":"bbad7a9512b4873294e73ce806e36e43973e6ed8","modified":1559614447455},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"18dcb00ab9c62eb810d492047214331c51bb654f","modified":1559614447457},{"_id":"themes/cactus/source/css/_highlight/brown-paper.styl","hash":"d14846542a26b75fd26df5b5df6c5493733f73f6","modified":1559614447458},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"add3d88c9d12567dcfae7a8e49984d119fc72227","modified":1559614447458},{"_id":"themes/cactus/source/css/_highlight/codepen-embed.styl","hash":"1de45e603e2c71c7f6b0c1372a3ba00b1bc153a8","modified":1559614447459},{"_id":"themes/cactus/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1559614447459},{"_id":"themes/cactus/source/css/_highlight/dark.styl","hash":"98d7884806838a0b46132d759d60ac27c0c4bd9c","modified":1559614447460},{"_id":"themes/cactus/source/css/_highlight/color-brewer.styl","hash":"9c4905eab730d0b389e6972e907057577f7e25f1","modified":1559614447460},{"_id":"themes/cactus/source/css/_highlight/darkula.styl","hash":"8965ad6920601c275ca97e617beff5536925a266","modified":1559614447461},{"_id":"themes/cactus/source/css/_highlight/far.styl","hash":"8da83d66724f2ce508a40f21b4f6dc0d704be562","modified":1559614447462},{"_id":"themes/cactus/source/css/_highlight/docco.styl","hash":"7bd3389ce16d20488ab336d557056cc703c921c7","modified":1559614447461},{"_id":"themes/cactus/source/css/_highlight/github-gist.styl","hash":"71f4b0fca91a587e6eba15a5306dca963bb8f441","modified":1559614447462},{"_id":"themes/cactus/source/css/_highlight/foundation.styl","hash":"28c59a31467c33bd51cbf3b6085782c2a724ff6c","modified":1559614447462},{"_id":"themes/cactus/source/css/_highlight/github.styl","hash":"a84eb710b302006120c3e7f8ca18f9e6fbc231c3","modified":1559614447463},{"_id":"themes/cactus/source/css/_highlight/googlecode.styl","hash":"7f5082ae008925a23eb713f160773fe647eb3ff7","modified":1559614447463},{"_id":"themes/cactus/source/css/_highlight/grayscale.styl","hash":"c83804abe39faebd80f8f4ff64fbd7137674cb1c","modified":1559614447464},{"_id":"themes/cactus/source/css/_highlight/gruvbox-dark.styl","hash":"f66403ce77dcb16b1f98a5061b72f7581630d69f","modified":1559614447464},{"_id":"themes/cactus/source/css/_highlight/hybrid.styl","hash":"4906456025787de04b48a87c42bb704c5ff67065","modified":1559614447465},{"_id":"themes/cactus/source/css/_highlight/highlightjs.styl","hash":"fd796a58c5d4590cda67c998187933788d45f30a","modified":1559614447464},{"_id":"themes/cactus/source/css/_highlight/idea.styl","hash":"e284c1760e8da0848f56cd5601d867ceeb0192d7","modified":1559614447466},{"_id":"themes/cactus/source/css/_highlight/hopscotch.styl","hash":"dd3c78c42d4a865f11623235e5e5f6829d789706","modified":1559614447465},{"_id":"themes/cactus/source/css/_highlight/index.styl","hash":"36c44375229613a5bb9ee84a8e90214978070439","modified":1559614447466},{"_id":"themes/cactus/source/css/_highlight/ir-black.styl","hash":"aa31b30069ebee39e2c3ebb75e2c96ba8678eb14","modified":1559614447467},{"_id":"themes/cactus/source/css/_highlight/kimbie.light.styl","hash":"e901738455ec9a1bddde7b62bbd8595de6033e1e","modified":1559614447468},{"_id":"themes/cactus/source/css/_highlight/kimbie.dark.styl","hash":"3b998c640eeb2c6192fee24bc78b4137de475dd7","modified":1559614447467},{"_id":"themes/cactus/source/css/_highlight/kimbie.styl","hash":"13113af220dfed09cb49d85102babb352c3eff97","modified":1559614447468},{"_id":"themes/cactus/source/css/_highlight/magula.styl","hash":"ab179306c12a1cf2949482beaca328e379ef034a","modified":1559614447469},{"_id":"themes/cactus/source/css/_highlight/monokai-sublime.styl","hash":"84a27bd29d939105d65f4164c219d6cc2e09ae60","modified":1559614447470},{"_id":"themes/cactus/source/css/_highlight/mono-blue.styl","hash":"14fb8678739b77f35771b6d63101ddbf1e7a9fbc","modified":1559614447469},{"_id":"themes/cactus/source/css/_highlight/monokai.styl","hash":"c3a3bfae1eb864505fbc8748db734600057af1af","modified":1559614447470},{"_id":"themes/cactus/source/css/_highlight/paraiso-dark.styl","hash":"50f1cee8a5b3b165b4184ead0a99dc564b62ef4f","modified":1559614447471},{"_id":"themes/cactus/source/css/_highlight/paraiso-light.styl","hash":"e428e8202b01e83b0f018a96058d806e7f4c76bf","modified":1559614447472},{"_id":"themes/cactus/source/css/_highlight/obsidian.styl","hash":"efba069860181d2b709e1548dd16cf102ca267fa","modified":1559614447471},{"_id":"themes/cactus/source/css/_highlight/pojoaque.styl","hash":"4d4a9360c35f3c5a0c5b9b49a1f2284e7e21317b","modified":1559614447473},{"_id":"themes/cactus/source/css/_highlight/paraiso.styl","hash":"846a06a57fa0b3db7f83ec7ac2bf34911f32cf66","modified":1559614447472},{"_id":"themes/cactus/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1559614447473},{"_id":"themes/cactus/source/css/_highlight/railscasts.styl","hash":"5dc9ce33cecee87fe9ca8f2ed9342602194484ec","modified":1559614447473},{"_id":"themes/cactus/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1559614447474},{"_id":"themes/cactus/source/css/_highlight/rainbow.styl","hash":"95246afef181bd96f9adb1a2e84fb3ef302d4598","modified":1559614447474},{"_id":"themes/cactus/source/css/_highlight/school-book.styl","hash":"186c37e9c39e65aa291cce3317e35904693598d1","modified":1559614447475},{"_id":"themes/cactus/source/css/_highlight/solarized-dark.styl","hash":"9fc9400d3a8cae97eb5761c284140acb0f847538","modified":1559614447475},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-bright.styl","hash":"e6b025b247e4027fb3c1b7833588f5a5b04a549c","modified":1559614447477},{"_id":"themes/cactus/source/css/_highlight/solarized-light.styl","hash":"bb04944fc06c12ecd7b56ad933dbedde60c2259a","modified":1559614447476},{"_id":"themes/cactus/source/css/_highlight/sunburst.styl","hash":"e6e4c009b10b9805f0c593446bf013edec47d146","modified":1559614447476},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-blue.styl","hash":"92f4423d4964fcfe34ff7ca6cb21012b5738c697","modified":1559614447477},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-eighties.styl","hash":"8b16876cf205111d5d5454100de712bc3ea8f477","modified":1559614447478},{"_id":"themes/cactus/source/css/_highlight/tomorrow.styl","hash":"502335f0fac07ed74ca78207bcf3ef8dd2252cf6","modified":1559614447478},{"_id":"themes/cactus/source/css/_highlight/zenburn.styl","hash":"f63534764dd6598e81177d64714a184f98153b11","modified":1559614447480},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night.styl","hash":"eb8441364bb1664ecebde77b965dc36c91133aa0","modified":1559614447478},{"_id":"themes/cactus/source/css/_highlight/vs.styl","hash":"14fbb0c43af440a290280b26968c8a5c0786b27f","modified":1559614447479},{"_id":"themes/cactus/source/css/_partial/archive.styl","hash":"c89529b82ff8bd059384aedf36e7690a9a67da7a","modified":1559614447481},{"_id":"themes/cactus/source/css/_partial/comments.styl","hash":"6e839bae0ddc11447579c34541e79b50d3670bbd","modified":1559614447482},{"_id":"themes/cactus/source/css/_partial/categories.styl","hash":"92fffe2baacb9d19021b3d120d2038417ca466df","modified":1559614447482},{"_id":"themes/cactus/source/css/_highlight/xcode.styl","hash":"5fa8999c7f807c1aae29c7a1cdf681678576fb69","modified":1559614447479},{"_id":"themes/cactus/source/css/_partial/article.styl","hash":"0d137addc5e5e7e91da38cc47ce60be6e2967fca","modified":1559614447481},{"_id":"themes/cactus/source/css/_partial/footer.styl","hash":"60e6220a944859f1cd0b73b301bdfea6d017c96f","modified":1559614447483},{"_id":"themes/cactus/source/css/_partial/index.styl","hash":"4a0da5d7052204b23d390a7f76cdc903b9eef52b","modified":1559614447483},{"_id":"themes/cactus/source/css/_partial/header.styl","hash":"7d5aa9a75d48ae9e24643829af47a55577a8bd5f","modified":1559614447483},{"_id":"themes/cactus/source/css/_partial/pagination.styl","hash":"4ff9755163aae6039bb72eac8360bc1c50fa26b6","modified":1559614447484},{"_id":"themes/cactus/source/css/_partial/tags.styl","hash":"7a2f9051b21bce525ecb71799d37c4b65a4f9171","modified":1559614447486},{"_id":"themes/cactus/source/css/_partial/search.styl","hash":"680ea535e5a20c01a31bee4d3eb680b86267a281","modified":1559614447486},{"_id":"themes/cactus/.git/objects/pack/pack-b09066dc6d0b70cdc33a61527c68cda59c4e5138.idx","hash":"10b47b678ba11ebbef2459e210c056dda1019780","modified":1559614447344},{"_id":"themes/cactus/source/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1559614447494},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","hash":"0c3192b500a4fd550e483cf77a49806a5872185b","modified":1559614447555},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","hash":"13d026ff857c853cbd0dc519b6e58669db309441","modified":1559614447651},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","hash":"f76ec625e15522ff60d21f7a9a3b71c65bc27556","modified":1559614447663},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","hash":"2e6c9df9f775373fb1988ae8529aa8f05313dae6","modified":1559614447666},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","hash":"594dc3344ad14903c247615427d1009709f0f5a4","modified":1559614447655},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","hash":"9f1e2934098a6a4a7c5584c8f3fa24a707070da3","modified":1559614447677},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","hash":"1f5a73db7947ef22c8a2bb19d6449b80496c03c4","modified":1559614447685},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","hash":"3edffd7bb61eee8cd46b57225f9f9e5264e3362b","modified":1559614447673},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","hash":"295f7e02c9b157e7ea63ad09613b00ceab85c5cd","modified":1559614447689},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","hash":"08e1503d1181188690fd9c81860d6c890c1465f6","modified":1559614447697},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.eot","hash":"31a9219c25fe1991fb745ec8dbbcf45c6094a702","modified":1559614447709},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","hash":"a6aa450ee6e0f85786474ca6b04827ef97e81af4","modified":1559614447700},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.ttf","hash":"f22b219824026e490a581ddb3b36b07997dff0e3","modified":1559614447713},{"_id":"themes/cactus/.git/logs/refs/heads/master","hash":"7ec96918e0c3c8fa69f92e5c8e97036cdf53b0c1","modified":1559614447396},{"_id":"themes/cactus/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1559614447390},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","hash":"cd4637e23ddae0dd64a8076261adc6fe90307c0f","modified":1559614447496},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"d902f8db3e021155f177f698a252fb98d6e61768","modified":1559614447516},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"42ff503f20e97503cef8e5b2ec10ae07699d7c01","modified":1559614447519},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"7b3f44b4d3028f3c87ddf0f4bd62511c9bf4a87e","modified":1559614447525},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"9784edb76f8a2ed595ea4bf74d46cda4eff3b303","modified":1559614447526},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"c140085833a38abec6b7df99d4ccac93eb266031","modified":1559614447524},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","hash":"a21698ea1b6caf22116d82e27c81115b8d5e9dcc","modified":1559614447556},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"92da6e3c7121e21cdfde25ef08797a3937a683e1","modified":1559614447553},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"6f5433cc9f19ce2403e903e5d01a4c7b38f0969b","modified":1559614447558},{"_id":"themes/cactus/source/css/_partial/post/actions_mobile.styl","hash":"b06410ed34b5ecbcac11357aa359149e1f827aa6","modified":1559614447485},{"_id":"themes/cactus/source/css/_partial/post/actions_desktop.styl","hash":"d5838e3abb1723fcf579066e04c3a16cdfd0e370","modified":1559614447484},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"f9d835a0f9248b1bb33d66968e87c4a50103ed8d","modified":1559614447515},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"644ece8263d2f96b087eebf7f6d4e309e5898eb5","modified":1559614447499},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"19e302760e39e25a5f8d90d6cd0164ef6cd74f8c","modified":1559614447513},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"80d33a73cbb60e206ef6f5c898988641576c7dda","modified":1559614447551},{"_id":"themes/cactus/.git/logs/refs/remotes/origin/HEAD","hash":"7ec96918e0c3c8fa69f92e5c8e97036cdf53b0c1","modified":1559614447389},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"6fcc06741ad4cdeefa546ce3e1af3a9b682a8429","modified":1559614447522},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"10740942ec6b3f4985529d343402d0bf32f9f847","modified":1559614447532},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"b542b9591fbf33925d93f0695b6e123a9f0cfd43","modified":1559614447613},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"c445864a9646948e0d7ff44930ad732ee61427d8","modified":1559614447550},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"b7d24ab1e4fad720f31a2b0cca1904ce1740d846","modified":1559614447584},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"926035f0156cccf1b0ca507347f39bf9c510f51e","modified":1559614447634},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"93ebc5098cf57a32b7b8d297681f31692c09bdfa","modified":1559614447618},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"9d757cc9f928fc83b2133283dd639c12b11d94ad","modified":1559614447640},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"de559f8d70d5b1ab2810597bfd0b1b9506f3ef01","modified":1559614447647},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"6c090d6bff3928fbf8a5f4104e58ed7f421aea7c","modified":1559614447598},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"9a23c6898b0943bd3d96c04df9a0f66e919451d8","modified":1559614447592},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"58be4b7760e9a84daa81929d046f9a15c4fd1c1a","modified":1559614447605},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"20ce1fc7ae1254558ca044ae48283faaa58897e5","modified":1559614447623},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"f9918fb93d6ab6850f5d38069a999c311af78816","modified":1559614447629},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"21731b0d1c81d0eb710b89fafb50a056dda00b97","modified":1559614447544},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"34f7db59f1d023294e69976aa20b7d52b86165a4","modified":1559614447577},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"ec8851ea10277bc5c73eb56da111b3e3f52b9def","modified":1559614447509},{"_id":"themes/cactus/.git/objects/pack/pack-b09066dc6d0b70cdc33a61527c68cda59c4e5138.pack","hash":"7aa1e95061be4f3631e73b30f8ee8602259742be","modified":1559614447333},{"_id":"public/search.xml","hash":"44290bbe2acd0d9fe1bc499daa33e65a6218746d","modified":1563012901341},{"_id":"public/about/index.html","hash":"a6a6925dad12cbdb336aa4054f1a66d48c78a218","modified":1563012901868},{"_id":"public/categories/index.html","hash":"680149104c5caa81bcc9830f7a2986c841908913","modified":1563012902812},{"_id":"public/tags/index.html","hash":"5e2b20f87b7abbccdfa3903f5eb2920d0efac249","modified":1563012902813},{"_id":"public/search/index.html","hash":"5179f03e2dba290b23baa793171f6cb983efe960","modified":1563012902813},{"_id":"public/2019/06/06/Graduation-sentiment/index.html","hash":"c8df42e8f8ba9cb70cc5e4c5e651f17a81b68273","modified":1563012902813},{"_id":"public/2019/04/20/weekly-report-190420/index.html","hash":"56d95600183b0d974ff494005090443c3349da49","modified":1563012902813},{"_id":"public/2019/04/19/StudyHBaseJavaAPI/index.html","hash":"1207e7b3bda2a2ce08944f2028de7c9bf1b8d1bf","modified":1563012902813},{"_id":"public/2019/04/14/weekly-report-190414/index.html","hash":"67ced7123314230488804560901fda1b15907fcd","modified":1563012902813},{"_id":"public/2019/03/30/weeklyReport190330/index.html","hash":"439b75c5d3f1600bc047c81cc4c03b7ebcf58b80","modified":1563012902813},{"_id":"public/2019/03/27/minionsUsage/index.html","hash":"a2aa49661fd28eaf416d99f90e9d4e09a2376ee1","modified":1563012902813},{"_id":"public/2019/03/23/weeklyReport190323/index.html","hash":"4a29236ad0a316cb4f45267aed2acf380f858aa2","modified":1563012902813},{"_id":"public/2019/03/18/frontendstudy/index.html","hash":"3f5f335f0a332b1928da2613618cfcdaacbf225c","modified":1563012902813},{"_id":"public/2019/03/18/axioscorsquestions/index.html","hash":"9881f7fd988cd1e5ddddf6c08f24004f10130848","modified":1563012902813},{"_id":"public/2019/03/18/websitesafari/index.html","hash":"103ccf59e3f798582879205c6407428136342a08","modified":1563012902814},{"_id":"public/archives/index.html","hash":"25a81c68b12aab843165edf391d33c7c83bfe5b2","modified":1563012902814},{"_id":"public/archives/page/2/index.html","hash":"78576579e819690d5c0c8f1e400de4125ee044c8","modified":1563012902814},{"_id":"public/archives/page/3/index.html","hash":"f50eb7323ea263799272012f4594a9d3b6b8d206","modified":1563012902814},{"_id":"public/archives/2019/index.html","hash":"4dba4aa2c3650e9b1dc36d515e4b1bfc0679412a","modified":1563012902814},{"_id":"public/archives/2019/page/2/index.html","hash":"71ebb243bdbc2fbef3f0b85e1ca84b67a79f70e3","modified":1563012902814},{"_id":"public/archives/2019/page/3/index.html","hash":"716aa60a73b08e82d0640565570dce5e7f479f13","modified":1563012902814},{"_id":"public/archives/2019/03/index.html","hash":"417fafc83276ba2a3f27884a073fd0c800f12b83","modified":1563012902814},{"_id":"public/archives/2019/03/page/2/index.html","hash":"d34e46a33a93a7127fcd80a45cf6f8253a37ba88","modified":1563012902814},{"_id":"public/archives/2019/04/index.html","hash":"d289bcec712ec8969c008fe0aa59a707339bc03e","modified":1563012902814},{"_id":"public/archives/2019/05/index.html","hash":"9177d6f076723f72f4223487633caa8bfb8b232d","modified":1563012902814},{"_id":"public/index.html","hash":"745932ff1769ec44281f3147e12173392375fb54","modified":1563012902815},{"_id":"public/archives/2019/06/index.html","hash":"55cebf76e8b01a6eecbd98db56887c66e10ae22a","modified":1563012902815},{"_id":"public/page/2/index.html","hash":"ee5d3bad486d258331f0f4184e6ce50b9e3aa48f","modified":1563012902815},{"_id":"public/page/3/index.html","hash":"60f86a7024a22cea6bb6508cb1530cba4465c232","modified":1563012902815},{"_id":"public/categories/技术总结/index.html","hash":"1dda56909a42753732792ad6d2cfb8773e7337bb","modified":1563012902815},{"_id":"public/categories/毕业设计/index.html","hash":"86e1b037eb19f5a78f6edf4bdef8709af186e19f","modified":1563012902815},{"_id":"public/categories/算法/index.html","hash":"6a2892fddd6fe886b0349d40f046d6f763d2c1cb","modified":1563012902815},{"_id":"public/categories/每周记录/index.html","hash":"0e6fd697e038ae4c17db861fa3232a3ba1c66d4e","modified":1563012902815},{"_id":"public/categories/毕业/index.html","hash":"58fec6afb65017997afd0c8a9ccd56a9fa3e9863","modified":1563012902816},{"_id":"public/tags/Java/index.html","hash":"a76bd98efc435137187b1103b9bcfbc9d4d58212","modified":1563012902816},{"_id":"public/tags/开发相关/index.html","hash":"0993675344af93ab94ece03458546e819f41e472","modified":1563012902816},{"_id":"public/tags/Flume/index.html","hash":"03b229917976e6d341757f8c3badcf1f1df54ec6","modified":1563012902816},{"_id":"public/tags/axios/index.html","hash":"6fb8923b0d69684ed7624c52a0cce305514b31a5","modified":1563012902816},{"_id":"public/tags/CORS/index.html","hash":"4ba5a6c8a92b1c95c16a8a99cbdc76bcaabbf486","modified":1563012902816},{"_id":"public/tags/react-ES6/index.html","hash":"618fe94e569669cd1dbcc730682201b3caea37ac","modified":1563012902816},{"_id":"public/tags/毕业设计/index.html","hash":"8f528ea83324d9657500479eaa94d4f85442c5db","modified":1563012902816},{"_id":"public/tags/翻译/index.html","hash":"2000f334d5328217eeb19a6e5103dd060e061144","modified":1563012902817},{"_id":"public/tags/大型网络扩展/index.html","hash":"3eafac8f45e022316aeb620af0f689303584d76a","modified":1563012902817},{"_id":"public/tags/LeetCode/index.html","hash":"d9612d77f9a90cd46dd5db3c04db32bc638ec2fa","modified":1563012902817},{"_id":"public/tags/Spark/index.html","hash":"852edc4815501e2bcd9f57952eecd892efc09e47","modified":1563012902817},{"_id":"public/tags/RDD/index.html","hash":"f232c4fd7b52b70a78fce1a7fe0d0f9de856aca5","modified":1563012902817},{"_id":"public/2019/06/28/graduation-letter-from-Hushi/index.html","hash":"8531a0b14653d2abf92ae3f97d4664f9fd3e70f4","modified":1563012902817},{"_id":"public/2019/06/01/graduation-design-paper/index.html","hash":"3c3c1ad21f0357503b72434dd8304c4aedda505d","modified":1563012902817},{"_id":"public/2019/05/27/Kadane's-Algorithm/index.html","hash":"fbaef35ab6e020fb2de7991bb5e10dffcdec28c6","modified":1563012902817},{"_id":"public/2019/05/03/KMP/index.html","hash":"b39b9e7eb89fb295fd1818689b50f34aeb623aba","modified":1563012902817},{"_id":"public/2019/04/24/Minions-Visualization-first/index.html","hash":"21e07f6c745f74719a3dd89a75f4ea5b52725522","modified":1563012902817},{"_id":"public/2019/04/22/mock-accesslog-python/index.html","hash":"ec12de8345c6df62454471e4da10e80277675424","modified":1563012902817},{"_id":"public/2019/04/22/user-action-logger-design/index.html","hash":"db21353f9a7623408b0a9a6545dccfcb4b4ddd49","modified":1563012902818},{"_id":"public/2019/04/13/tanslate-Resilient-Distributed-Datasets-A-Fault-Tolerant-Abstraction–for-In-Memory-Cluster-Computing/index.html","hash":"5bf99cae150640cf1bde717a508102971cf207f1","modified":1563012902818},{"_id":"public/2019/04/09/transfer-scaling-webapps-for-newbs-and-non-techies/index.html","hash":"81a278434e3672ce1ec5a0cbbd65cc9223be9f42","modified":1563012902818},{"_id":"public/2019/03/21/minions-frontend-overview/index.html","hash":"7631a8aec509a459965f5f815f81f22ca3088b8f","modified":1563012902818},{"_id":"public/2019/03/19/graduationDesignReferences/index.html","hash":"4c975d1809c8d83a99374359636b1bc6622d3778","modified":1563012902818},{"_id":"public/2019/03/18/JavaUtilsAPI/index.html","hash":"daf26ab9637a1ae0af58e5162de941377375df59","modified":1563012902818},{"_id":"public/2019/03/16/weeklyReport190316/index.html","hash":"340be174a95a89412d296d84af4b80745292a511","modified":1563012902818},{"_id":"public/2019/03/16/FluemToKafkaBaseOnDifferntTopic/index.html","hash":"0a23e0d9ece4a381ab43af30c9c53f32258c560c","modified":1563012902818},{"_id":"public/2019/03/15/MyLeetCodeSummarize/index.html","hash":"45a27b25adcdcb754e10ee179fd14cef106e54d2","modified":1563012902818},{"_id":"public/2019/03/14/微信小程序学习意见/index.html","hash":"47e655517bf96ebfccc390c035145b3e39231e1a","modified":1563012902818},{"_id":"public/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1563012902818},{"_id":"public/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1563012902818},{"_id":"public/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1563012902818},{"_id":"public/lib/vazir-font/Vazir-Black.woff","hash":"37443d0040f0d7af381c955e4c15919a15d0349e","modified":1563012902818},{"_id":"public/lib/vazir-font/Vazir-Black.woff2","hash":"0a257c8b60e0f20802c1dc8daeed2d3cb0d44f17","modified":1563012902818},{"_id":"public/lib/vazir-font/Vazir-Bold.woff2","hash":"62447a951d48b21c4696ae72df4bc4adef636e26","modified":1563012902818},{"_id":"public/lib/vazir-font/Vazir-Bold.woff","hash":"df15fd1e74b6f4a50bea57e2b44d9627f38495b5","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Light.woff2","hash":"ef07a250766fea840c1049e67c0405d9216ee0a8","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Light.woff","hash":"32ae5c0d1d5943c8bb8e0f6ab07c3269c6f8b8a8","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Medium.woff","hash":"f5653059b2a5929516e4aab02329a978600b9b67","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir.woff","hash":"bbee70033f0f5882e9869e417b69c6a38f56f187","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Medium.woff2","hash":"668400ae92700965f03f2371faaee0ab8c8347c3","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Thin.woff2","hash":"c3be79b553ec394db71268d604b1d29183b867dc","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir.woff2","hash":"30ce165216db078951a690a6ad665b9b78f5dd81","modified":1563012902819},{"_id":"public/lib/vazir-font/Vazir-Thin.woff","hash":"ad4d46a99a1daf6353c86c79ac3a2b030213859c","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"d902f8db3e021155f177f698a252fb98d6e61768","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"42ff503f20e97503cef8e5b2ec10ae07699d7c01","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"7b3f44b4d3028f3c87ddf0f4bd62511c9bf4a87e","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"9784edb76f8a2ed595ea4bf74d46cda4eff3b303","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"c140085833a38abec6b7df99d4ccac93eb266031","modified":1563012902819},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"92da6e3c7121e21cdfde25ef08797a3937a683e1","modified":1563012902820},{"_id":"public/lib/vazir-font/Vazir-Bold.eot","hash":"f76ec625e15522ff60d21f7a9a3b71c65bc27556","modified":1563012903461},{"_id":"public/lib/vazir-font/Vazir-Black.eot","hash":"13d026ff857c853cbd0dc519b6e58669db309441","modified":1563012903462},{"_id":"public/lib/vazir-font/Vazir-Bold.ttf","hash":"2e6c9df9f775373fb1988ae8529aa8f05313dae6","modified":1563012903462},{"_id":"public/lib/vazir-font/Vazir-Light.ttf","hash":"9f1e2934098a6a4a7c5584c8f3fa24a707070da3","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Medium.eot","hash":"1f5a73db7947ef22c8a2bb19d6449b80496c03c4","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Black.ttf","hash":"594dc3344ad14903c247615427d1009709f0f5a4","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Light.eot","hash":"3edffd7bb61eee8cd46b57225f9f9e5264e3362b","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Thin.eot","hash":"08e1503d1181188690fd9c81860d6c890c1465f6","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Medium.ttf","hash":"295f7e02c9b157e7ea63ad09613b00ceab85c5cd","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir-Thin.ttf","hash":"a6aa450ee6e0f85786474ca6b04827ef97e81af4","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir.eot","hash":"31a9219c25fe1991fb745ec8dbbcf45c6094a702","modified":1563012903463},{"_id":"public/lib/vazir-font/Vazir.ttf","hash":"f22b219824026e490a581ddb3b36b07997dff0e3","modified":1563012903463},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"f9d835a0f9248b1bb33d66968e87c4a50103ed8d","modified":1563012903463},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"644ece8263d2f96b087eebf7f6d4e309e5898eb5","modified":1563012903464},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"19e302760e39e25a5f8d90d6cd0164ef6cd74f8c","modified":1563012903464},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"80d33a73cbb60e206ef6f5c898988641576c7dda","modified":1563012903464},{"_id":"public/js/main.js","hash":"584c5a69ac81a483a1c4377a2e2cf326c2795e7b","modified":1563012903473},{"_id":"public/css/rtl.css","hash":"c2c2bc4ce311b3129275e009e903088b45e7ed77","modified":1563012903474},{"_id":"public/js/search.js","hash":"a74d0c601f820160825a2e4ad13618074d714933","modified":1563012903474},{"_id":"public/lib/vazir-font/font-face.css","hash":"8f2bf6b59ae1f2ed4c2fead6cea4b8314fcf62e5","modified":1563012903474},{"_id":"public/lib/justified-gallery/css/justifiedGallery.min.css","hash":"92bb6e468a1db7fbd99ccb960e15e28572254263","modified":1563012903474},{"_id":"public/css/style.css","hash":"75db29e695525c39e3718260c84bb475a4fe8160","modified":1563012903474},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"6fcc06741ad4cdeefa546ce3e1af3a9b682a8429","modified":1563012903474},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"10740942ec6b3f4985529d343402d0bf32f9f847","modified":1563012903474},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"c445864a9646948e0d7ff44930ad732ee61427d8","modified":1563012903480},{"_id":"public/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"82ab395176c927ffbb2f7c95132ee0a06cd5d64a","modified":1563012903488},{"_id":"public/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1563012903488},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"cf1a3fd771900af34f2af22142beecfb47367548","modified":1563012903546},{"_id":"public/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"b7d24ab1e4fad720f31a2b0cca1904ce1740d846","modified":1563012903547},{"_id":"public/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"93ebc5098cf57a32b7b8d297681f31692c09bdfa","modified":1563012903547},{"_id":"public/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"9d757cc9f928fc83b2133283dd639c12b11d94ad","modified":1563012903547},{"_id":"public/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"9a23c6898b0943bd3d96c04df9a0f66e919451d8","modified":1563012903548},{"_id":"public/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"b542b9591fbf33925d93f0695b6e123a9f0cfd43","modified":1563012903570},{"_id":"public/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"926035f0156cccf1b0ca507347f39bf9c510f51e","modified":1563012903570},{"_id":"public/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"de559f8d70d5b1ab2810597bfd0b1b9506f3ef01","modified":1563012903570},{"_id":"public/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"6c090d6bff3928fbf8a5f4104e58ed7f421aea7c","modified":1563012903571},{"_id":"public/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"58be4b7760e9a84daa81929d046f9a15c4fd1c1a","modified":1563012903571},{"_id":"public/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"f9918fb93d6ab6850f5d38069a999c311af78816","modified":1563012903572},{"_id":"public/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"20ce1fc7ae1254558ca044ae48283faaa58897e5","modified":1563012903574},{"_id":"public/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"34f7db59f1d023294e69976aa20b7d52b86165a4","modified":1563012903574},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"ec8851ea10277bc5c73eb56da111b3e3f52b9def","modified":1563012903655},{"_id":"public/lib/jquery/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1563012903671},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"21731b0d1c81d0eb710b89fafb50a056dda00b97","modified":1563012903807}],"Category":[{"name":"技术总结","_id":"cjy1diwah000248updve23zlm"},{"name":"毕业设计","_id":"cjy1diwcv000g48upvqmtx0um"},{"name":"算法","_id":"cjy1diwdy000r48uposf4iov0"},{"name":"每周记录","_id":"cjy1diwfn001f48upqbwwl1h8"},{"name":"毕业","_id":"cjy1diwi8002148upe1vmo6bq"}],"Data":[{"_id":"projects","data":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}],"Page":[{"title":"About Me","date":"2019-03-15T07:04:35.000Z","_content":"# Enjoy Fruits!\n\n\n\n<center>\n⁣🍐🍌🍋🍋🍊🍊🍎🍎🍎\n🍐🍏🍌🍋🍋🍊🍊🍎🍎\n🍆🍐🍐🍌🍋🍋🍊🍊🍎\n🍇🍇🍏      🍋🍊🍊\n🍎🍆🍇  ⁣😎  🍋🍋🍊\n🍒🍓🍇      🍌🍋🍋\n🍊🍎🍉🍇🍆🍐🍐🍌🍋\n🍊🍊🍓🍎🍇🍇🍏🍐🍌\n🍋🍊🍊🍎🍓🍇🍇🍐🍏\n</center>","source":"about/index.md","raw":"---\ntitle: About Me\ndate: 2019-03-15 15:04:35\n---\n# Enjoy Fruits!\n\n\n\n<center>\n⁣🍐🍌🍋🍋🍊🍊🍎🍎🍎\n🍐🍏🍌🍋🍋🍊🍊🍎🍎\n🍆🍐🍐🍌🍋🍋🍊🍊🍎\n🍇🍇🍏      🍋🍊🍊\n🍎🍆🍇  ⁣😎  🍋🍋🍊\n🍒🍓🍇      🍌🍋🍋\n🍊🍎🍉🍇🍆🍐🍐🍌🍋\n🍊🍊🍓🍎🍇🍇🍏🍐🍌\n🍋🍊🍊🍎🍓🍇🍇🍐🍏\n</center>","updated":"2019-06-04T01:06:18.988Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjy1diwcn000d48upsvwv2ssr","content":"<h1 id=\"Enjoy-Fruits\"><a href=\"#Enjoy-Fruits\" class=\"headerlink\" title=\"Enjoy Fruits!\"></a>Enjoy Fruits!</h1><center><br>⁣🍐🍌🍋🍋🍊🍊🍎🍎🍎<br>🍐🍏🍌🍋🍋🍊🍊🍎🍎<br>🍆🍐🍐🍌🍋🍋🍊🍊🍎<br>🍇🍇🍏      🍋🍊🍊<br>🍎🍆🍇  ⁣😎  🍋🍋🍊<br>🍒🍓🍇      🍌🍋🍋<br>🍊🍎🍉🍇🍆🍐🍐🍌🍋<br>🍊🍊🍓🍎🍇🍇🍏🍐🍌<br>🍋🍊🍊🍎🍓🍇🍇🍐🍏<br></center>","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h1 id=\"Enjoy-Fruits\"><a href=\"#Enjoy-Fruits\" class=\"headerlink\" title=\"Enjoy Fruits!\"></a>Enjoy Fruits!</h1><center><br>⁣🍐🍌🍋🍋🍊🍊🍎🍎🍎<br>🍐🍏🍌🍋🍋🍊🍊🍎🍎<br>🍆🍐🍐🍌🍋🍋🍊🍊🍎<br>🍇🍇🍏      🍋🍊🍊<br>🍎🍆🍇  ⁣😎  🍋🍋🍊<br>🍒🍓🍇      🍌🍋🍋<br>🍊🍎🍉🍇🍆🍐🍐🍌🍋<br>🍊🍊🍓🍎🍇🍇🍏🍐🍌<br>🍋🍊🍊🍎🍓🍇🍇🍐🍏<br></center>"},{"title":"categories","date":"2019-03-15T07:38:37.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-03-15 15:38:37\ntype: categories\n---\n","updated":"2019-06-04T01:06:18.989Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjy1diwcr000f48upzo715prn","content":"","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":""},{"title":"tags","type":"tags","date":"2019-03-15T07:35:40.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ntype: tags\ndate: 2019-03-15 15:35:40\n---\n","updated":"2019-06-04T01:06:18.990Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjy1diwd2000i48upk66p2u1n","content":"","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":""},{"title":"search","type":"search","date":"2019-03-15T07:47:20.000Z","_content":"","source":"search/index.md","raw":"---\ntitle: search\ntype: search\ndate: 2019-03-15 15:47:20\n---\n","updated":"2019-06-04T01:06:18.989Z","path":"search/index.html","comments":1,"layout":"page","_id":"cjy1diwdb000k48upaz48bbfg","content":"","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":""}],"Post":[{"title":"Java中常用API","date":"2019-03-17T16:00:00.000Z","_content":"## 转换类型\n* String转Char[]:\n\n```java\n    Char[] a = string。toCharArray();\n```\n\n* Char[]转String\n\n```java\n    //只能用\n    return new String(char[])\n```\n\n## String操作\n\n```java\n    //创建String\n    StringBuilder res = new StringBuilder();\n    //添加字符\n    res.append(something);\n    //转换\n    String(res) stringbuilder.toString()\n    //reverse\n    res.reverse();\n    //去掉首位空格\n    res.trim();\n    //取char\n    string.charAt(int i)\n    //大小写转变\n    toLowerCase()\n    toUpperCase()\n    //字符串比较\n    A.equals(B)\n    //split任意数目个空格\n    s.split(\"\\\\s+\")\n```\n\n## 数字操作\n\n```java\n    //sqrt\n    Math.sqrt(num);\n```\n\n## 重要数据结构\n\n### HashSet\n\n```java\n    //创建\n    Set<Object> hSet = new HashSet();\n    //添加元素\n    hSet.add(somethin);\n    //删除\n    hSet.remove(object);\n    //判断包含\n    hSet.contains(object)\n```\n","source":"_posts/JavaUtilsAPI.md","raw":"---\ntitle: Java中常用API\ndate: 2019/3/18\ncategories:\n    - 技术总结\ntags:\n    - Java\n---\n## 转换类型\n* String转Char[]:\n\n```java\n    Char[] a = string。toCharArray();\n```\n\n* Char[]转String\n\n```java\n    //只能用\n    return new String(char[])\n```\n\n## String操作\n\n```java\n    //创建String\n    StringBuilder res = new StringBuilder();\n    //添加字符\n    res.append(something);\n    //转换\n    String(res) stringbuilder.toString()\n    //reverse\n    res.reverse();\n    //去掉首位空格\n    res.trim();\n    //取char\n    string.charAt(int i)\n    //大小写转变\n    toLowerCase()\n    toUpperCase()\n    //字符串比较\n    A.equals(B)\n    //split任意数目个空格\n    s.split(\"\\\\s+\")\n```\n\n## 数字操作\n\n```java\n    //sqrt\n    Math.sqrt(num);\n```\n\n## 重要数据结构\n\n### HashSet\n\n```java\n    //创建\n    Set<Object> hSet = new HashSet();\n    //添加元素\n    hSet.add(somethin);\n    //删除\n    hSet.remove(object);\n    //判断包含\n    hSet.contains(object)\n```\n","slug":"JavaUtilsAPI","published":1,"updated":"2019-07-13T08:48:25.886Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwa5000048ups53lj72k","content":"<h2 id=\"转换类型\"><a href=\"#转换类型\" class=\"headerlink\" title=\"转换类型\"></a>转换类型</h2><ul>\n<li>String转Char[]:</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Char[] a = string。toCharArray();</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Char[]转String</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//只能用</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"keyword\">new</span> String(<span class=\"keyword\">char</span>[])</span><br></pre></td></tr></table></figure>\n<h2 id=\"String操作\"><a href=\"#String操作\" class=\"headerlink\" title=\"String操作\"></a>String操作</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建String</span></span><br><span class=\"line\">StringBuilder res = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\"><span class=\"comment\">//添加字符</span></span><br><span class=\"line\">res.append(something);</span><br><span class=\"line\"><span class=\"comment\">//转换</span></span><br><span class=\"line\">String(res) stringbuilder.toString()</span><br><span class=\"line\"><span class=\"comment\">//reverse</span></span><br><span class=\"line\">res.reverse();</span><br><span class=\"line\"><span class=\"comment\">//去掉首位空格</span></span><br><span class=\"line\">res.trim();</span><br><span class=\"line\"><span class=\"comment\">//取char</span></span><br><span class=\"line\">string.charAt(<span class=\"keyword\">int</span> i)</span><br><span class=\"line\"><span class=\"comment\">//大小写转变</span></span><br><span class=\"line\">toLowerCase()</span><br><span class=\"line\">toUpperCase()</span><br><span class=\"line\"><span class=\"comment\">//字符串比较</span></span><br><span class=\"line\">A.equals(B)</span><br><span class=\"line\"><span class=\"comment\">//split任意数目个空格</span></span><br><span class=\"line\">s.split(<span class=\"string\">\"\\\\s+\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"数字操作\"><a href=\"#数字操作\" class=\"headerlink\" title=\"数字操作\"></a>数字操作</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//sqrt</span></span><br><span class=\"line\">Math.sqrt(num);</span><br></pre></td></tr></table></figure>\n<h2 id=\"重要数据结构\"><a href=\"#重要数据结构\" class=\"headerlink\" title=\"重要数据结构\"></a>重要数据结构</h2><h3 id=\"HashSet\"><a href=\"#HashSet\" class=\"headerlink\" title=\"HashSet\"></a>HashSet</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建</span></span><br><span class=\"line\">Set&lt;Object&gt; hSet = <span class=\"keyword\">new</span> HashSet();</span><br><span class=\"line\"><span class=\"comment\">//添加元素</span></span><br><span class=\"line\">hSet.add(somethin);</span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\">hSet.remove(object);</span><br><span class=\"line\"><span class=\"comment\">//判断包含</span></span><br><span class=\"line\">hSet.contains(object)</span><br></pre></td></tr></table></figure>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"转换类型\"><a href=\"#转换类型\" class=\"headerlink\" title=\"转换类型\"></a>转换类型</h2><ul>\n<li>String转Char[]:</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Char[] a = string。toCharArray();</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Char[]转String</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//只能用</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"keyword\">new</span> String(<span class=\"keyword\">char</span>[])</span><br></pre></td></tr></table></figure>\n<h2 id=\"String操作\"><a href=\"#String操作\" class=\"headerlink\" title=\"String操作\"></a>String操作</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建String</span></span><br><span class=\"line\">StringBuilder res = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\"><span class=\"comment\">//添加字符</span></span><br><span class=\"line\">res.append(something);</span><br><span class=\"line\"><span class=\"comment\">//转换</span></span><br><span class=\"line\">String(res) stringbuilder.toString()</span><br><span class=\"line\"><span class=\"comment\">//reverse</span></span><br><span class=\"line\">res.reverse();</span><br><span class=\"line\"><span class=\"comment\">//去掉首位空格</span></span><br><span class=\"line\">res.trim();</span><br><span class=\"line\"><span class=\"comment\">//取char</span></span><br><span class=\"line\">string.charAt(<span class=\"keyword\">int</span> i)</span><br><span class=\"line\"><span class=\"comment\">//大小写转变</span></span><br><span class=\"line\">toLowerCase()</span><br><span class=\"line\">toUpperCase()</span><br><span class=\"line\"><span class=\"comment\">//字符串比较</span></span><br><span class=\"line\">A.equals(B)</span><br><span class=\"line\"><span class=\"comment\">//split任意数目个空格</span></span><br><span class=\"line\">s.split(<span class=\"string\">\"\\\\s+\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"数字操作\"><a href=\"#数字操作\" class=\"headerlink\" title=\"数字操作\"></a>数字操作</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//sqrt</span></span><br><span class=\"line\">Math.sqrt(num);</span><br></pre></td></tr></table></figure>\n<h2 id=\"重要数据结构\"><a href=\"#重要数据结构\" class=\"headerlink\" title=\"重要数据结构\"></a>重要数据结构</h2><h3 id=\"HashSet\"><a href=\"#HashSet\" class=\"headerlink\" title=\"HashSet\"></a>HashSet</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建</span></span><br><span class=\"line\">Set&lt;Object&gt; hSet = <span class=\"keyword\">new</span> HashSet();</span><br><span class=\"line\"><span class=\"comment\">//添加元素</span></span><br><span class=\"line\">hSet.add(somethin);</span><br><span class=\"line\"><span class=\"comment\">//删除</span></span><br><span class=\"line\">hSet.remove(object);</span><br><span class=\"line\"><span class=\"comment\">//判断包含</span></span><br><span class=\"line\">hSet.contains(object)</span><br></pre></td></tr></table></figure>\n"},{"title":"Flume使用interceptors向不同的Kafka Consumers发送基于Topic维度的信息","date":"2019-03-15T16:00:00.000Z","_content":"# 需求\n* 后端接口通过log4j转发给flume的Event是直接发给avro source，之后要交给kafka sink供不同的应用处理。如果对于不同的topic都用一个agent处理感觉略尴尬（没有尝试），所以用一个avro source进行接收，用flume的interceptor进行按topic分流。\n\n# 方案流程\n* 前端\n    * 向后端上传数据时，加入topic字段\n  \n```javascript\naxios.get(\"url/logtest\", {\n            params:{\n                K_topic: 'specialTopic',\n                contents: 'contents'\n            }\n```\n\n* 后端\n    * 在log4j的logger.info中加入topic字段\n\n```java\nlogger.info(\"topic:\" + k_topic + \" \" + \"contents:\"+ contents);\n```\n\n* flume配置（部分）\n\n```\n#source interceptor\nagent1.sources.avro-source.interceptors=i1\nagent1.sources.avro-source.interceptors.i1.type=regex_extractor\nagent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) contents:(.*?)\nagent1.sources.avro-source.interceptors.i1.serializers=s1 s2\n#agent1.sources.avro-source.interceptors.i1.serializers.s1.type=default\nagent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic\n#agent1.sources.avro-source.interceptors.i1.serializers.s2.type=default\nagent1.sources.avro-source.interceptors.i1.serializers.s2.name=contents\n\n#define sink\nagent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink\nagent1.sinks.kafka-sink.topic = %{topic}\n```\n\n* 结果\n不同的kafka Producer能够接收不同topic的埋点数据，之后的consumer也能消费到了\n\n# 参考文档\n* [regex-extractor-interceptor正则表达式interceptor](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#regex-extractor-interceptor)\n* [interceptor](http://lxw1234.com/archives/2015/11/543.htm)","source":"_posts/FluemToKafkaBaseOnDifferntTopic.md","raw":"---\ntitle: Flume使用interceptors向不同的Kafka Consumers发送基于Topic维度的信息\ndate: 2019/3/16\ncategories:\n    - 技术总结\ntags:\n    - 开发相关\n    - Flume\n---\n# 需求\n* 后端接口通过log4j转发给flume的Event是直接发给avro source，之后要交给kafka sink供不同的应用处理。如果对于不同的topic都用一个agent处理感觉略尴尬（没有尝试），所以用一个avro source进行接收，用flume的interceptor进行按topic分流。\n\n# 方案流程\n* 前端\n    * 向后端上传数据时，加入topic字段\n  \n```javascript\naxios.get(\"url/logtest\", {\n            params:{\n                K_topic: 'specialTopic',\n                contents: 'contents'\n            }\n```\n\n* 后端\n    * 在log4j的logger.info中加入topic字段\n\n```java\nlogger.info(\"topic:\" + k_topic + \" \" + \"contents:\"+ contents);\n```\n\n* flume配置（部分）\n\n```\n#source interceptor\nagent1.sources.avro-source.interceptors=i1\nagent1.sources.avro-source.interceptors.i1.type=regex_extractor\nagent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) contents:(.*?)\nagent1.sources.avro-source.interceptors.i1.serializers=s1 s2\n#agent1.sources.avro-source.interceptors.i1.serializers.s1.type=default\nagent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic\n#agent1.sources.avro-source.interceptors.i1.serializers.s2.type=default\nagent1.sources.avro-source.interceptors.i1.serializers.s2.name=contents\n\n#define sink\nagent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink\nagent1.sinks.kafka-sink.topic = %{topic}\n```\n\n* 结果\n不同的kafka Producer能够接收不同topic的埋点数据，之后的consumer也能消费到了\n\n# 参考文档\n* [regex-extractor-interceptor正则表达式interceptor](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#regex-extractor-interceptor)\n* [interceptor](http://lxw1234.com/archives/2015/11/543.htm)","slug":"FluemToKafkaBaseOnDifferntTopic","published":1,"updated":"2019-06-04T01:06:18.973Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwad000148up1ktf88xv","content":"<h1 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h1><ul>\n<li>后端接口通过log4j转发给flume的Event是直接发给avro source，之后要交给kafka sink供不同的应用处理。如果对于不同的topic都用一个agent处理感觉略尴尬（没有尝试），所以用一个avro source进行接收，用flume的interceptor进行按topic分流。</li>\n</ul>\n<h1 id=\"方案流程\"><a href=\"#方案流程\" class=\"headerlink\" title=\"方案流程\"></a>方案流程</h1><ul>\n<li>前端<ul>\n<li>向后端上传数据时，加入topic字段</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">axios.get(<span class=\"string\">\"url/logtest\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'specialTopic'</span>,</span><br><span class=\"line\">                contents: <span class=\"string\">'contents'</span></span><br><span class=\"line\">            &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>后端<ul>\n<li>在log4j的logger.info中加入topic字段</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger.info(<span class=\"string\">\"topic:\"</span> + k_topic + <span class=\"string\">\" \"</span> + <span class=\"string\">\"contents:\"</span>+ contents);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>flume配置（部分）</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#source interceptor</span><br><span class=\"line\">agent1.sources.avro-source.interceptors=i1</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.type=regex_extractor</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) contents:(.*?)</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers=s1 s2</span><br><span class=\"line\">#agent1.sources.avro-source.interceptors.i1.serializers.s1.type=default</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic</span><br><span class=\"line\">#agent1.sources.avro-source.interceptors.i1.serializers.s2.type=default</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s2.name=contents</span><br><span class=\"line\"></span><br><span class=\"line\">#define sink</span><br><span class=\"line\">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">agent1.sinks.kafka-sink.topic = %&#123;topic&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>结果<br>不同的kafka Producer能够接收不同topic的埋点数据，之后的consumer也能消费到了</li>\n</ul>\n<h1 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h1><ul>\n<li><a href=\"http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#regex-extractor-interceptor\" target=\"_blank\" rel=\"noopener\">regex-extractor-interceptor正则表达式interceptor</a></li>\n<li><a href=\"http://lxw1234.com/archives/2015/11/543.htm\" target=\"_blank\" rel=\"noopener\">interceptor</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h1 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h1><ul>\n<li>后端接口通过log4j转发给flume的Event是直接发给avro source，之后要交给kafka sink供不同的应用处理。如果对于不同的topic都用一个agent处理感觉略尴尬（没有尝试），所以用一个avro source进行接收，用flume的interceptor进行按topic分流。</li>\n</ul>\n<h1 id=\"方案流程\"><a href=\"#方案流程\" class=\"headerlink\" title=\"方案流程\"></a>方案流程</h1><ul>\n<li>前端<ul>\n<li>向后端上传数据时，加入topic字段</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">axios.get(<span class=\"string\">\"url/logtest\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'specialTopic'</span>,</span><br><span class=\"line\">                contents: <span class=\"string\">'contents'</span></span><br><span class=\"line\">            &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>后端<ul>\n<li>在log4j的logger.info中加入topic字段</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logger.info(<span class=\"string\">\"topic:\"</span> + k_topic + <span class=\"string\">\" \"</span> + <span class=\"string\">\"contents:\"</span>+ contents);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>flume配置（部分）</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#source interceptor</span><br><span class=\"line\">agent1.sources.avro-source.interceptors=i1</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.type=regex_extractor</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) contents:(.*?)</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers=s1 s2</span><br><span class=\"line\">#agent1.sources.avro-source.interceptors.i1.serializers.s1.type=default</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic</span><br><span class=\"line\">#agent1.sources.avro-source.interceptors.i1.serializers.s2.type=default</span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s2.name=contents</span><br><span class=\"line\"></span><br><span class=\"line\">#define sink</span><br><span class=\"line\">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">agent1.sinks.kafka-sink.topic = %&#123;topic&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>结果<br>不同的kafka Producer能够接收不同topic的埋点数据，之后的consumer也能消费到了</li>\n</ul>\n<h1 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h1><ul>\n<li><a href=\"http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#regex-extractor-interceptor\" target=\"_blank\" rel=\"noopener\">regex-extractor-interceptor正则表达式interceptor</a></li>\n<li><a href=\"http://lxw1234.com/archives/2015/11/543.htm\" target=\"_blank\" rel=\"noopener\">interceptor</a></li>\n</ul>\n"},{"title":"【毕业设计Minions一期】可视化部分展示","date":"2019-04-23T16:00:00.000Z","_content":"## 题目\n* 基于Spark Streaming的网站实时流量分析\n\n## 技术栈\n* 前端框架\n    * react\n    * 组件\n        * ant-design\n        * ant-design Pro\n        * BizCharts\n\n* 后端框架\n    * springboot\n\n* 持久化\n    * HBase\n    * MySQL\n\n## 可视化内容概述\n* 可视化展示用户访问日志数据和用户行为日志，本设计具体化为\"音乐网站\"日志分析\n\n## 细化展示\n---\n* 今日访问日志数据指标\n    * 指标数据模块展示部分，从左向右、从上到下指标维度不断细化\n    \n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2zx0cj21hc0u07h0.jpg)\n\n* 当前PV（page view）值统计\n    * 显示周同比、日环比和7日PV均值\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u6f5j20gg062t9h.jpg)\n\n* 7日PV走势\n    * PV值在7日周维度上的拓展\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u8fvj20fk04cgm3.jpg)\n\n* 今日实时PV走势\n    * 每隔30分钟统计前30分钟窗口内PV数据\n    * 是 当前PV统计 在时间维度下的细化\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxf2w57fj20fh0anmyl.jpg)\n\n* 各类目访问统计\n    * 音乐网站首页下属6个子类目\n    * 统计今日当前时间下各类目访问情况\n    * 是 当前PV值统计 在类目维度下的细化\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer2jnaj20fi0bpmz4.jpg)\n\n* 各类目访问趋势\n    * 今日实时PV走势 在类目维度的细化\n    * 亦是 各类目访问统计 在时间维度上的拓展\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer2k6fj20fg0eitcb.jpg)\n\n* Top来源网站统计\n    * 访问日志其他信息统计\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer06woj20f70b6wfk.jpg)\n\n---\n\n* 网站历史数据Review\n    * 可选择日期查看该日相应指标数据\n    * 部分指标与今日数据相同\n    * 增加时间维度对比统计展示\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer7xxnj21hc0u0n8c.jpg)\n\n* 选择日期模块\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer2rfgj20910f4jt9.jpg)\n\n* 30天月PV数据统计\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxeqyzmfj20ff07nq4i.jpg)\n\n---\n* 音乐网站用户行为分析模拟\n    * 设计有音乐播放模块，模拟用户音乐播放、收藏、评论行为\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer21m0j21hc0u0qm6.jpg)\n\n* 音乐模块\n    * 按钮从左到右，模拟音乐播放、收藏和评论\n    * 音乐播放显示全局提示，模拟播放60秒\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxeqw5o6j20700c4ab6.jpg)\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer1oocj219t0ik7ht.jpg)\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3s5u7j204z02a749.jpg)\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxe3sn9zj204x023q2w.jpg)\n\n* 今日歌曲播放数据\n    * 从记录的日志中，通过不同维度展示用户操作歌曲的行为\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3y2z4j21hc0u0al0.jpg)\n\n* 今日歌曲播放数据一览\n    * 根据播放数量排列\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3wuz4j20fi0l8di5.jpg)\n\n* 近一小时播放TOP歌曲\n    * 每隔十分钟统计前一个小时窗口期内的播放数据\n    * 是今日歌曲播放总览的子集\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3umj4j20f30iv75k.jpg)\n\n* 播放音乐类型统计\n    * 根据播放歌曲的类型进行统计\n    * 展示各个类型的流行程度\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3vbvgj20f40el3zz.jpg)\n\n* 今日TOP10歌曲标签云\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3u8lsj20f308uq43.jpg)\n\n* 其他歌曲信息维度的统计\n    * 收藏量统计\n    * 评论量统计\n    * 不放在一起是因为可能有的歌有收藏没有评论\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxe3uzugj20f80mo76a.jpg)\n\n---\n\n## TODO...\n* 推荐？协同过滤？","source":"_posts/Minions-Visualization-first.md","raw":"---\ntitle: 【毕业设计Minions一期】可视化部分展示\ndate: 2019/04/24\ncategories: \n    - 毕业设计\n---\n## 题目\n* 基于Spark Streaming的网站实时流量分析\n\n## 技术栈\n* 前端框架\n    * react\n    * 组件\n        * ant-design\n        * ant-design Pro\n        * BizCharts\n\n* 后端框架\n    * springboot\n\n* 持久化\n    * HBase\n    * MySQL\n\n## 可视化内容概述\n* 可视化展示用户访问日志数据和用户行为日志，本设计具体化为\"音乐网站\"日志分析\n\n## 细化展示\n---\n* 今日访问日志数据指标\n    * 指标数据模块展示部分，从左向右、从上到下指标维度不断细化\n    \n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2zx0cj21hc0u07h0.jpg)\n\n* 当前PV（page view）值统计\n    * 显示周同比、日环比和7日PV均值\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u6f5j20gg062t9h.jpg)\n\n* 7日PV走势\n    * PV值在7日周维度上的拓展\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u8fvj20fk04cgm3.jpg)\n\n* 今日实时PV走势\n    * 每隔30分钟统计前30分钟窗口内PV数据\n    * 是 当前PV统计 在时间维度下的细化\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxf2w57fj20fh0anmyl.jpg)\n\n* 各类目访问统计\n    * 音乐网站首页下属6个子类目\n    * 统计今日当前时间下各类目访问情况\n    * 是 当前PV值统计 在类目维度下的细化\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer2jnaj20fi0bpmz4.jpg)\n\n* 各类目访问趋势\n    * 今日实时PV走势 在类目维度的细化\n    * 亦是 各类目访问统计 在时间维度上的拓展\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer2k6fj20fg0eitcb.jpg)\n\n* Top来源网站统计\n    * 访问日志其他信息统计\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer06woj20f70b6wfk.jpg)\n\n---\n\n* 网站历史数据Review\n    * 可选择日期查看该日相应指标数据\n    * 部分指标与今日数据相同\n    * 增加时间维度对比统计展示\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer7xxnj21hc0u0n8c.jpg)\n\n* 选择日期模块\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer2rfgj20910f4jt9.jpg)\n\n* 30天月PV数据统计\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxeqyzmfj20ff07nq4i.jpg)\n\n---\n* 音乐网站用户行为分析模拟\n    * 设计有音乐播放模块，模拟用户音乐播放、收藏、评论行为\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer21m0j21hc0u0qm6.jpg)\n\n* 音乐模块\n    * 按钮从左到右，模拟音乐播放、收藏和评论\n    * 音乐播放显示全局提示，模拟播放60秒\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxeqw5o6j20700c4ab6.jpg)\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer1oocj219t0ik7ht.jpg)\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3s5u7j204z02a749.jpg)\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxe3sn9zj204x023q2w.jpg)\n\n* 今日歌曲播放数据\n    * 从记录的日志中，通过不同维度展示用户操作歌曲的行为\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3y2z4j21hc0u0al0.jpg)\n\n* 今日歌曲播放数据一览\n    * 根据播放数量排列\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3wuz4j20fi0l8di5.jpg)\n\n* 近一小时播放TOP歌曲\n    * 每隔十分钟统计前一个小时窗口期内的播放数据\n    * 是今日歌曲播放总览的子集\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3umj4j20f30iv75k.jpg)\n\n* 播放音乐类型统计\n    * 根据播放歌曲的类型进行统计\n    * 展示各个类型的流行程度\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3vbvgj20f40el3zz.jpg)\n\n* 今日TOP10歌曲标签云\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3u8lsj20f308uq43.jpg)\n\n* 其他歌曲信息维度的统计\n    * 收藏量统计\n    * 评论量统计\n    * 不放在一起是因为可能有的歌有收藏没有评论\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxe3uzugj20f80mo76a.jpg)\n\n---\n\n## TODO...\n* 推荐？协同过滤？","slug":"Minions-Visualization-first","published":1,"updated":"2019-06-04T01:06:18.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwch000c48uphen7v88y","content":"<h2 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h2><ul>\n<li>基于Spark Streaming的网站实时流量分析</li>\n</ul>\n<h2 id=\"技术栈\"><a href=\"#技术栈\" class=\"headerlink\" title=\"技术栈\"></a>技术栈</h2><ul>\n<li><p>前端框架</p>\n<ul>\n<li>react</li>\n<li>组件<ul>\n<li>ant-design</li>\n<li>ant-design Pro</li>\n<li>BizCharts</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>后端框架</p>\n<ul>\n<li>springboot</li>\n</ul>\n</li>\n<li><p>持久化</p>\n<ul>\n<li>HBase</li>\n<li>MySQL</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"可视化内容概述\"><a href=\"#可视化内容概述\" class=\"headerlink\" title=\"可视化内容概述\"></a>可视化内容概述</h2><ul>\n<li>可视化展示用户访问日志数据和用户行为日志，本设计具体化为”音乐网站”日志分析</li>\n</ul>\n<h2 id=\"细化展示\"><a href=\"#细化展示\" class=\"headerlink\" title=\"细化展示\"></a>细化展示</h2><hr>\n<ul>\n<li>今日访问日志数据指标<ul>\n<li>指标数据模块展示部分，从左向右、从上到下指标维度不断细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2zx0cj21hc0u07h0.jpg\" alt></p>\n<ul>\n<li>当前PV（page view）值统计<ul>\n<li>显示周同比、日环比和7日PV均值</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u6f5j20gg062t9h.jpg\" alt></p>\n<ul>\n<li>7日PV走势<ul>\n<li>PV值在7日周维度上的拓展</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u8fvj20fk04cgm3.jpg\" alt></p>\n<ul>\n<li>今日实时PV走势<ul>\n<li>每隔30分钟统计前30分钟窗口内PV数据</li>\n<li>是 当前PV统计 在时间维度下的细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxf2w57fj20fh0anmyl.jpg\" alt></p>\n<ul>\n<li>各类目访问统计<ul>\n<li>音乐网站首页下属6个子类目</li>\n<li>统计今日当前时间下各类目访问情况</li>\n<li>是 当前PV值统计 在类目维度下的细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer2jnaj20fi0bpmz4.jpg\" alt></p>\n<ul>\n<li>各类目访问趋势<ul>\n<li>今日实时PV走势 在类目维度的细化</li>\n<li>亦是 各类目访问统计 在时间维度上的拓展</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer2k6fj20fg0eitcb.jpg\" alt></p>\n<ul>\n<li>Top来源网站统计<ul>\n<li>访问日志其他信息统计</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer06woj20f70b6wfk.jpg\" alt></p>\n<hr>\n<ul>\n<li>网站历史数据Review<ul>\n<li>可选择日期查看该日相应指标数据</li>\n<li>部分指标与今日数据相同</li>\n<li>增加时间维度对比统计展示</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer7xxnj21hc0u0n8c.jpg\" alt></p>\n<ul>\n<li>选择日期模块</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer2rfgj20910f4jt9.jpg\" alt></p>\n<ul>\n<li>30天月PV数据统计</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxeqyzmfj20ff07nq4i.jpg\" alt></p>\n<hr>\n<ul>\n<li>音乐网站用户行为分析模拟<ul>\n<li>设计有音乐播放模块，模拟用户音乐播放、收藏、评论行为</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer21m0j21hc0u0qm6.jpg\" alt></p>\n<ul>\n<li>音乐模块<ul>\n<li>按钮从左到右，模拟音乐播放、收藏和评论</li>\n<li>音乐播放显示全局提示，模拟播放60秒</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxeqw5o6j20700c4ab6.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer1oocj219t0ik7ht.jpg\" alt></p>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3s5u7j204z02a749.jpg\" alt></p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxe3sn9zj204x023q2w.jpg\" alt></p>\n<ul>\n<li>今日歌曲播放数据<ul>\n<li>从记录的日志中，通过不同维度展示用户操作歌曲的行为</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3y2z4j21hc0u0al0.jpg\" alt></p>\n<ul>\n<li>今日歌曲播放数据一览<ul>\n<li>根据播放数量排列</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3wuz4j20fi0l8di5.jpg\" alt></p>\n<ul>\n<li>近一小时播放TOP歌曲<ul>\n<li>每隔十分钟统计前一个小时窗口期内的播放数据</li>\n<li>是今日歌曲播放总览的子集</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3umj4j20f30iv75k.jpg\" alt></p>\n<ul>\n<li>播放音乐类型统计<ul>\n<li>根据播放歌曲的类型进行统计</li>\n<li>展示各个类型的流行程度</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3vbvgj20f40el3zz.jpg\" alt></p>\n<ul>\n<li>今日TOP10歌曲标签云</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3u8lsj20f308uq43.jpg\" alt></p>\n<ul>\n<li>其他歌曲信息维度的统计<ul>\n<li>收藏量统计</li>\n<li>评论量统计</li>\n<li>不放在一起是因为可能有的歌有收藏没有评论</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxe3uzugj20f80mo76a.jpg\" alt></p>\n<hr>\n<h2 id=\"TODO…\"><a href=\"#TODO…\" class=\"headerlink\" title=\"TODO…\"></a>TODO…</h2><ul>\n<li>推荐？协同过滤？</li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h2><ul>\n<li>基于Spark Streaming的网站实时流量分析</li>\n</ul>\n<h2 id=\"技术栈\"><a href=\"#技术栈\" class=\"headerlink\" title=\"技术栈\"></a>技术栈</h2><ul>\n<li><p>前端框架</p>\n<ul>\n<li>react</li>\n<li>组件<ul>\n<li>ant-design</li>\n<li>ant-design Pro</li>\n<li>BizCharts</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>后端框架</p>\n<ul>\n<li>springboot</li>\n</ul>\n</li>\n<li><p>持久化</p>\n<ul>\n<li>HBase</li>\n<li>MySQL</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"可视化内容概述\"><a href=\"#可视化内容概述\" class=\"headerlink\" title=\"可视化内容概述\"></a>可视化内容概述</h2><ul>\n<li>可视化展示用户访问日志数据和用户行为日志，本设计具体化为”音乐网站”日志分析</li>\n</ul>\n<h2 id=\"细化展示\"><a href=\"#细化展示\" class=\"headerlink\" title=\"细化展示\"></a>细化展示</h2><hr>\n<ul>\n<li>今日访问日志数据指标<ul>\n<li>指标数据模块展示部分，从左向右、从上到下指标维度不断细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2zx0cj21hc0u07h0.jpg\" alt></p>\n<ul>\n<li>当前PV（page view）值统计<ul>\n<li>显示周同比、日环比和7日PV均值</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u6f5j20gg062t9h.jpg\" alt></p>\n<ul>\n<li>7日PV走势<ul>\n<li>PV值在7日周维度上的拓展</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxf2u8fvj20fk04cgm3.jpg\" alt></p>\n<ul>\n<li>今日实时PV走势<ul>\n<li>每隔30分钟统计前30分钟窗口内PV数据</li>\n<li>是 当前PV统计 在时间维度下的细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxf2w57fj20fh0anmyl.jpg\" alt></p>\n<ul>\n<li>各类目访问统计<ul>\n<li>音乐网站首页下属6个子类目</li>\n<li>统计今日当前时间下各类目访问情况</li>\n<li>是 当前PV值统计 在类目维度下的细化</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer2jnaj20fi0bpmz4.jpg\" alt></p>\n<ul>\n<li>各类目访问趋势<ul>\n<li>今日实时PV走势 在类目维度的细化</li>\n<li>亦是 各类目访问统计 在时间维度上的拓展</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer2k6fj20fg0eitcb.jpg\" alt></p>\n<ul>\n<li>Top来源网站统计<ul>\n<li>访问日志其他信息统计</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxer06woj20f70b6wfk.jpg\" alt></p>\n<hr>\n<ul>\n<li>网站历史数据Review<ul>\n<li>可选择日期查看该日相应指标数据</li>\n<li>部分指标与今日数据相同</li>\n<li>增加时间维度对比统计展示</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer7xxnj21hc0u0n8c.jpg\" alt></p>\n<ul>\n<li>选择日期模块</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxer2rfgj20910f4jt9.jpg\" alt></p>\n<ul>\n<li>30天月PV数据统计</li>\n</ul>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxeqyzmfj20ff07nq4i.jpg\" alt></p>\n<hr>\n<ul>\n<li>音乐网站用户行为分析模拟<ul>\n<li>设计有音乐播放模块，模拟用户音乐播放、收藏、评论行为</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer21m0j21hc0u0qm6.jpg\" alt></p>\n<ul>\n<li>音乐模块<ul>\n<li>按钮从左到右，模拟音乐播放、收藏和评论</li>\n<li>音乐播放显示全局提示，模拟播放60秒</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxeqw5o6j20700c4ab6.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxer1oocj219t0ik7ht.jpg\" alt></p>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3s5u7j204z02a749.jpg\" alt></p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g2dxe3sn9zj204x023q2w.jpg\" alt></p>\n<ul>\n<li>今日歌曲播放数据<ul>\n<li>从记录的日志中，通过不同维度展示用户操作歌曲的行为</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3y2z4j21hc0u0al0.jpg\" alt></p>\n<ul>\n<li>今日歌曲播放数据一览<ul>\n<li>根据播放数量排列</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3wuz4j20fi0l8di5.jpg\" alt></p>\n<ul>\n<li>近一小时播放TOP歌曲<ul>\n<li>每隔十分钟统计前一个小时窗口期内的播放数据</li>\n<li>是今日歌曲播放总览的子集</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2dxe3umj4j20f30iv75k.jpg\" alt></p>\n<ul>\n<li>播放音乐类型统计<ul>\n<li>根据播放歌曲的类型进行统计</li>\n<li>展示各个类型的流行程度</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3vbvgj20f40el3zz.jpg\" alt></p>\n<ul>\n<li>今日TOP10歌曲标签云</li>\n</ul>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2dxe3u8lsj20f308uq43.jpg\" alt></p>\n<ul>\n<li>其他歌曲信息维度的统计<ul>\n<li>收藏量统计</li>\n<li>评论量统计</li>\n<li>不放在一起是因为可能有的歌有收藏没有评论</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g2dxe3uzugj20f80mo76a.jpg\" alt></p>\n<hr>\n<h2 id=\"TODO…\"><a href=\"#TODO…\" class=\"headerlink\" title=\"TODO…\"></a>TODO…</h2><ul>\n<li>推荐？协同过滤？</li>\n</ul>\n"},{"title":"毕业设计随想","date":"2019-06-05T16:00:00.000Z","_content":"今天终于把毕设相关的材料签名等等都弄好了，感觉应该要结束了。\n\n毕业设计做的是一个实时流处理系统，大体上就是使用各种工具集成出一个系统。\n题目也算是比较新潮，自己之前也没接触过这方面，挺感谢导师给我找这个题目的，让我接触了Spark，以及一些\n有用的工具。期间一直是自我放飞的完成这个毕设，虽然最后答辩的时候被老师怼，说这个系统没啥创新点。\n其实这个问题，我觉得找到自己特定的需求，并且将需求进行分解，每个部分使用最合适的工具或者解决方案去完成，就是创新了。\n当然自己的需求不可能现有的工具就能完全解决，所以需要根据自己的需求对方案进行改进，这样就有一定的创新了。\n\n我感觉在毕设期间，导师没有逼我太紧，一直说做得挺不错的，所以给我一种想法就是，毕设随便做一下就可以了。\n然而在答辩的时候，被老师说毕设没有创新点。所以以后要多方面的了解自己，be humble。\n\n\n","source":"_posts/Graduation-sentiment.md","raw":"---\ntitle: 毕业设计随想\ndate: 2019/06/06\ncategories: \n    - 毕业设计\n---\n今天终于把毕设相关的材料签名等等都弄好了，感觉应该要结束了。\n\n毕业设计做的是一个实时流处理系统，大体上就是使用各种工具集成出一个系统。\n题目也算是比较新潮，自己之前也没接触过这方面，挺感谢导师给我找这个题目的，让我接触了Spark，以及一些\n有用的工具。期间一直是自我放飞的完成这个毕设，虽然最后答辩的时候被老师怼，说这个系统没啥创新点。\n其实这个问题，我觉得找到自己特定的需求，并且将需求进行分解，每个部分使用最合适的工具或者解决方案去完成，就是创新了。\n当然自己的需求不可能现有的工具就能完全解决，所以需要根据自己的需求对方案进行改进，这样就有一定的创新了。\n\n我感觉在毕设期间，导师没有逼我太紧，一直说做得挺不错的，所以给我一种想法就是，毕设随便做一下就可以了。\n然而在答辩的时候，被老师说毕设没有创新点。所以以后要多方面的了解自己，be humble。\n\n\n","slug":"Graduation-sentiment","published":1,"updated":"2019-06-06T12:16:18.566Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwcq000e48up4fimq95r","content":"<p>今天终于把毕设相关的材料签名等等都弄好了，感觉应该要结束了。</p>\n<p>毕业设计做的是一个实时流处理系统，大体上就是使用各种工具集成出一个系统。<br>题目也算是比较新潮，自己之前也没接触过这方面，挺感谢导师给我找这个题目的，让我接触了Spark，以及一些<br>有用的工具。期间一直是自我放飞的完成这个毕设，虽然最后答辩的时候被老师怼，说这个系统没啥创新点。<br>其实这个问题，我觉得找到自己特定的需求，并且将需求进行分解，每个部分使用最合适的工具或者解决方案去完成，就是创新了。<br>当然自己的需求不可能现有的工具就能完全解决，所以需要根据自己的需求对方案进行改进，这样就有一定的创新了。</p>\n<p>我感觉在毕设期间，导师没有逼我太紧，一直说做得挺不错的，所以给我一种想法就是，毕设随便做一下就可以了。<br>然而在答辩的时候，被老师说毕设没有创新点。所以以后要多方面的了解自己，be humble。</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p>今天终于把毕设相关的材料签名等等都弄好了，感觉应该要结束了。</p>\n<p>毕业设计做的是一个实时流处理系统，大体上就是使用各种工具集成出一个系统。<br>题目也算是比较新潮，自己之前也没接触过这方面，挺感谢导师给我找这个题目的，让我接触了Spark，以及一些<br>有用的工具。期间一直是自我放飞的完成这个毕设，虽然最后答辩的时候被老师怼，说这个系统没啥创新点。<br>其实这个问题，我觉得找到自己特定的需求，并且将需求进行分解，每个部分使用最合适的工具或者解决方案去完成，就是创新了。<br>当然自己的需求不可能现有的工具就能完全解决，所以需要根据自己的需求对方案进行改进，这样就有一定的创新了。</p>\n<p>我感觉在毕设期间，导师没有逼我太紧，一直说做得挺不错的，所以给我一种想法就是，毕设随便做一下就可以了。<br>然而在答辩的时候，被老师说毕设没有创新点。所以以后要多方面的了解自己，be humble。</p>\n"},{"title":"Kadane算法","date":"2019-05-26T16:00:00.000Z","_content":"## context\n[53. Maximum Subarray](https://leetcode.com/problems/maximum-subarray/)\n[121. Best Time to Buy and Sell Stock](https://leetcode.com/problems/best-time-to-buy-and-sell-stock/discuss/?currentPage=1&orderBy=most_posts&query=)\n\n## 最大子数组之和\n\nalgorithm that operates on arrays: it starts at the left end (element A[1]) and scans through to the right end (element A[n]), keeping track of the maximum sum subvector seen so far. The maximum is initially A[0]. Suppose we've solved the problem for A[1 .. i - 1]; how can we extend that to A[1 .. i]? The maximum\nsum in the first I elements is either the maximum sum in the first i - 1 elements (which we'll call MaxSoFar), or it is that of a subvector that ends in position i (which we'll call MaxEndingHere).\nMaxEndingHere is either A[i] plus the previous MaxEndingHere, or just A[i], whichever is larger.\n\n```java\npublic static int maxSubArray(int[] A) {\n    int maxSoFar=A[0], maxEndingHere=A[0];\n    for (int i=1;i<A.length;++i){\n    \tmaxEndingHere= Math.max(maxEndingHere+A[i],A[i]);\n    \tmaxSoFar=Math.max(maxSoFar, maxEndingHere);\t\n    }\n    return maxSoFar;\n}\n```\n\n## 最大利润\n\n最大利润是一段时间的利润总和，如果我们知道了每天相对于前一天的利润，也就是每天卖出一次，再买进一次。\n计算出一个表，表中的每一项为prices[i] - prices[i - 1]。\n\n求这个以天为维度的利润序列的最大子数组之和（用上面一题的算法），就是最大利润。\n\n```java\npublic class Solution {\n    public int maxProfit(int[] prices) {\n        int max_ending_here, max_so_far;\n        max_ending_here = max_so_far =0;\n        for(int n = 1; n < prices.length; n++){\n            max_ending_here = Math.max(max_ending_here + prices[n] - prices[n-1], prices[n] - prices[n-1]);\n            max_so_far = Math.max(max_ending_here, max_so_far);\n        }\n        return max_so_far;\n        }\n    }\n```\n\n将上面的流程精简\n```java\npublic int maxProfit(int[] prices) {\n        int maxCur = 0, maxSoFar = 0;\n        for(int i = 1; i < prices.length; i++) {\n            maxCur = Math.max(0, maxCur += prices[i] - prices[i-1]);\n            maxSoFar = Math.max(maxCur, maxSoFar);\n        }\n        return maxSoFar;\n    }\n```\nmaxCur是当前一段时间的最大利润，当利润降为0时，就不再计算这个时间段的利润，重新开始一个新的序列\nmaxSoFar就是所有天中，利润最大的时间序列的利润值\n\n### 解释\nA more clear explanation on why sum of subarray works.:\n\nSuppose we have original array:\n[a0, a1, a2, a3, a4, a5, a6]\n\nwhat we are given here(or we calculate ourselves) is:\n[b0, b1, b2, b3, b4, b5, b6]\n\nwhere,\nb[i] = 0, when i == 0\nb[i] = a[i] - a[i - 1], when i != 0\n\nsuppose if a2 and a6 are the points that give us the max difference (a2 < a6)\nthen in our given array, we need to find the sum of sub array from b3 to b6.\n\nb3 = a3 - a2\nb4 = a4 - a3\nb5 = a5 - a4\nb6 = a6 - a5\n\nadding all these, all the middle terms will cancel out except two\ni.e.\n\nb3 + b4 + b5 + b6 = a6 - a2\n\na6 - a2 is the required solution.\n\nso we need to find the largest sub array sum to get the most profit\n\n\n\n","source":"_posts/Kadane's-Algorithm.md","raw":"---\ntitle: Kadane算法\ndate: 2019/05/27\ncategories: \n    - 算法\n---\n## context\n[53. Maximum Subarray](https://leetcode.com/problems/maximum-subarray/)\n[121. Best Time to Buy and Sell Stock](https://leetcode.com/problems/best-time-to-buy-and-sell-stock/discuss/?currentPage=1&orderBy=most_posts&query=)\n\n## 最大子数组之和\n\nalgorithm that operates on arrays: it starts at the left end (element A[1]) and scans through to the right end (element A[n]), keeping track of the maximum sum subvector seen so far. The maximum is initially A[0]. Suppose we've solved the problem for A[1 .. i - 1]; how can we extend that to A[1 .. i]? The maximum\nsum in the first I elements is either the maximum sum in the first i - 1 elements (which we'll call MaxSoFar), or it is that of a subvector that ends in position i (which we'll call MaxEndingHere).\nMaxEndingHere is either A[i] plus the previous MaxEndingHere, or just A[i], whichever is larger.\n\n```java\npublic static int maxSubArray(int[] A) {\n    int maxSoFar=A[0], maxEndingHere=A[0];\n    for (int i=1;i<A.length;++i){\n    \tmaxEndingHere= Math.max(maxEndingHere+A[i],A[i]);\n    \tmaxSoFar=Math.max(maxSoFar, maxEndingHere);\t\n    }\n    return maxSoFar;\n}\n```\n\n## 最大利润\n\n最大利润是一段时间的利润总和，如果我们知道了每天相对于前一天的利润，也就是每天卖出一次，再买进一次。\n计算出一个表，表中的每一项为prices[i] - prices[i - 1]。\n\n求这个以天为维度的利润序列的最大子数组之和（用上面一题的算法），就是最大利润。\n\n```java\npublic class Solution {\n    public int maxProfit(int[] prices) {\n        int max_ending_here, max_so_far;\n        max_ending_here = max_so_far =0;\n        for(int n = 1; n < prices.length; n++){\n            max_ending_here = Math.max(max_ending_here + prices[n] - prices[n-1], prices[n] - prices[n-1]);\n            max_so_far = Math.max(max_ending_here, max_so_far);\n        }\n        return max_so_far;\n        }\n    }\n```\n\n将上面的流程精简\n```java\npublic int maxProfit(int[] prices) {\n        int maxCur = 0, maxSoFar = 0;\n        for(int i = 1; i < prices.length; i++) {\n            maxCur = Math.max(0, maxCur += prices[i] - prices[i-1]);\n            maxSoFar = Math.max(maxCur, maxSoFar);\n        }\n        return maxSoFar;\n    }\n```\nmaxCur是当前一段时间的最大利润，当利润降为0时，就不再计算这个时间段的利润，重新开始一个新的序列\nmaxSoFar就是所有天中，利润最大的时间序列的利润值\n\n### 解释\nA more clear explanation on why sum of subarray works.:\n\nSuppose we have original array:\n[a0, a1, a2, a3, a4, a5, a6]\n\nwhat we are given here(or we calculate ourselves) is:\n[b0, b1, b2, b3, b4, b5, b6]\n\nwhere,\nb[i] = 0, when i == 0\nb[i] = a[i] - a[i - 1], when i != 0\n\nsuppose if a2 and a6 are the points that give us the max difference (a2 < a6)\nthen in our given array, we need to find the sum of sub array from b3 to b6.\n\nb3 = a3 - a2\nb4 = a4 - a3\nb5 = a5 - a4\nb6 = a6 - a5\n\nadding all these, all the middle terms will cancel out except two\ni.e.\n\nb3 + b4 + b5 + b6 = a6 - a2\n\na6 - a2 is the required solution.\n\nso we need to find the largest sub array sum to get the most profit\n\n\n\n","slug":"Kadane's-Algorithm","published":1,"updated":"2019-06-04T01:06:18.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwcx000h48upxz1if1ym","content":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p><a href=\"https://leetcode.com/problems/maximum-subarray/\" target=\"_blank\" rel=\"noopener\">53. Maximum Subarray</a><br><a href=\"https://leetcode.com/problems/best-time-to-buy-and-sell-stock/discuss/?currentPage=1&amp;orderBy=most_posts&amp;query=\" target=\"_blank\" rel=\"noopener\">121. Best Time to Buy and Sell Stock</a></p>\n<h2 id=\"最大子数组之和\"><a href=\"#最大子数组之和\" class=\"headerlink\" title=\"最大子数组之和\"></a>最大子数组之和</h2><p>algorithm that operates on arrays: it starts at the left end (element A[1]) and scans through to the right end (element A[n]), keeping track of the maximum sum subvector seen so far. The maximum is initially A[0]. Suppose we’ve solved the problem for A[1 .. i - 1]; how can we extend that to A[1 .. i]? The maximum<br>sum in the first I elements is either the maximum sum in the first i - 1 elements (which we’ll call MaxSoFar), or it is that of a subvector that ends in position i (which we’ll call MaxEndingHere).<br>MaxEndingHere is either A[i] plus the previous MaxEndingHere, or just A[i], whichever is larger.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span> <span class=\"title\">maxSubArray</span><span class=\"params\">(<span class=\"keyword\">int</span>[] A)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> maxSoFar=A[<span class=\"number\">0</span>], maxEndingHere=A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;A.length;++i)&#123;</span><br><span class=\"line\">    \tmaxEndingHere= Math.max(maxEndingHere+A[i],A[i]);</span><br><span class=\"line\">    \tmaxSoFar=Math.max(maxSoFar, maxEndingHere);\t</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> maxSoFar;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"最大利润\"><a href=\"#最大利润\" class=\"headerlink\" title=\"最大利润\"></a>最大利润</h2><p>最大利润是一段时间的利润总和，如果我们知道了每天相对于前一天的利润，也就是每天卖出一次，再买进一次。<br>计算出一个表，表中的每一项为prices[i] - prices[i - 1]。</p>\n<p>求这个以天为维度的利润序列的最大子数组之和（用上面一题的算法），就是最大利润。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">maxProfit</span><span class=\"params\">(<span class=\"keyword\">int</span>[] prices)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> max_ending_here, max_so_far;</span><br><span class=\"line\">        max_ending_here = max_so_far =<span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> n = <span class=\"number\">1</span>; n &lt; prices.length; n++)&#123;</span><br><span class=\"line\">            max_ending_here = Math.max(max_ending_here + prices[n] - prices[n-<span class=\"number\">1</span>], prices[n] - prices[n-<span class=\"number\">1</span>]);</span><br><span class=\"line\">            max_so_far = Math.max(max_ending_here, max_so_far);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max_so_far;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>将上面的流程精简<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">maxProfit</span><span class=\"params\">(<span class=\"keyword\">int</span>[] prices)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> maxCur = <span class=\"number\">0</span>, maxSoFar = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; prices.length; i++) &#123;</span><br><span class=\"line\">            maxCur = Math.max(<span class=\"number\">0</span>, maxCur += prices[i] - prices[i-<span class=\"number\">1</span>]);</span><br><span class=\"line\">            maxSoFar = Math.max(maxCur, maxSoFar);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxSoFar;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>maxCur是当前一段时间的最大利润，当利润降为0时，就不再计算这个时间段的利润，重新开始一个新的序列<br>maxSoFar就是所有天中，利润最大的时间序列的利润值</p>\n<h3 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h3><p>A more clear explanation on why sum of subarray works.:</p>\n<p>Suppose we have original array:<br>[a0, a1, a2, a3, a4, a5, a6]</p>\n<p>what we are given here(or we calculate ourselves) is:<br>[b0, b1, b2, b3, b4, b5, b6]</p>\n<p>where,<br>b[i] = 0, when i == 0<br>b[i] = a[i] - a[i - 1], when i != 0</p>\n<p>suppose if a2 and a6 are the points that give us the max difference (a2 &lt; a6)<br>then in our given array, we need to find the sum of sub array from b3 to b6.</p>\n<p>b3 = a3 - a2<br>b4 = a4 - a3<br>b5 = a5 - a4<br>b6 = a6 - a5</p>\n<p>adding all these, all the middle terms will cancel out except two<br>i.e.</p>\n<p>b3 + b4 + b5 + b6 = a6 - a2</p>\n<p>a6 - a2 is the required solution.</p>\n<p>so we need to find the largest sub array sum to get the most profit</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p><a href=\"https://leetcode.com/problems/maximum-subarray/\" target=\"_blank\" rel=\"noopener\">53. Maximum Subarray</a><br><a href=\"https://leetcode.com/problems/best-time-to-buy-and-sell-stock/discuss/?currentPage=1&amp;orderBy=most_posts&amp;query=\" target=\"_blank\" rel=\"noopener\">121. Best Time to Buy and Sell Stock</a></p>\n<h2 id=\"最大子数组之和\"><a href=\"#最大子数组之和\" class=\"headerlink\" title=\"最大子数组之和\"></a>最大子数组之和</h2><p>algorithm that operates on arrays: it starts at the left end (element A[1]) and scans through to the right end (element A[n]), keeping track of the maximum sum subvector seen so far. The maximum is initially A[0]. Suppose we’ve solved the problem for A[1 .. i - 1]; how can we extend that to A[1 .. i]? The maximum<br>sum in the first I elements is either the maximum sum in the first i - 1 elements (which we’ll call MaxSoFar), or it is that of a subvector that ends in position i (which we’ll call MaxEndingHere).<br>MaxEndingHere is either A[i] plus the previous MaxEndingHere, or just A[i], whichever is larger.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span> <span class=\"title\">maxSubArray</span><span class=\"params\">(<span class=\"keyword\">int</span>[] A)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> maxSoFar=A[<span class=\"number\">0</span>], maxEndingHere=A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;A.length;++i)&#123;</span><br><span class=\"line\">    \tmaxEndingHere= Math.max(maxEndingHere+A[i],A[i]);</span><br><span class=\"line\">    \tmaxSoFar=Math.max(maxSoFar, maxEndingHere);\t</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> maxSoFar;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"最大利润\"><a href=\"#最大利润\" class=\"headerlink\" title=\"最大利润\"></a>最大利润</h2><p>最大利润是一段时间的利润总和，如果我们知道了每天相对于前一天的利润，也就是每天卖出一次，再买进一次。<br>计算出一个表，表中的每一项为prices[i] - prices[i - 1]。</p>\n<p>求这个以天为维度的利润序列的最大子数组之和（用上面一题的算法），就是最大利润。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">maxProfit</span><span class=\"params\">(<span class=\"keyword\">int</span>[] prices)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> max_ending_here, max_so_far;</span><br><span class=\"line\">        max_ending_here = max_so_far =<span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> n = <span class=\"number\">1</span>; n &lt; prices.length; n++)&#123;</span><br><span class=\"line\">            max_ending_here = Math.max(max_ending_here + prices[n] - prices[n-<span class=\"number\">1</span>], prices[n] - prices[n-<span class=\"number\">1</span>]);</span><br><span class=\"line\">            max_so_far = Math.max(max_ending_here, max_so_far);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> max_so_far;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>将上面的流程精简<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">maxProfit</span><span class=\"params\">(<span class=\"keyword\">int</span>[] prices)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> maxCur = <span class=\"number\">0</span>, maxSoFar = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; prices.length; i++) &#123;</span><br><span class=\"line\">            maxCur = Math.max(<span class=\"number\">0</span>, maxCur += prices[i] - prices[i-<span class=\"number\">1</span>]);</span><br><span class=\"line\">            maxSoFar = Math.max(maxCur, maxSoFar);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxSoFar;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>maxCur是当前一段时间的最大利润，当利润降为0时，就不再计算这个时间段的利润，重新开始一个新的序列<br>maxSoFar就是所有天中，利润最大的时间序列的利润值</p>\n<h3 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h3><p>A more clear explanation on why sum of subarray works.:</p>\n<p>Suppose we have original array:<br>[a0, a1, a2, a3, a4, a5, a6]</p>\n<p>what we are given here(or we calculate ourselves) is:<br>[b0, b1, b2, b3, b4, b5, b6]</p>\n<p>where,<br>b[i] = 0, when i == 0<br>b[i] = a[i] - a[i - 1], when i != 0</p>\n<p>suppose if a2 and a6 are the points that give us the max difference (a2 &lt; a6)<br>then in our given array, we need to find the sum of sub array from b3 to b6.</p>\n<p>b3 = a3 - a2<br>b4 = a4 - a3<br>b5 = a5 - a4<br>b6 = a6 - a5</p>\n<p>adding all these, all the middle terms will cancel out except two<br>i.e.</p>\n<p>b3 + b4 + b5 + b6 = a6 - a2</p>\n<p>a6 - a2 is the required solution.</p>\n<p>so we need to find the largest sub array sum to get the most profit</p>\n"},{"title":"学习HBase JAVA API","date":"2019-04-18T16:00:00.000Z","tages":["HBase"],"_content":"## 网站\n* [HBase with Java](https://www.baeldung.com/hbase)\n* [HBase表概念](https://www.bilibili.com/video/av21911076)\n* [Filter 过滤器](https://www.jianshu.com/p/fb4bd7a2a23e)\n\n## 概念\n* Column Family\n    * 列族是列的集合\n    * 表可以有一个或多个列族\n    * 属于同一个列族的所有列前缀相同\n        * containInfo:fname\n        * containInfo:lname\n    * 表按照列族维度进行存储，一个列族的数据存在一起\n    * 对一个列族可以单独定义存储属性调优，比如：一个列族存储图片数据，那么就可以对这个列族进行存储压缩\n    * 一个列族想有多少列都可宜，在进行数据添加操作的时候插入即可。不用修改表定义，在创建表的时候，定好列族就可以\n    *设计原则\n        * 不经常在一起访问的数据分开设计，比如：一个用户信息的表，可以将用户名、真实姓名、花名放在一个列族，将头像放在另一个列族\n        * 使用不同列族存储设置时分开设计\n \n* Columns 列\n    * 列中包含表的数据\n    * 可以动态实时地创建\n    \n* Rows 行\n    * 每一行有一个行键（RowKey)\n    * RowKey类似于关系型数据库的主键\n    * RowKey设计原则：方便scan操作，结合具体业务\n    * 行按照行键排序进行存储，这样就提高了检索效率\n    \n* Cell 单元\n    * 行与列的交叉点\n    * 版本化，支持多个版本，更新之前的数据可以保存（要定义），比如，QQ头像可以看到原来的头像\n    * 内容是不可分割的\n    * 存储的是byte array字节数组，无字段类型。只要能转成byte array的都可以存\n    * 单元为空时，物理上是不存储的\n\n## HBase Java API\n* [HBase Java API 01：基础操作](https://www.jianshu.com/p/3c85060901da)\n* [HBase Java API 02：过滤器](https://www.jianshu.com/p/fb4bd7a2a23e)\n","source":"_posts/StudyHBaseJavaAPI.md","raw":"---\ntitle: 学习HBase JAVA API\ndate: 2019/04/19\ncategories: \n    - 技术总结\ntages:\n    - HBase\n---\n## 网站\n* [HBase with Java](https://www.baeldung.com/hbase)\n* [HBase表概念](https://www.bilibili.com/video/av21911076)\n* [Filter 过滤器](https://www.jianshu.com/p/fb4bd7a2a23e)\n\n## 概念\n* Column Family\n    * 列族是列的集合\n    * 表可以有一个或多个列族\n    * 属于同一个列族的所有列前缀相同\n        * containInfo:fname\n        * containInfo:lname\n    * 表按照列族维度进行存储，一个列族的数据存在一起\n    * 对一个列族可以单独定义存储属性调优，比如：一个列族存储图片数据，那么就可以对这个列族进行存储压缩\n    * 一个列族想有多少列都可宜，在进行数据添加操作的时候插入即可。不用修改表定义，在创建表的时候，定好列族就可以\n    *设计原则\n        * 不经常在一起访问的数据分开设计，比如：一个用户信息的表，可以将用户名、真实姓名、花名放在一个列族，将头像放在另一个列族\n        * 使用不同列族存储设置时分开设计\n \n* Columns 列\n    * 列中包含表的数据\n    * 可以动态实时地创建\n    \n* Rows 行\n    * 每一行有一个行键（RowKey)\n    * RowKey类似于关系型数据库的主键\n    * RowKey设计原则：方便scan操作，结合具体业务\n    * 行按照行键排序进行存储，这样就提高了检索效率\n    \n* Cell 单元\n    * 行与列的交叉点\n    * 版本化，支持多个版本，更新之前的数据可以保存（要定义），比如，QQ头像可以看到原来的头像\n    * 内容是不可分割的\n    * 存储的是byte array字节数组，无字段类型。只要能转成byte array的都可以存\n    * 单元为空时，物理上是不存储的\n\n## HBase Java API\n* [HBase Java API 01：基础操作](https://www.jianshu.com/p/3c85060901da)\n* [HBase Java API 02：过滤器](https://www.jianshu.com/p/fb4bd7a2a23e)\n","slug":"StudyHBaseJavaAPI","published":1,"updated":"2019-06-04T01:06:18.976Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwd6000j48up0kt0aij4","content":"<h2 id=\"网站\"><a href=\"#网站\" class=\"headerlink\" title=\"网站\"></a>网站</h2><ul>\n<li><a href=\"https://www.baeldung.com/hbase\" target=\"_blank\" rel=\"noopener\">HBase with Java</a></li>\n<li><a href=\"https://www.bilibili.com/video/av21911076\" target=\"_blank\" rel=\"noopener\">HBase表概念</a></li>\n<li><a href=\"https://www.jianshu.com/p/fb4bd7a2a23e\" target=\"_blank\" rel=\"noopener\">Filter 过滤器</a></li>\n</ul>\n<h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><ul>\n<li><p>Column Family</p>\n<ul>\n<li>列族是列的集合</li>\n<li>表可以有一个或多个列族</li>\n<li>属于同一个列族的所有列前缀相同<ul>\n<li>containInfo:fname</li>\n<li>containInfo:lname</li>\n</ul>\n</li>\n<li>表按照列族维度进行存储，一个列族的数据存在一起</li>\n<li>对一个列族可以单独定义存储属性调优，比如：一个列族存储图片数据，那么就可以对这个列族进行存储压缩</li>\n<li>一个列族想有多少列都可宜，在进行数据添加操作的时候插入即可。不用修改表定义，在创建表的时候，定好列族就可以<br>*设计原则<ul>\n<li>不经常在一起访问的数据分开设计，比如：一个用户信息的表，可以将用户名、真实姓名、花名放在一个列族，将头像放在另一个列族</li>\n<li>使用不同列族存储设置时分开设计</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Columns 列</p>\n<ul>\n<li>列中包含表的数据</li>\n<li>可以动态实时地创建</li>\n</ul>\n</li>\n<li><p>Rows 行</p>\n<ul>\n<li>每一行有一个行键（RowKey)</li>\n<li>RowKey类似于关系型数据库的主键</li>\n<li>RowKey设计原则：方便scan操作，结合具体业务</li>\n<li>行按照行键排序进行存储，这样就提高了检索效率</li>\n</ul>\n</li>\n<li><p>Cell 单元</p>\n<ul>\n<li>行与列的交叉点</li>\n<li>版本化，支持多个版本，更新之前的数据可以保存（要定义），比如，QQ头像可以看到原来的头像</li>\n<li>内容是不可分割的</li>\n<li>存储的是byte array字节数组，无字段类型。只要能转成byte array的都可以存</li>\n<li>单元为空时，物理上是不存储的</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"HBase-Java-API\"><a href=\"#HBase-Java-API\" class=\"headerlink\" title=\"HBase Java API\"></a>HBase Java API</h2><ul>\n<li><a href=\"https://www.jianshu.com/p/3c85060901da\" target=\"_blank\" rel=\"noopener\">HBase Java API 01：基础操作</a></li>\n<li><a href=\"https://www.jianshu.com/p/fb4bd7a2a23e\" target=\"_blank\" rel=\"noopener\">HBase Java API 02：过滤器</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"网站\"><a href=\"#网站\" class=\"headerlink\" title=\"网站\"></a>网站</h2><ul>\n<li><a href=\"https://www.baeldung.com/hbase\" target=\"_blank\" rel=\"noopener\">HBase with Java</a></li>\n<li><a href=\"https://www.bilibili.com/video/av21911076\" target=\"_blank\" rel=\"noopener\">HBase表概念</a></li>\n<li><a href=\"https://www.jianshu.com/p/fb4bd7a2a23e\" target=\"_blank\" rel=\"noopener\">Filter 过滤器</a></li>\n</ul>\n<h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><ul>\n<li><p>Column Family</p>\n<ul>\n<li>列族是列的集合</li>\n<li>表可以有一个或多个列族</li>\n<li>属于同一个列族的所有列前缀相同<ul>\n<li>containInfo:fname</li>\n<li>containInfo:lname</li>\n</ul>\n</li>\n<li>表按照列族维度进行存储，一个列族的数据存在一起</li>\n<li>对一个列族可以单独定义存储属性调优，比如：一个列族存储图片数据，那么就可以对这个列族进行存储压缩</li>\n<li>一个列族想有多少列都可宜，在进行数据添加操作的时候插入即可。不用修改表定义，在创建表的时候，定好列族就可以<br>*设计原则<ul>\n<li>不经常在一起访问的数据分开设计，比如：一个用户信息的表，可以将用户名、真实姓名、花名放在一个列族，将头像放在另一个列族</li>\n<li>使用不同列族存储设置时分开设计</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Columns 列</p>\n<ul>\n<li>列中包含表的数据</li>\n<li>可以动态实时地创建</li>\n</ul>\n</li>\n<li><p>Rows 行</p>\n<ul>\n<li>每一行有一个行键（RowKey)</li>\n<li>RowKey类似于关系型数据库的主键</li>\n<li>RowKey设计原则：方便scan操作，结合具体业务</li>\n<li>行按照行键排序进行存储，这样就提高了检索效率</li>\n</ul>\n</li>\n<li><p>Cell 单元</p>\n<ul>\n<li>行与列的交叉点</li>\n<li>版本化，支持多个版本，更新之前的数据可以保存（要定义），比如，QQ头像可以看到原来的头像</li>\n<li>内容是不可分割的</li>\n<li>存储的是byte array字节数组，无字段类型。只要能转成byte array的都可以存</li>\n<li>单元为空时，物理上是不存储的</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"HBase-Java-API\"><a href=\"#HBase-Java-API\" class=\"headerlink\" title=\"HBase Java API\"></a>HBase Java API</h2><ul>\n<li><a href=\"https://www.jianshu.com/p/3c85060901da\" target=\"_blank\" rel=\"noopener\">HBase Java API 01：基础操作</a></li>\n<li><a href=\"https://www.jianshu.com/p/fb4bd7a2a23e\" target=\"_blank\" rel=\"noopener\">HBase Java API 02：过滤器</a></li>\n</ul>\n"},{"title":"关于axios的CORS问题的解决","date":"2019-03-17T16:00:00.000Z","_content":"## Axios基础\n* 参考网站\n\t* [中文文档](https://www.jianshu.com/p/7a9fbcbb1114)\n\t* [使用说明](https://www.kancloud.cn/yunye/axios/234845)\n* 上下文知识\n\t* [Promise in ES6](http://es6.ruanyifeng.com/#docs/promise)\n\t* [浏览器同源政策及其规避方法](http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html) 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据\n\t* [跨域资源共享 CORS 详解](http://www.ruanyifeng.com/blog/2016/04/cors.html) 实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信\n\n## 问题描述\nAccess to XMLHttpRequest at 'http://localhost:8080/hello' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\n## 解决方法\n* [Spring利用@CrossOrigin注解 实现 支持CORS跨域请求](https://my.oschina.net/hccake/blog/886606) 在controller类头部加入注释\n\n```java\n@CrossOrigin(origins = \"*\", maxAge = 3600)\n```","source":"_posts/axioscorsquestions.md","raw":"---\ntitle: 关于axios的CORS问题的解决\ndate: 2019/3/18\ncategories:\n    - 技术总结\ntags: \n    - axios\n    - CORS\n---\n## Axios基础\n* 参考网站\n\t* [中文文档](https://www.jianshu.com/p/7a9fbcbb1114)\n\t* [使用说明](https://www.kancloud.cn/yunye/axios/234845)\n* 上下文知识\n\t* [Promise in ES6](http://es6.ruanyifeng.com/#docs/promise)\n\t* [浏览器同源政策及其规避方法](http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html) 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据\n\t* [跨域资源共享 CORS 详解](http://www.ruanyifeng.com/blog/2016/04/cors.html) 实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信\n\n## 问题描述\nAccess to XMLHttpRequest at 'http://localhost:8080/hello' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\n## 解决方法\n* [Spring利用@CrossOrigin注解 实现 支持CORS跨域请求](https://my.oschina.net/hccake/blog/886606) 在controller类头部加入注释\n\n```java\n@CrossOrigin(origins = \"*\", maxAge = 3600)\n```","slug":"axioscorsquestions","published":1,"updated":"2019-06-04T01:06:18.976Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwdi000l48upmu5ixpaa","content":"<h2 id=\"Axios基础\"><a href=\"#Axios基础\" class=\"headerlink\" title=\"Axios基础\"></a>Axios基础</h2><ul>\n<li>参考网站<ul>\n<li><a href=\"https://www.jianshu.com/p/7a9fbcbb1114\" target=\"_blank\" rel=\"noopener\">中文文档</a></li>\n<li><a href=\"https://www.kancloud.cn/yunye/axios/234845\" target=\"_blank\" rel=\"noopener\">使用说明</a></li>\n</ul>\n</li>\n<li>上下文知识<ul>\n<li><a href=\"http://es6.ruanyifeng.com/#docs/promise\" target=\"_blank\" rel=\"noopener\">Promise in ES6</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html\" target=\"_blank\" rel=\"noopener\">浏览器同源政策及其规避方法</a> 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据</li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/04/cors.html\" target=\"_blank\" rel=\"noopener\">跨域资源共享 CORS 详解</a> 实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>Access to XMLHttpRequest at ‘<a href=\"http://localhost:8080/hello&#39;\" target=\"_blank\" rel=\"noopener\">http://localhost:8080/hello&#39;</a> from origin ‘<a href=\"http://localhost:3000&#39;\" target=\"_blank\" rel=\"noopener\">http://localhost:3000&#39;</a> has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource.</p>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><ul>\n<li><a href=\"https://my.oschina.net/hccake/blog/886606\" target=\"_blank\" rel=\"noopener\">Spring利用@CrossOrigin注解 实现 支持CORS跨域请求</a> 在controller类头部加入注释</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@CrossOrigin</span>(origins = <span class=\"string\">\"*\"</span>, maxAge = <span class=\"number\">3600</span>)</span><br></pre></td></tr></table></figure>","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"Axios基础\"><a href=\"#Axios基础\" class=\"headerlink\" title=\"Axios基础\"></a>Axios基础</h2><ul>\n<li>参考网站<ul>\n<li><a href=\"https://www.jianshu.com/p/7a9fbcbb1114\" target=\"_blank\" rel=\"noopener\">中文文档</a></li>\n<li><a href=\"https://www.kancloud.cn/yunye/axios/234845\" target=\"_blank\" rel=\"noopener\">使用说明</a></li>\n</ul>\n</li>\n<li>上下文知识<ul>\n<li><a href=\"http://es6.ruanyifeng.com/#docs/promise\" target=\"_blank\" rel=\"noopener\">Promise in ES6</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html\" target=\"_blank\" rel=\"noopener\">浏览器同源政策及其规避方法</a> 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据</li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/04/cors.html\" target=\"_blank\" rel=\"noopener\">跨域资源共享 CORS 详解</a> 实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>Access to XMLHttpRequest at ‘<a href=\"http://localhost:8080/hello&#39;\" target=\"_blank\" rel=\"noopener\">http://localhost:8080/hello&#39;</a> from origin ‘<a href=\"http://localhost:3000&#39;\" target=\"_blank\" rel=\"noopener\">http://localhost:3000&#39;</a> has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource.</p>\n<h2 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h2><ul>\n<li><a href=\"https://my.oschina.net/hccake/blog/886606\" target=\"_blank\" rel=\"noopener\">Spring利用@CrossOrigin注解 实现 支持CORS跨域请求</a> 在controller类头部加入注释</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@CrossOrigin</span>(origins = <span class=\"string\">\"*\"</span>, maxAge = <span class=\"number\">3600</span>)</span><br></pre></td></tr></table></figure>"},{"title":"学习KMP","date":"2019-05-02T16:00:00.000Z","_content":"## 引入\n* [Implement strStr()](https://leetcode.com/problems/implement-strstr/)\n    * LeetCode实现String.indexOf()，即返回子串在字符串中第一次出现的索引位置，没有则返回-1，子串为空则返回0\n* 暴力解法\n    * 从0位置开始，比对与子串是否相同，出现不同则在字符串中往后移一位，再次比较子串，直到找到。\n    * 复杂度\n        * 设字符串长度为m，要查找的子串长度为n\n        * 时间 O(m*n)\n    * 问题\n        * 每次出现不一致的情况，只向后移动一位，再从头开始比对，很多没有必要\n        * 如：在aaaaaaaaaaaab,查找aaaab\n\n![](https://puui.qpic.cn/fans_admin/0/3_1500912214_1556864943460/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1486517634_1556864983556/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_308750603_1556864998323/0)\n\n## KMP\n* 概念\n    * KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。\n\n* 对于暴力破解的优化思考\n    * 主串不要每次只移一位，可以让它（i）保持不动，移动模式中j的位置\n    * 利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置\n    \n![](https://puui.qpic.cn/fans_admin/0/3_308750603_1556865032698/0)\n\n* j要怎么移动\n    * 当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的\n\n![](https://puui.qpic.cn/fans_admin/0/3_1486517634_1556865346487/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_771348268_1556865899518/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866125545/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_122902507_1556866189608/0)\n\n* 数学表示\n    * 如果k表示不匹配时，j要移动的位置\n    * P[0 ~ k-1] == P[j-k ~ j-1]\n    \n    ![](https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866470014/0)\n    \n    * 当T[i] != P[j]时\n      \n      有T[i-j ~ i-1] == P[0 ~ j-1]\n      \n      由P[0 ~ k-1] == P[j-k ~ j-1]\n      \n      必然：T[i-k ~ i-1] == P[0 ~ k-1]\n      \n    * 因此可以直接将j移动到k而无须再比较前面的k个字符\n\n* 怎么找到不匹配时j要移动的k位置\n    * 模式串中每个位置发生不匹配时，所要移动到的k位置都是不同的，每个位置都要计算\n    * 使用next数据来保存模式串中每个位置的k值\n    * next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置\n    * 当j为0时，如果这时候不匹配\n        * j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。有next[0] = -1\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869374669/0)\n    \n    * 当j为1\n        * j指针一定是后移到0位置的\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_767595719_1556869496396/0)\n        \n    * 其他\n        * 当P[k] == P[j]时\n        * next[j+1] == next[j] + 1\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869639696/0)\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869662493/0)\n        \n        * P[k] != P[j]\n        * k = next[k]\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869752648/0)\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556870613284/0)\n        \n## 参考\n\n* https://www.cnblogs.com/yjiyjige/p/3263858.html#top\n\n        \n    \n    \n\n    ","source":"_posts/KMP.md","raw":"---\ntitle: 学习KMP\ndate: 2019/05/03\ncategories: \n    - 算法\n---\n## 引入\n* [Implement strStr()](https://leetcode.com/problems/implement-strstr/)\n    * LeetCode实现String.indexOf()，即返回子串在字符串中第一次出现的索引位置，没有则返回-1，子串为空则返回0\n* 暴力解法\n    * 从0位置开始，比对与子串是否相同，出现不同则在字符串中往后移一位，再次比较子串，直到找到。\n    * 复杂度\n        * 设字符串长度为m，要查找的子串长度为n\n        * 时间 O(m*n)\n    * 问题\n        * 每次出现不一致的情况，只向后移动一位，再从头开始比对，很多没有必要\n        * 如：在aaaaaaaaaaaab,查找aaaab\n\n![](https://puui.qpic.cn/fans_admin/0/3_1500912214_1556864943460/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1486517634_1556864983556/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_308750603_1556864998323/0)\n\n## KMP\n* 概念\n    * KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。\n\n* 对于暴力破解的优化思考\n    * 主串不要每次只移一位，可以让它（i）保持不动，移动模式中j的位置\n    * 利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置\n    \n![](https://puui.qpic.cn/fans_admin/0/3_308750603_1556865032698/0)\n\n* j要怎么移动\n    * 当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的\n\n![](https://puui.qpic.cn/fans_admin/0/3_1486517634_1556865346487/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_771348268_1556865899518/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866125545/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_122902507_1556866189608/0)\n\n* 数学表示\n    * 如果k表示不匹配时，j要移动的位置\n    * P[0 ~ k-1] == P[j-k ~ j-1]\n    \n    ![](https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866470014/0)\n    \n    * 当T[i] != P[j]时\n      \n      有T[i-j ~ i-1] == P[0 ~ j-1]\n      \n      由P[0 ~ k-1] == P[j-k ~ j-1]\n      \n      必然：T[i-k ~ i-1] == P[0 ~ k-1]\n      \n    * 因此可以直接将j移动到k而无须再比较前面的k个字符\n\n* 怎么找到不匹配时j要移动的k位置\n    * 模式串中每个位置发生不匹配时，所要移动到的k位置都是不同的，每个位置都要计算\n    * 使用next数据来保存模式串中每个位置的k值\n    * next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置\n    * 当j为0时，如果这时候不匹配\n        * j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。有next[0] = -1\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869374669/0)\n    \n    * 当j为1\n        * j指针一定是后移到0位置的\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_767595719_1556869496396/0)\n        \n    * 其他\n        * 当P[k] == P[j]时\n        * next[j+1] == next[j] + 1\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869639696/0)\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869662493/0)\n        \n        * P[k] != P[j]\n        * k = next[k]\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556869752648/0)\n        \n        ![](https://puui.qpic.cn/fans_admin/0/3_912665407_1556870613284/0)\n        \n## 参考\n\n* https://www.cnblogs.com/yjiyjige/p/3263858.html#top\n\n        \n    \n    \n\n    ","slug":"KMP","published":1,"updated":"2019-06-04T01:06:18.974Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwdo000n48upze803e9e","content":"<h2 id=\"引入\"><a href=\"#引入\" class=\"headerlink\" title=\"引入\"></a>引入</h2><ul>\n<li><a href=\"https://leetcode.com/problems/implement-strstr/\" target=\"_blank\" rel=\"noopener\">Implement strStr()</a><ul>\n<li>LeetCode实现String.indexOf()，即返回子串在字符串中第一次出现的索引位置，没有则返回-1，子串为空则返回0</li>\n</ul>\n</li>\n<li>暴力解法<ul>\n<li>从0位置开始，比对与子串是否相同，出现不同则在字符串中往后移一位，再次比较子串，直到找到。</li>\n<li>复杂度<ul>\n<li>设字符串长度为m，要查找的子串长度为n</li>\n<li>时间 O(m*n)</li>\n</ul>\n</li>\n<li>问题<ul>\n<li>每次出现不一致的情况，只向后移动一位，再从头开始比对，很多没有必要</li>\n<li>如：在aaaaaaaaaaaab,查找aaaab</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1500912214_1556864943460/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1486517634_1556864983556/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_308750603_1556864998323/0\" alt></p>\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><ul>\n<li><p>概念</p>\n<ul>\n<li>KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。</li>\n</ul>\n</li>\n<li><p>对于暴力破解的优化思考</p>\n<ul>\n<li>主串不要每次只移一位，可以让它（i）保持不动，移动模式中j的位置</li>\n<li>利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_308750603_1556865032698/0\" alt></p>\n<ul>\n<li>j要怎么移动<ul>\n<li>当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1486517634_1556865346487/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_771348268_1556865899518/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866125545/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_122902507_1556866189608/0\" alt></p>\n<ul>\n<li><p>数学表示</p>\n<ul>\n<li>如果k表示不匹配时，j要移动的位置</li>\n<li><p>P[0 ~ k-1] == P[j-k ~ j-1]</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866470014/0\" alt></p>\n</li>\n<li><p>当T[i] != P[j]时</p>\n<p>有T[i-j ~ i-1] == P[0 ~ j-1]</p>\n<p>由P[0 ~ k-1] == P[j-k ~ j-1]</p>\n<p>必然：T[i-k ~ i-1] == P[0 ~ k-1]</p>\n</li>\n<li><p>因此可以直接将j移动到k而无须再比较前面的k个字符</p>\n</li>\n</ul>\n</li>\n<li><p>怎么找到不匹配时j要移动的k位置</p>\n<ul>\n<li>模式串中每个位置发生不匹配时，所要移动到的k位置都是不同的，每个位置都要计算</li>\n<li>使用next数据来保存模式串中每个位置的k值</li>\n<li>next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置</li>\n<li><p>当j为0时，如果这时候不匹配</p>\n<ul>\n<li><p>j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。有next[0] = -1</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869374669/0\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>当j为1</p>\n<ul>\n<li><p>j指针一定是后移到0位置的</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_767595719_1556869496396/0\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>其他</p>\n<ul>\n<li>当P[k] == P[j]时</li>\n<li><p>next[j+1] == next[j] + 1</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869639696/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869662493/0\" alt></p>\n</li>\n<li><p>P[k] != P[j]</p>\n</li>\n<li><p>k = next[k]</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869752648/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556870613284/0\" alt></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://www.cnblogs.com/yjiyjige/p/3263858.html#top\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/yjiyjige/p/3263858.html#top</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"引入\"><a href=\"#引入\" class=\"headerlink\" title=\"引入\"></a>引入</h2><ul>\n<li><a href=\"https://leetcode.com/problems/implement-strstr/\" target=\"_blank\" rel=\"noopener\">Implement strStr()</a><ul>\n<li>LeetCode实现String.indexOf()，即返回子串在字符串中第一次出现的索引位置，没有则返回-1，子串为空则返回0</li>\n</ul>\n</li>\n<li>暴力解法<ul>\n<li>从0位置开始，比对与子串是否相同，出现不同则在字符串中往后移一位，再次比较子串，直到找到。</li>\n<li>复杂度<ul>\n<li>设字符串长度为m，要查找的子串长度为n</li>\n<li>时间 O(m*n)</li>\n</ul>\n</li>\n<li>问题<ul>\n<li>每次出现不一致的情况，只向后移动一位，再从头开始比对，很多没有必要</li>\n<li>如：在aaaaaaaaaaaab,查找aaaab</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1500912214_1556864943460/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1486517634_1556864983556/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_308750603_1556864998323/0\" alt></p>\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><ul>\n<li><p>概念</p>\n<ul>\n<li>KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。</li>\n</ul>\n</li>\n<li><p>对于暴力破解的优化思考</p>\n<ul>\n<li>主串不要每次只移一位，可以让它（i）保持不动，移动模式中j的位置</li>\n<li>利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_308750603_1556865032698/0\" alt></p>\n<ul>\n<li>j要怎么移动<ul>\n<li>当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1486517634_1556865346487/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_771348268_1556865899518/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866125545/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_122902507_1556866189608/0\" alt></p>\n<ul>\n<li><p>数学表示</p>\n<ul>\n<li>如果k表示不匹配时，j要移动的位置</li>\n<li><p>P[0 ~ k-1] == P[j-k ~ j-1]</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1268522869_1556866470014/0\" alt></p>\n</li>\n<li><p>当T[i] != P[j]时</p>\n<p>有T[i-j ~ i-1] == P[0 ~ j-1]</p>\n<p>由P[0 ~ k-1] == P[j-k ~ j-1]</p>\n<p>必然：T[i-k ~ i-1] == P[0 ~ k-1]</p>\n</li>\n<li><p>因此可以直接将j移动到k而无须再比较前面的k个字符</p>\n</li>\n</ul>\n</li>\n<li><p>怎么找到不匹配时j要移动的k位置</p>\n<ul>\n<li>模式串中每个位置发生不匹配时，所要移动到的k位置都是不同的，每个位置都要计算</li>\n<li>使用next数据来保存模式串中每个位置的k值</li>\n<li>next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置</li>\n<li><p>当j为0时，如果这时候不匹配</p>\n<ul>\n<li><p>j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。有next[0] = -1</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869374669/0\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>当j为1</p>\n<ul>\n<li><p>j指针一定是后移到0位置的</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_767595719_1556869496396/0\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>其他</p>\n<ul>\n<li>当P[k] == P[j]时</li>\n<li><p>next[j+1] == next[j] + 1</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869639696/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869662493/0\" alt></p>\n</li>\n<li><p>P[k] != P[j]</p>\n</li>\n<li><p>k = next[k]</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556869752648/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_912665407_1556870613284/0\" alt></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://www.cnblogs.com/yjiyjige/p/3263858.html#top\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/yjiyjige/p/3263858.html#top</a></li>\n</ul>\n"},{"title":"毕业设计参考文献记录🎓","date":"2019-03-18T16:00:00.000Z","_content":"## 实时流处理应用场景\n* [Spark Streaming在实时计算中的应用研究](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=DNZS201825112&uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!):延时限制在秒级的应用场景,电商实时营销。\n* [基于Spark Streaming的实时日志分析与信息管理系统的设计与实现](http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018894622.nh&uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MDAwNDFyQ1VSTE9mWk9kbUZpRGhWNzdJVkYyNkZydXhHdGZPclpFYlBJUjhlWDFMdXhZUzdEaDFUM3FUcldNMUY=):分析DDoS攻击。系统日志的信息安全审计\n* [基于海量网络日志数据的实时流处理系统的设计与实现](http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017292559.nh&v=MTU5MjBlWDFMdXhZUzdEaDFUM3FUcldNMUZyQ1VSTE9mWk9kbUZ5amtWTHZMVkYyNkdiR3hITlRKcHBFYlBJUjg=&uid=WEEvREcwSlJHSldRa1FhdkJkVG1CcDNRL2o5cjJMcDdKU2FGb00xSGNGND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!):可以参考这部分的绪论部分\n* [大公司的应用](https://www.infoq.cn/article/2016/01/spark-streaming-what-is-it-and-w)\n* [苏宁基于 Spark Streaming 的实时日志分析系统实践](https://www.infoq.cn/article/suning-realtime-log-analysis-system-spark-streaming)\n\n## 日志信息隐藏关系\n* [大数据下基于Spark的电商实时推荐系统的设计与实现](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=WJSY201705010&UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&autoLogin=0)\n\n## 研究目的/出发点\n* [互联网闭环](https://www.zhihu.com/question/20624296/answer/15731592)\n\n## 关于spark streaming业务算法的创新\n* [对有状态计算操作的基数计算的精确计算方法和估算方法进行了比较分析](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=WJSY201705010&UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&autoLogin=0)\n\n## 架构参考\n* [参考](https://book.douban.com/reading/37743267/)\n\n## Spark\n* [Spark入门：RDD的设计与运行原理](http://dblab.xmu.edu.cn/blog/985-2/)","source":"_posts/graduationDesignReferences.md","raw":"---\ntitle: 毕业设计参考文献记录🎓\ndate: 2019/3/19\ncategories: \n    - 毕业设计\ntags:\n---\n## 实时流处理应用场景\n* [Spark Streaming在实时计算中的应用研究](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=DNZS201825112&uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!):延时限制在秒级的应用场景,电商实时营销。\n* [基于Spark Streaming的实时日志分析与信息管理系统的设计与实现](http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018894622.nh&uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MDAwNDFyQ1VSTE9mWk9kbUZpRGhWNzdJVkYyNkZydXhHdGZPclpFYlBJUjhlWDFMdXhZUzdEaDFUM3FUcldNMUY=):分析DDoS攻击。系统日志的信息安全审计\n* [基于海量网络日志数据的实时流处理系统的设计与实现](http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201801&filename=1017292559.nh&v=MTU5MjBlWDFMdXhZUzdEaDFUM3FUcldNMUZyQ1VSTE9mWk9kbUZ5amtWTHZMVkYyNkdiR3hITlRKcHBFYlBJUjg=&uid=WEEvREcwSlJHSldRa1FhdkJkVG1CcDNRL2o5cjJMcDdKU2FGb00xSGNGND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!):可以参考这部分的绪论部分\n* [大公司的应用](https://www.infoq.cn/article/2016/01/spark-streaming-what-is-it-and-w)\n* [苏宁基于 Spark Streaming 的实时日志分析系统实践](https://www.infoq.cn/article/suning-realtime-log-analysis-system-spark-streaming)\n\n## 日志信息隐藏关系\n* [大数据下基于Spark的电商实时推荐系统的设计与实现](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=WJSY201705010&UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&autoLogin=0)\n\n## 研究目的/出发点\n* [互联网闭环](https://www.zhihu.com/question/20624296/answer/15731592)\n\n## 关于spark streaming业务算法的创新\n* [对有状态计算操作的基数计算的精确计算方法和估算方法进行了比较分析](http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&filename=WJSY201705010&UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&autoLogin=0)\n\n## 架构参考\n* [参考](https://book.douban.com/reading/37743267/)\n\n## Spark\n* [Spark入门：RDD的设计与运行原理](http://dblab.xmu.edu.cn/blog/985-2/)","slug":"graduationDesignReferences","published":1,"updated":"2019-06-04T01:06:18.979Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwds000p48upxsp2bzec","content":"<h2 id=\"实时流处理应用场景\"><a href=\"#实时流处理应用场景\" class=\"headerlink\" title=\"实时流处理应用场景\"></a>实时流处理应用场景</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=DNZS201825112&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!\" target=\"_blank\" rel=\"noopener\">Spark Streaming在实时计算中的应用研究</a>:延时限制在秒级的应用场景,电商实时营销。</li>\n<li><a href=\"http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201901&amp;filename=1018894622.nh&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;v=MDAwNDFyQ1VSTE9mWk9kbUZpRGhWNzdJVkYyNkZydXhHdGZPclpFYlBJUjhlWDFMdXhZUzdEaDFUM3FUcldNMUY=\" target=\"_blank\" rel=\"noopener\">基于Spark Streaming的实时日志分析与信息管理系统的设计与实现</a>:分析DDoS攻击。系统日志的信息安全审计</li>\n<li><a href=\"http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201801&amp;filename=1017292559.nh&amp;v=MTU5MjBlWDFMdXhZUzdEaDFUM3FUcldNMUZyQ1VSTE9mWk9kbUZ5amtWTHZMVkYyNkdiR3hITlRKcHBFYlBJUjg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG1CcDNRL2o5cjJMcDdKU2FGb00xSGNGND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!\" target=\"_blank\" rel=\"noopener\">基于海量网络日志数据的实时流处理系统的设计与实现</a>:可以参考这部分的绪论部分</li>\n<li><a href=\"https://www.infoq.cn/article/2016/01/spark-streaming-what-is-it-and-w\" target=\"_blank\" rel=\"noopener\">大公司的应用</a></li>\n<li><a href=\"https://www.infoq.cn/article/suning-realtime-log-analysis-system-spark-streaming\" target=\"_blank\" rel=\"noopener\">苏宁基于 Spark Streaming 的实时日志分析系统实践</a></li>\n</ul>\n<h2 id=\"日志信息隐藏关系\"><a href=\"#日志信息隐藏关系\" class=\"headerlink\" title=\"日志信息隐藏关系\"></a>日志信息隐藏关系</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=WJSY201705010&amp;UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;autoLogin=0\" target=\"_blank\" rel=\"noopener\">大数据下基于Spark的电商实时推荐系统的设计与实现</a></li>\n</ul>\n<h2 id=\"研究目的-出发点\"><a href=\"#研究目的-出发点\" class=\"headerlink\" title=\"研究目的/出发点\"></a>研究目的/出发点</h2><ul>\n<li><a href=\"https://www.zhihu.com/question/20624296/answer/15731592\" target=\"_blank\" rel=\"noopener\">互联网闭环</a></li>\n</ul>\n<h2 id=\"关于spark-streaming业务算法的创新\"><a href=\"#关于spark-streaming业务算法的创新\" class=\"headerlink\" title=\"关于spark streaming业务算法的创新\"></a>关于spark streaming业务算法的创新</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=WJSY201705010&amp;UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;autoLogin=0\" target=\"_blank\" rel=\"noopener\">对有状态计算操作的基数计算的精确计算方法和估算方法进行了比较分析</a></li>\n</ul>\n<h2 id=\"架构参考\"><a href=\"#架构参考\" class=\"headerlink\" title=\"架构参考\"></a>架构参考</h2><ul>\n<li><a href=\"https://book.douban.com/reading/37743267/\" target=\"_blank\" rel=\"noopener\">参考</a></li>\n</ul>\n<h2 id=\"Spark\"><a href=\"#Spark\" class=\"headerlink\" title=\"Spark\"></a>Spark</h2><ul>\n<li><a href=\"http://dblab.xmu.edu.cn/blog/985-2/\" target=\"_blank\" rel=\"noopener\">Spark入门：RDD的设计与运行原理</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"实时流处理应用场景\"><a href=\"#实时流处理应用场景\" class=\"headerlink\" title=\"实时流处理应用场景\"></a>实时流处理应用场景</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=DNZS201825112&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!\" target=\"_blank\" rel=\"noopener\">Spark Streaming在实时计算中的应用研究</a>:延时限制在秒级的应用场景,电商实时营销。</li>\n<li><a href=\"http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201901&amp;filename=1018894622.nh&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0pkRlg1TFVqTzAvOUVLVWZDZmNBOEF0az0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;v=MDAwNDFyQ1VSTE9mWk9kbUZpRGhWNzdJVkYyNkZydXhHdGZPclpFYlBJUjhlWDFMdXhZUzdEaDFUM3FUcldNMUY=\" target=\"_blank\" rel=\"noopener\">基于Spark Streaming的实时日志分析与信息管理系统的设计与实现</a>:分析DDoS攻击。系统日志的信息安全审计</li>\n<li><a href=\"http://new.gb.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201801&amp;filename=1017292559.nh&amp;v=MTU5MjBlWDFMdXhZUzdEaDFUM3FUcldNMUZyQ1VSTE9mWk9kbUZ5amtWTHZMVkYyNkdiR3hITlRKcHBFYlBJUjg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG1CcDNRL2o5cjJMcDdKU2FGb00xSGNGND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!\" target=\"_blank\" rel=\"noopener\">基于海量网络日志数据的实时流处理系统的设计与实现</a>:可以参考这部分的绪论部分</li>\n<li><a href=\"https://www.infoq.cn/article/2016/01/spark-streaming-what-is-it-and-w\" target=\"_blank\" rel=\"noopener\">大公司的应用</a></li>\n<li><a href=\"https://www.infoq.cn/article/suning-realtime-log-analysis-system-spark-streaming\" target=\"_blank\" rel=\"noopener\">苏宁基于 Spark Streaming 的实时日志分析系统实践</a></li>\n</ul>\n<h2 id=\"日志信息隐藏关系\"><a href=\"#日志信息隐藏关系\" class=\"headerlink\" title=\"日志信息隐藏关系\"></a>日志信息隐藏关系</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=WJSY201705010&amp;UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;autoLogin=0\" target=\"_blank\" rel=\"noopener\">大数据下基于Spark的电商实时推荐系统的设计与实现</a></li>\n</ul>\n<h2 id=\"研究目的-出发点\"><a href=\"#研究目的-出发点\" class=\"headerlink\" title=\"研究目的/出发点\"></a>研究目的/出发点</h2><ul>\n<li><a href=\"https://www.zhihu.com/question/20624296/answer/15731592\" target=\"_blank\" rel=\"noopener\">互联网闭环</a></li>\n</ul>\n<h2 id=\"关于spark-streaming业务算法的创新\"><a href=\"#关于spark-streaming业务算法的创新\" class=\"headerlink\" title=\"关于spark streaming业务算法的创新\"></a>关于spark streaming业务算法的创新</h2><ul>\n<li><a href=\"http://new.gb.oversea.cnki.net/KXReader/Detail?dbcode=CJFD&amp;filename=WJSY201705010&amp;UID=WEEvREcwSlJHSldRa1FhdkJkVG1CcDc5Ym4weGJzK0hVZ1FjREZpT1JEcz0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&amp;autoLogin=0\" target=\"_blank\" rel=\"noopener\">对有状态计算操作的基数计算的精确计算方法和估算方法进行了比较分析</a></li>\n</ul>\n<h2 id=\"架构参考\"><a href=\"#架构参考\" class=\"headerlink\" title=\"架构参考\"></a>架构参考</h2><ul>\n<li><a href=\"https://book.douban.com/reading/37743267/\" target=\"_blank\" rel=\"noopener\">参考</a></li>\n</ul>\n<h2 id=\"Spark\"><a href=\"#Spark\" class=\"headerlink\" title=\"Spark\"></a>Spark</h2><ul>\n<li><a href=\"http://dblab.xmu.edu.cn/blog/985-2/\" target=\"_blank\" rel=\"noopener\">Spark入门：RDD的设计与运行原理</a></li>\n</ul>\n"},{"title":"移动互联实验室-前端组-参考培训计划🖥（1期）","date":"2019-03-17T16:00:00.000Z","_content":" \n ## 基础知识🚶\n * [HTML代码练习](http://www.imooc.com/code/49)(第4周-第6周）\n * [JavaScript语言入门教程](https://wangdoc.com/javascript/)\n * [Js视频](http://www.imooc.com/learn/36)\n \n \n ## 进阶🏃\n * [ECMAScript 6 入门](http://es6.ruanyifeng.com/)\n * [ES6快速入门](https://www.imooc.com/learn/955)\n * [React入门教程](http://www.ruanyifeng.com/blog/2015/03/react.html)\n * 使用React的网站：\n \t* [Facebook](http://www.facebook.com/)\n \t* [Instagram](http://instagram.com/)\n \t* [知乎](https://www.zhihu.com/)\n \n \n ## React🚕\n * [从React脚手架工具学习React项目的最佳实践](https://juejin.im/post/59dcd87451882578c2084515)\n * [组件库（了解）](https://juejin.im/entry/5b3f3c82e51d45190905d3a2)\n * [React 技术栈系列教程](http://www.ruanyifeng.com/blog/2016/09/react-technology-stack.html)","source":"_posts/frontendstudy.md","raw":"---\ntitle: 移动互联实验室-前端组-参考培训计划🖥（1期）\ndate: 2019/3/18\ncategories: \n    - 技术总结\ntags:\n    - react ES6\n---\n \n ## 基础知识🚶\n * [HTML代码练习](http://www.imooc.com/code/49)(第4周-第6周）\n * [JavaScript语言入门教程](https://wangdoc.com/javascript/)\n * [Js视频](http://www.imooc.com/learn/36)\n \n \n ## 进阶🏃\n * [ECMAScript 6 入门](http://es6.ruanyifeng.com/)\n * [ES6快速入门](https://www.imooc.com/learn/955)\n * [React入门教程](http://www.ruanyifeng.com/blog/2015/03/react.html)\n * 使用React的网站：\n \t* [Facebook](http://www.facebook.com/)\n \t* [Instagram](http://instagram.com/)\n \t* [知乎](https://www.zhihu.com/)\n \n \n ## React🚕\n * [从React脚手架工具学习React项目的最佳实践](https://juejin.im/post/59dcd87451882578c2084515)\n * [组件库（了解）](https://juejin.im/entry/5b3f3c82e51d45190905d3a2)\n * [React 技术栈系列教程](http://www.ruanyifeng.com/blog/2016/09/react-technology-stack.html)","slug":"frontendstudy","published":1,"updated":"2019-06-04T01:06:18.977Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwe2000s48up35q1gm6o","content":"<h2 id=\"基础知识🚶\"><a href=\"#基础知识🚶\" class=\"headerlink\" title=\"基础知识🚶\"></a>基础知识🚶</h2><ul>\n<li><a href=\"http://www.imooc.com/code/49\" target=\"_blank\" rel=\"noopener\">HTML代码练习</a>(第4周-第6周）</li>\n<li><a href=\"https://wangdoc.com/javascript/\" target=\"_blank\" rel=\"noopener\">JavaScript语言入门教程</a></li>\n<li><a href=\"http://www.imooc.com/learn/36\" target=\"_blank\" rel=\"noopener\">Js视频</a></li>\n</ul>\n<h2 id=\"进阶🏃\"><a href=\"#进阶🏃\" class=\"headerlink\" title=\"进阶🏃\"></a>进阶🏃</h2><ul>\n<li><a href=\"http://es6.ruanyifeng.com/\" target=\"_blank\" rel=\"noopener\">ECMAScript 6 入门</a></li>\n<li><a href=\"https://www.imooc.com/learn/955\" target=\"_blank\" rel=\"noopener\">ES6快速入门</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2015/03/react.html\" target=\"_blank\" rel=\"noopener\">React入门教程</a></li>\n<li>使用React的网站：<ul>\n<li><a href=\"http://www.facebook.com/\" target=\"_blank\" rel=\"noopener\">Facebook</a></li>\n<li><a href=\"http://instagram.com/\" target=\"_blank\" rel=\"noopener\">Instagram</a></li>\n<li><a href=\"https://www.zhihu.com/\" target=\"_blank\" rel=\"noopener\">知乎</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"React🚕\"><a href=\"#React🚕\" class=\"headerlink\" title=\"React🚕\"></a>React🚕</h2><ul>\n<li><a href=\"https://juejin.im/post/59dcd87451882578c2084515\" target=\"_blank\" rel=\"noopener\">从React脚手架工具学习React项目的最佳实践</a></li>\n<li><a href=\"https://juejin.im/entry/5b3f3c82e51d45190905d3a2\" target=\"_blank\" rel=\"noopener\">组件库（了解）</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/09/react-technology-stack.html\" target=\"_blank\" rel=\"noopener\">React 技术栈系列教程</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"基础知识🚶\"><a href=\"#基础知识🚶\" class=\"headerlink\" title=\"基础知识🚶\"></a>基础知识🚶</h2><ul>\n<li><a href=\"http://www.imooc.com/code/49\" target=\"_blank\" rel=\"noopener\">HTML代码练习</a>(第4周-第6周）</li>\n<li><a href=\"https://wangdoc.com/javascript/\" target=\"_blank\" rel=\"noopener\">JavaScript语言入门教程</a></li>\n<li><a href=\"http://www.imooc.com/learn/36\" target=\"_blank\" rel=\"noopener\">Js视频</a></li>\n</ul>\n<h2 id=\"进阶🏃\"><a href=\"#进阶🏃\" class=\"headerlink\" title=\"进阶🏃\"></a>进阶🏃</h2><ul>\n<li><a href=\"http://es6.ruanyifeng.com/\" target=\"_blank\" rel=\"noopener\">ECMAScript 6 入门</a></li>\n<li><a href=\"https://www.imooc.com/learn/955\" target=\"_blank\" rel=\"noopener\">ES6快速入门</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2015/03/react.html\" target=\"_blank\" rel=\"noopener\">React入门教程</a></li>\n<li>使用React的网站：<ul>\n<li><a href=\"http://www.facebook.com/\" target=\"_blank\" rel=\"noopener\">Facebook</a></li>\n<li><a href=\"http://instagram.com/\" target=\"_blank\" rel=\"noopener\">Instagram</a></li>\n<li><a href=\"https://www.zhihu.com/\" target=\"_blank\" rel=\"noopener\">知乎</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"React🚕\"><a href=\"#React🚕\" class=\"headerlink\" title=\"React🚕\"></a>React🚕</h2><ul>\n<li><a href=\"https://juejin.im/post/59dcd87451882578c2084515\" target=\"_blank\" rel=\"noopener\">从React脚手架工具学习React项目的最佳实践</a></li>\n<li><a href=\"https://juejin.im/entry/5b3f3c82e51d45190905d3a2\" target=\"_blank\" rel=\"noopener\">组件库（了解）</a></li>\n<li><a href=\"http://www.ruanyifeng.com/blog/2016/09/react-technology-stack.html\" target=\"_blank\" rel=\"noopener\">React 技术栈系列教程</a></li>\n</ul>\n"},{"title":"Minions初始化手册✋","date":"2019-03-26T16:00:00.000Z","_content":"\n* kill所有进程\n\n* 启动zooKeeper\n\n`zookeeper>bin ./zkServer.sh start`\n\n* 启动Kafka（后台）\n\n`kafka> bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties`\n\n* 启动两个flume\n\t* 对接Python产生的伪日志\n\t* 对接前端埋点的数据\n\n```\n//页面浏览日志 agent\nflume-ng agent \\\n--name exec-memory-kafka \\\n--conf $FLUME_HOME/conf \\\n--conf-file /home/hadoop/data/project/streaming_project2.conf \\\n-Dflume.root.logger=INFO,console\n\n//页面行为日志 agent\nflume-ng agent \\\n--name agent1 \\\n--conf $FLUME_HOME/conf \\\n--conf-file $FLUME_HOME/conf/streaming2.conf \\\n-Dflume.root.logger=INFO,console\n```\n\n* 启动Hadoop\n\t* DataNode\n\t* SecondaryNameNode\n\t* NameNode \n\n```\nhadoop/sbin> ./start-dfs.sh\n\n//单独启动命令：\n./hadoop-daemon.sh start secondarynamenode\n```\n\n* 启动HBase\n\t* HMaster\n\t* HRegionServer \n\n`bin> ./start-hbase.sh`\n\n```\n./hbase shell\n\nlist\nscan 'table_name'\ndesc 'table_name'\n```\n\n* 启动spark作业\n\t* MyStreamingApp : 统计访问量\n\t* KafKaStreamingApp : 处理前端埋点数据（hello_ladygaga_topic)\n\n```java\nspark-submit --master local[5] --jars $(echo /home/hadoop/app/hbase-1.2.0-cdh5.7.0/lib/*.jar | tr ' ' ',') --class com.chaoyue.spark.project.scala.MyStreamingApp --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 /home/hadoop/lib/sparktrain-1.0.jar hadoop000:2181 test streamingtopic 1\n```\n\n* 启动Springboot后端和react前端","source":"_posts/minionsUsage.md","raw":"---\ntitle: Minions初始化手册✋\ndate: 2019/03/27\ncategories: \n    - 毕业设计\n---\n\n* kill所有进程\n\n* 启动zooKeeper\n\n`zookeeper>bin ./zkServer.sh start`\n\n* 启动Kafka（后台）\n\n`kafka> bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties`\n\n* 启动两个flume\n\t* 对接Python产生的伪日志\n\t* 对接前端埋点的数据\n\n```\n//页面浏览日志 agent\nflume-ng agent \\\n--name exec-memory-kafka \\\n--conf $FLUME_HOME/conf \\\n--conf-file /home/hadoop/data/project/streaming_project2.conf \\\n-Dflume.root.logger=INFO,console\n\n//页面行为日志 agent\nflume-ng agent \\\n--name agent1 \\\n--conf $FLUME_HOME/conf \\\n--conf-file $FLUME_HOME/conf/streaming2.conf \\\n-Dflume.root.logger=INFO,console\n```\n\n* 启动Hadoop\n\t* DataNode\n\t* SecondaryNameNode\n\t* NameNode \n\n```\nhadoop/sbin> ./start-dfs.sh\n\n//单独启动命令：\n./hadoop-daemon.sh start secondarynamenode\n```\n\n* 启动HBase\n\t* HMaster\n\t* HRegionServer \n\n`bin> ./start-hbase.sh`\n\n```\n./hbase shell\n\nlist\nscan 'table_name'\ndesc 'table_name'\n```\n\n* 启动spark作业\n\t* MyStreamingApp : 统计访问量\n\t* KafKaStreamingApp : 处理前端埋点数据（hello_ladygaga_topic)\n\n```java\nspark-submit --master local[5] --jars $(echo /home/hadoop/app/hbase-1.2.0-cdh5.7.0/lib/*.jar | tr ' ' ',') --class com.chaoyue.spark.project.scala.MyStreamingApp --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 /home/hadoop/lib/sparktrain-1.0.jar hadoop000:2181 test streamingtopic 1\n```\n\n* 启动Springboot后端和react前端","slug":"minionsUsage","published":1,"updated":"2019-06-04T01:06:18.980Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwe6000u48upww8b8t15","content":"<ul>\n<li><p>kill所有进程</p>\n</li>\n<li><p>启动zooKeeper</p>\n</li>\n</ul>\n<p><code>zookeeper&gt;bin ./zkServer.sh start</code></p>\n<ul>\n<li>启动Kafka（后台）</li>\n</ul>\n<p><code>kafka&gt; bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</code></p>\n<ul>\n<li>启动两个flume<ul>\n<li>对接Python产生的伪日志</li>\n<li>对接前端埋点的数据</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//页面浏览日志 agent</span><br><span class=\"line\">flume-ng agent \\</span><br><span class=\"line\">--name exec-memory-kafka \\</span><br><span class=\"line\">--conf $FLUME_HOME/conf \\</span><br><span class=\"line\">--conf-file /home/hadoop/data/project/streaming_project2.conf \\</span><br><span class=\"line\">-Dflume.root.logger=INFO,console</span><br><span class=\"line\"></span><br><span class=\"line\">//页面行为日志 agent</span><br><span class=\"line\">flume-ng agent \\</span><br><span class=\"line\">--name agent1 \\</span><br><span class=\"line\">--conf $FLUME_HOME/conf \\</span><br><span class=\"line\">--conf-file $FLUME_HOME/conf/streaming2.conf \\</span><br><span class=\"line\">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动Hadoop<ul>\n<li>DataNode</li>\n<li>SecondaryNameNode</li>\n<li>NameNode </li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop/sbin&gt; ./start-dfs.sh</span><br><span class=\"line\"></span><br><span class=\"line\">//单独启动命令：</span><br><span class=\"line\">./hadoop-daemon.sh start secondarynamenode</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动HBase<ul>\n<li>HMaster</li>\n<li>HRegionServer </li>\n</ul>\n</li>\n</ul>\n<p><code>bin&gt; ./start-hbase.sh</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./hbase shell</span><br><span class=\"line\"></span><br><span class=\"line\">list</span><br><span class=\"line\">scan &apos;table_name&apos;</span><br><span class=\"line\">desc &apos;table_name&apos;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动spark作业<ul>\n<li>MyStreamingApp : 统计访问量</li>\n<li>KafKaStreamingApp : 处理前端埋点数据（hello_ladygaga_topic)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-submit --master local[<span class=\"number\">5</span>] --jars $(echo /home/hadoop/app/hbase-<span class=\"number\">1.2</span>.0-cdh5.7.0/lib<span class=\"comment\">/*.jar | tr ' ' ',') --class com.chaoyue.spark.project.scala.MyStreamingApp --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 /home/hadoop/lib/sparktrain-1.0.jar hadoop000:2181 test streamingtopic 1</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动Springboot后端和react前端</li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<ul>\n<li><p>kill所有进程</p>\n</li>\n<li><p>启动zooKeeper</p>\n</li>\n</ul>\n<p><code>zookeeper&gt;bin ./zkServer.sh start</code></p>\n<ul>\n<li>启动Kafka（后台）</li>\n</ul>\n<p><code>kafka&gt; bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</code></p>\n<ul>\n<li>启动两个flume<ul>\n<li>对接Python产生的伪日志</li>\n<li>对接前端埋点的数据</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//页面浏览日志 agent</span><br><span class=\"line\">flume-ng agent \\</span><br><span class=\"line\">--name exec-memory-kafka \\</span><br><span class=\"line\">--conf $FLUME_HOME/conf \\</span><br><span class=\"line\">--conf-file /home/hadoop/data/project/streaming_project2.conf \\</span><br><span class=\"line\">-Dflume.root.logger=INFO,console</span><br><span class=\"line\"></span><br><span class=\"line\">//页面行为日志 agent</span><br><span class=\"line\">flume-ng agent \\</span><br><span class=\"line\">--name agent1 \\</span><br><span class=\"line\">--conf $FLUME_HOME/conf \\</span><br><span class=\"line\">--conf-file $FLUME_HOME/conf/streaming2.conf \\</span><br><span class=\"line\">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动Hadoop<ul>\n<li>DataNode</li>\n<li>SecondaryNameNode</li>\n<li>NameNode </li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop/sbin&gt; ./start-dfs.sh</span><br><span class=\"line\"></span><br><span class=\"line\">//单独启动命令：</span><br><span class=\"line\">./hadoop-daemon.sh start secondarynamenode</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动HBase<ul>\n<li>HMaster</li>\n<li>HRegionServer </li>\n</ul>\n</li>\n</ul>\n<p><code>bin&gt; ./start-hbase.sh</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./hbase shell</span><br><span class=\"line\"></span><br><span class=\"line\">list</span><br><span class=\"line\">scan &apos;table_name&apos;</span><br><span class=\"line\">desc &apos;table_name&apos;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动spark作业<ul>\n<li>MyStreamingApp : 统计访问量</li>\n<li>KafKaStreamingApp : 处理前端埋点数据（hello_ladygaga_topic)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-submit --master local[<span class=\"number\">5</span>] --jars $(echo /home/hadoop/app/hbase-<span class=\"number\">1.2</span>.0-cdh5.7.0/lib<span class=\"comment\">/*.jar | tr ' ' ',') --class com.chaoyue.spark.project.scala.MyStreamingApp --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 /home/hadoop/lib/sparktrain-1.0.jar hadoop000:2181 test streamingtopic 1</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动Springboot后端和react前端</li>\n</ul>\n"},{"title":"Websites Safari","date":"2019-03-17T16:00:00.000Z","_content":"## 脚手架\n* [antd脚手架市场](http://scaffold.ant.design/#/)\n\n## 图解IT\n* [Scaling webapps for newbs & non-techies 大型网络扩展](https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/)\n* [算法可视化](https://visualgo.net/zh)\n\n## React Framework\n* [Next.js](https://nextjs.org/)\n\n## 前端\n* [JS变化历史](https://ponyfoo.com/articles/brief-history-of-modularity)\n* [图标定制](https://iconsvg.xyz/)\n* [纯CSS图片](https://codepen.io/ivorjetski/pen/xMJoYO)\n\n## 开发\n* [领域驱动设计在互联网业务开发中的实践](https://zhuanlan.zhihu.com/p/32459776)\n\n## 工具\n* [画ER图的](https://dbdiagram.io/home)\n\n","source":"_posts/websitesafari.md","raw":"---\ntitle: Websites Safari\ndate: 2019/3/18\n---\n## 脚手架\n* [antd脚手架市场](http://scaffold.ant.design/#/)\n\n## 图解IT\n* [Scaling webapps for newbs & non-techies 大型网络扩展](https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/)\n* [算法可视化](https://visualgo.net/zh)\n\n## React Framework\n* [Next.js](https://nextjs.org/)\n\n## 前端\n* [JS变化历史](https://ponyfoo.com/articles/brief-history-of-modularity)\n* [图标定制](https://iconsvg.xyz/)\n* [纯CSS图片](https://codepen.io/ivorjetski/pen/xMJoYO)\n\n## 开发\n* [领域驱动设计在互联网业务开发中的实践](https://zhuanlan.zhihu.com/p/32459776)\n\n## 工具\n* [画ER图的](https://dbdiagram.io/home)\n\n","slug":"websitesafari","published":1,"updated":"2019-06-04T01:06:18.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwee000y48uppfnhtn70","content":"<h2 id=\"脚手架\"><a href=\"#脚手架\" class=\"headerlink\" title=\"脚手架\"></a>脚手架</h2><ul>\n<li><a href=\"http://scaffold.ant.design/#/\" target=\"_blank\" rel=\"noopener\">antd脚手架市场</a></li>\n</ul>\n<h2 id=\"图解IT\"><a href=\"#图解IT\" class=\"headerlink\" title=\"图解IT\"></a>图解IT</h2><ul>\n<li><a href=\"https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/\" target=\"_blank\" rel=\"noopener\">Scaling webapps for newbs &amp; non-techies 大型网络扩展</a></li>\n<li><a href=\"https://visualgo.net/zh\" target=\"_blank\" rel=\"noopener\">算法可视化</a></li>\n</ul>\n<h2 id=\"React-Framework\"><a href=\"#React-Framework\" class=\"headerlink\" title=\"React Framework\"></a>React Framework</h2><ul>\n<li><a href=\"https://nextjs.org/\" target=\"_blank\" rel=\"noopener\">Next.js</a></li>\n</ul>\n<h2 id=\"前端\"><a href=\"#前端\" class=\"headerlink\" title=\"前端\"></a>前端</h2><ul>\n<li><a href=\"https://ponyfoo.com/articles/brief-history-of-modularity\" target=\"_blank\" rel=\"noopener\">JS变化历史</a></li>\n<li><a href=\"https://iconsvg.xyz/\" target=\"_blank\" rel=\"noopener\">图标定制</a></li>\n<li><a href=\"https://codepen.io/ivorjetski/pen/xMJoYO\" target=\"_blank\" rel=\"noopener\">纯CSS图片</a></li>\n</ul>\n<h2 id=\"开发\"><a href=\"#开发\" class=\"headerlink\" title=\"开发\"></a>开发</h2><ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32459776\" target=\"_blank\" rel=\"noopener\">领域驱动设计在互联网业务开发中的实践</a></li>\n</ul>\n<h2 id=\"工具\"><a href=\"#工具\" class=\"headerlink\" title=\"工具\"></a>工具</h2><ul>\n<li><a href=\"https://dbdiagram.io/home\" target=\"_blank\" rel=\"noopener\">画ER图的</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"脚手架\"><a href=\"#脚手架\" class=\"headerlink\" title=\"脚手架\"></a>脚手架</h2><ul>\n<li><a href=\"http://scaffold.ant.design/#/\" target=\"_blank\" rel=\"noopener\">antd脚手架市场</a></li>\n</ul>\n<h2 id=\"图解IT\"><a href=\"#图解IT\" class=\"headerlink\" title=\"图解IT\"></a>图解IT</h2><ul>\n<li><a href=\"https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/\" target=\"_blank\" rel=\"noopener\">Scaling webapps for newbs &amp; non-techies 大型网络扩展</a></li>\n<li><a href=\"https://visualgo.net/zh\" target=\"_blank\" rel=\"noopener\">算法可视化</a></li>\n</ul>\n<h2 id=\"React-Framework\"><a href=\"#React-Framework\" class=\"headerlink\" title=\"React Framework\"></a>React Framework</h2><ul>\n<li><a href=\"https://nextjs.org/\" target=\"_blank\" rel=\"noopener\">Next.js</a></li>\n</ul>\n<h2 id=\"前端\"><a href=\"#前端\" class=\"headerlink\" title=\"前端\"></a>前端</h2><ul>\n<li><a href=\"https://ponyfoo.com/articles/brief-history-of-modularity\" target=\"_blank\" rel=\"noopener\">JS变化历史</a></li>\n<li><a href=\"https://iconsvg.xyz/\" target=\"_blank\" rel=\"noopener\">图标定制</a></li>\n<li><a href=\"https://codepen.io/ivorjetski/pen/xMJoYO\" target=\"_blank\" rel=\"noopener\">纯CSS图片</a></li>\n</ul>\n<h2 id=\"开发\"><a href=\"#开发\" class=\"headerlink\" title=\"开发\"></a>开发</h2><ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32459776\" target=\"_blank\" rel=\"noopener\">领域驱动设计在互联网业务开发中的实践</a></li>\n</ul>\n<h2 id=\"工具\"><a href=\"#工具\" class=\"headerlink\" title=\"工具\"></a>工具</h2><ul>\n<li><a href=\"https://dbdiagram.io/home\" target=\"_blank\" rel=\"noopener\">画ER图的</a></li>\n</ul>\n"},{"title":"Minions-frontend 框架技术结构","date":"2019-03-20T16:00:00.000Z","_content":"## 前端技术选型\n* react 16.8.4\n* Ant Design ([Pro v1](https://v1.pro.ant.design/docs/getting-started-cn)) 3.15.0\n* XMLRequest:axios 0.18.0\n* 路由: react-router-dom\n* [精选组件](https://ant.design/docs/react/recommendation-cn)\n\n## 目录结构\n* components 放置个性化组件\n    * PageView 展示PV相关数据组件\n* contents 主体部分抽象布局\n    * Overview 今日数据概览\n    * HistoryView 历史数据查询\n* pages 页面（目前为单页面）\n\n## React-Router\n* react-router-dom 是对 react-router 在浏览器开发环境下的封装，所以没有本质差别\n\n## ant design pro\n* 需要引入css文件\n`import 'ant-design-pro/dist/ant-design-pro.css';`\n\n## [BizChart](https://bizcharts.net/index)\n* 商业场景下的数据可视化\n\n## react相关\n* [dangerouslySetInnerHTML, 让React正常显示你的html代码](http://www.cnblogs.com/xianyulaodi/p/5038258.html)\n* [时间显示组件](https://github.com/pvoznyuk/react-live-clock)\n    * 获取今天的时间\n    \n    `const today = moment().format('YYYYMMDD');`\n    \n* react setState是异步调用的\n    * 使用props和componentWillReceiveProps(nextProps)来解决\n    * [见topicid的变换](https://github.com/fangmiao97/MessageWiKiPro-frontend/blob/master/src/TopicInfo.js)\n    * 在minions中的历史数据页面，需要根据选择的不同日期显示相关数据。日期选择为父组件中的state.date改变，并将state.date传给child组件。child接收后通过componentWillReceiveProps来进行更新\n\n* [flex布局](http://www.ruanyifeng.com/blog/2015/07/flex-grammar.html)\n\n* RestFul API\n\n* 组件定时刷新\n```javascript\ncomponentDidMount() {\n        this.timer = setInterval(\n            () => {\n                //更新组件state...\n            },\n            60000\n        );\n    }\n```\n\n* 组件动态渲染\n```javascript\n    //父组件\n    this.state = {\n                songsInfoList:[]\n            }\n    \n            render() {\n                    let songInfoList = this.state.songsInfoList;\n                    return (\n                        <div>\n                            {\n                                songInfoList.map((item, index) => {\n                                    return <MockSongComponent key={index} songInfo={item}/>\n                                })\n                            }\n                        </div>\n                    );\n                }\n                \n    //子组件\n    this.state = {\n                songInfo: this.props.songInfo\n            }\n            \n    render() {\n    \n            let songInfo = this.state.songInfo;\n    \n            return(\n                <div>\n                  {songInfo.name}\n                  {songInfo.coverUrl}\n                  ......\n                </div>\n            );\n    }\n```","source":"_posts/minions-frontend-overview.md","raw":"---\ntitle: Minions-frontend 框架技术结构\ndate: 2019/3/21\ncategories:\n    - 毕业设计\n---\n## 前端技术选型\n* react 16.8.4\n* Ant Design ([Pro v1](https://v1.pro.ant.design/docs/getting-started-cn)) 3.15.0\n* XMLRequest:axios 0.18.0\n* 路由: react-router-dom\n* [精选组件](https://ant.design/docs/react/recommendation-cn)\n\n## 目录结构\n* components 放置个性化组件\n    * PageView 展示PV相关数据组件\n* contents 主体部分抽象布局\n    * Overview 今日数据概览\n    * HistoryView 历史数据查询\n* pages 页面（目前为单页面）\n\n## React-Router\n* react-router-dom 是对 react-router 在浏览器开发环境下的封装，所以没有本质差别\n\n## ant design pro\n* 需要引入css文件\n`import 'ant-design-pro/dist/ant-design-pro.css';`\n\n## [BizChart](https://bizcharts.net/index)\n* 商业场景下的数据可视化\n\n## react相关\n* [dangerouslySetInnerHTML, 让React正常显示你的html代码](http://www.cnblogs.com/xianyulaodi/p/5038258.html)\n* [时间显示组件](https://github.com/pvoznyuk/react-live-clock)\n    * 获取今天的时间\n    \n    `const today = moment().format('YYYYMMDD');`\n    \n* react setState是异步调用的\n    * 使用props和componentWillReceiveProps(nextProps)来解决\n    * [见topicid的变换](https://github.com/fangmiao97/MessageWiKiPro-frontend/blob/master/src/TopicInfo.js)\n    * 在minions中的历史数据页面，需要根据选择的不同日期显示相关数据。日期选择为父组件中的state.date改变，并将state.date传给child组件。child接收后通过componentWillReceiveProps来进行更新\n\n* [flex布局](http://www.ruanyifeng.com/blog/2015/07/flex-grammar.html)\n\n* RestFul API\n\n* 组件定时刷新\n```javascript\ncomponentDidMount() {\n        this.timer = setInterval(\n            () => {\n                //更新组件state...\n            },\n            60000\n        );\n    }\n```\n\n* 组件动态渲染\n```javascript\n    //父组件\n    this.state = {\n                songsInfoList:[]\n            }\n    \n            render() {\n                    let songInfoList = this.state.songsInfoList;\n                    return (\n                        <div>\n                            {\n                                songInfoList.map((item, index) => {\n                                    return <MockSongComponent key={index} songInfo={item}/>\n                                })\n                            }\n                        </div>\n                    );\n                }\n                \n    //子组件\n    this.state = {\n                songInfo: this.props.songInfo\n            }\n            \n    render() {\n    \n            let songInfo = this.state.songInfo;\n    \n            return(\n                <div>\n                  {songInfo.name}\n                  {songInfo.coverUrl}\n                  ......\n                </div>\n            );\n    }\n```","slug":"minions-frontend-overview","published":1,"updated":"2019-06-04T01:06:18.979Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwej001048upllvs4fby","content":"<h2 id=\"前端技术选型\"><a href=\"#前端技术选型\" class=\"headerlink\" title=\"前端技术选型\"></a>前端技术选型</h2><ul>\n<li>react 16.8.4</li>\n<li>Ant Design (<a href=\"https://v1.pro.ant.design/docs/getting-started-cn\" target=\"_blank\" rel=\"noopener\">Pro v1</a>) 3.15.0</li>\n<li>XMLRequest:axios 0.18.0</li>\n<li>路由: react-router-dom</li>\n<li><a href=\"https://ant.design/docs/react/recommendation-cn\" target=\"_blank\" rel=\"noopener\">精选组件</a></li>\n</ul>\n<h2 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h2><ul>\n<li>components 放置个性化组件<ul>\n<li>PageView 展示PV相关数据组件</li>\n</ul>\n</li>\n<li>contents 主体部分抽象布局<ul>\n<li>Overview 今日数据概览</li>\n<li>HistoryView 历史数据查询</li>\n</ul>\n</li>\n<li>pages 页面（目前为单页面）</li>\n</ul>\n<h2 id=\"React-Router\"><a href=\"#React-Router\" class=\"headerlink\" title=\"React-Router\"></a>React-Router</h2><ul>\n<li>react-router-dom 是对 react-router 在浏览器开发环境下的封装，所以没有本质差别</li>\n</ul>\n<h2 id=\"ant-design-pro\"><a href=\"#ant-design-pro\" class=\"headerlink\" title=\"ant design pro\"></a>ant design pro</h2><ul>\n<li>需要引入css文件<br><code>import &#39;ant-design-pro/dist/ant-design-pro.css&#39;;</code></li>\n</ul>\n<h2 id=\"BizChart\"><a href=\"#BizChart\" class=\"headerlink\" title=\"BizChart\"></a><a href=\"https://bizcharts.net/index\" target=\"_blank\" rel=\"noopener\">BizChart</a></h2><ul>\n<li>商业场景下的数据可视化</li>\n</ul>\n<h2 id=\"react相关\"><a href=\"#react相关\" class=\"headerlink\" title=\"react相关\"></a>react相关</h2><ul>\n<li><a href=\"http://www.cnblogs.com/xianyulaodi/p/5038258.html\" target=\"_blank\" rel=\"noopener\">dangerouslySetInnerHTML, 让React正常显示你的html代码</a></li>\n<li><p><a href=\"https://github.com/pvoznyuk/react-live-clock\" target=\"_blank\" rel=\"noopener\">时间显示组件</a></p>\n<ul>\n<li><p>获取今天的时间</p>\n<p><code>const today = moment().format(&#39;YYYYMMDD&#39;);</code></p>\n</li>\n</ul>\n</li>\n<li><p>react setState是异步调用的</p>\n<ul>\n<li>使用props和componentWillReceiveProps(nextProps)来解决</li>\n<li><a href=\"https://github.com/fangmiao97/MessageWiKiPro-frontend/blob/master/src/TopicInfo.js\" target=\"_blank\" rel=\"noopener\">见topicid的变换</a></li>\n<li>在minions中的历史数据页面，需要根据选择的不同日期显示相关数据。日期选择为父组件中的state.date改变，并将state.date传给child组件。child接收后通过componentWillReceiveProps来进行更新</li>\n</ul>\n</li>\n<li><p><a href=\"http://www.ruanyifeng.com/blog/2015/07/flex-grammar.html\" target=\"_blank\" rel=\"noopener\">flex布局</a></p>\n</li>\n<li><p>RestFul API</p>\n</li>\n<li><p>组件定时刷新</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">componentDidMount() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.timer = setInterval(</span><br><span class=\"line\">            () =&gt; &#123;</span><br><span class=\"line\">                <span class=\"comment\">//更新组件state...</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"number\">60000</span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>组件动态渲染</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//父组件</span></span><br><span class=\"line\"><span class=\"keyword\">this</span>.state = &#123;</span><br><span class=\"line\">            songsInfoList:[]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        render() &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> songInfoList = <span class=\"keyword\">this</span>.state.songsInfoList;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> (</span><br><span class=\"line\">                    &lt;div&gt;</span><br><span class=\"line\">                        &#123;</span><br><span class=\"line\">                            songInfoList.map(<span class=\"function\">(<span class=\"params\">item, index</span>) =&gt;</span> &#123;</span><br><span class=\"line\">                                <span class=\"keyword\">return</span> &lt;MockSongComponent key=&#123;index&#125; songInfo=&#123;item&#125;/&gt;</span><br><span class=\"line\">                            &#125;)</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &lt;/div&gt;</span><br><span class=\"line\">                );</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            </span><br><span class=\"line\"><span class=\"comment\">//子组件</span></span><br><span class=\"line\"><span class=\"keyword\">this</span>.state = &#123;</span><br><span class=\"line\">            songInfo: <span class=\"keyword\">this</span>.props.songInfo</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> songInfo = <span class=\"keyword\">this</span>.state.songInfo;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(</span><br><span class=\"line\">            &lt;div&gt;</span><br><span class=\"line\">              &#123;songInfo.name&#125;</span><br><span class=\"line\">              &#123;songInfo.coverUrl&#125;</span><br><span class=\"line\">              ......</span><br><span class=\"line\">            &lt;<span class=\"regexp\">/div&gt;</span></span><br><span class=\"line\"><span class=\"regexp\">        );</span></span><br><span class=\"line\"><span class=\"regexp\">&#125;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"前端技术选型\"><a href=\"#前端技术选型\" class=\"headerlink\" title=\"前端技术选型\"></a>前端技术选型</h2><ul>\n<li>react 16.8.4</li>\n<li>Ant Design (<a href=\"https://v1.pro.ant.design/docs/getting-started-cn\" target=\"_blank\" rel=\"noopener\">Pro v1</a>) 3.15.0</li>\n<li>XMLRequest:axios 0.18.0</li>\n<li>路由: react-router-dom</li>\n<li><a href=\"https://ant.design/docs/react/recommendation-cn\" target=\"_blank\" rel=\"noopener\">精选组件</a></li>\n</ul>\n<h2 id=\"目录结构\"><a href=\"#目录结构\" class=\"headerlink\" title=\"目录结构\"></a>目录结构</h2><ul>\n<li>components 放置个性化组件<ul>\n<li>PageView 展示PV相关数据组件</li>\n</ul>\n</li>\n<li>contents 主体部分抽象布局<ul>\n<li>Overview 今日数据概览</li>\n<li>HistoryView 历史数据查询</li>\n</ul>\n</li>\n<li>pages 页面（目前为单页面）</li>\n</ul>\n<h2 id=\"React-Router\"><a href=\"#React-Router\" class=\"headerlink\" title=\"React-Router\"></a>React-Router</h2><ul>\n<li>react-router-dom 是对 react-router 在浏览器开发环境下的封装，所以没有本质差别</li>\n</ul>\n<h2 id=\"ant-design-pro\"><a href=\"#ant-design-pro\" class=\"headerlink\" title=\"ant design pro\"></a>ant design pro</h2><ul>\n<li>需要引入css文件<br><code>import &#39;ant-design-pro/dist/ant-design-pro.css&#39;;</code></li>\n</ul>\n<h2 id=\"BizChart\"><a href=\"#BizChart\" class=\"headerlink\" title=\"BizChart\"></a><a href=\"https://bizcharts.net/index\" target=\"_blank\" rel=\"noopener\">BizChart</a></h2><ul>\n<li>商业场景下的数据可视化</li>\n</ul>\n<h2 id=\"react相关\"><a href=\"#react相关\" class=\"headerlink\" title=\"react相关\"></a>react相关</h2><ul>\n<li><a href=\"http://www.cnblogs.com/xianyulaodi/p/5038258.html\" target=\"_blank\" rel=\"noopener\">dangerouslySetInnerHTML, 让React正常显示你的html代码</a></li>\n<li><p><a href=\"https://github.com/pvoznyuk/react-live-clock\" target=\"_blank\" rel=\"noopener\">时间显示组件</a></p>\n<ul>\n<li><p>获取今天的时间</p>\n<p><code>const today = moment().format(&#39;YYYYMMDD&#39;);</code></p>\n</li>\n</ul>\n</li>\n<li><p>react setState是异步调用的</p>\n<ul>\n<li>使用props和componentWillReceiveProps(nextProps)来解决</li>\n<li><a href=\"https://github.com/fangmiao97/MessageWiKiPro-frontend/blob/master/src/TopicInfo.js\" target=\"_blank\" rel=\"noopener\">见topicid的变换</a></li>\n<li>在minions中的历史数据页面，需要根据选择的不同日期显示相关数据。日期选择为父组件中的state.date改变，并将state.date传给child组件。child接收后通过componentWillReceiveProps来进行更新</li>\n</ul>\n</li>\n<li><p><a href=\"http://www.ruanyifeng.com/blog/2015/07/flex-grammar.html\" target=\"_blank\" rel=\"noopener\">flex布局</a></p>\n</li>\n<li><p>RestFul API</p>\n</li>\n<li><p>组件定时刷新</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">componentDidMount() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.timer = setInterval(</span><br><span class=\"line\">            () =&gt; &#123;</span><br><span class=\"line\">                <span class=\"comment\">//更新组件state...</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"number\">60000</span></span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>组件动态渲染</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//父组件</span></span><br><span class=\"line\"><span class=\"keyword\">this</span>.state = &#123;</span><br><span class=\"line\">            songsInfoList:[]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        render() &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> songInfoList = <span class=\"keyword\">this</span>.state.songsInfoList;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> (</span><br><span class=\"line\">                    &lt;div&gt;</span><br><span class=\"line\">                        &#123;</span><br><span class=\"line\">                            songInfoList.map(<span class=\"function\">(<span class=\"params\">item, index</span>) =&gt;</span> &#123;</span><br><span class=\"line\">                                <span class=\"keyword\">return</span> &lt;MockSongComponent key=&#123;index&#125; songInfo=&#123;item&#125;/&gt;</span><br><span class=\"line\">                            &#125;)</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &lt;/div&gt;</span><br><span class=\"line\">                );</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            </span><br><span class=\"line\"><span class=\"comment\">//子组件</span></span><br><span class=\"line\"><span class=\"keyword\">this</span>.state = &#123;</span><br><span class=\"line\">            songInfo: <span class=\"keyword\">this</span>.props.songInfo</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> songInfo = <span class=\"keyword\">this</span>.state.songInfo;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(</span><br><span class=\"line\">            &lt;div&gt;</span><br><span class=\"line\">              &#123;songInfo.name&#125;</span><br><span class=\"line\">              &#123;songInfo.coverUrl&#125;</span><br><span class=\"line\">              ......</span><br><span class=\"line\">            &lt;<span class=\"regexp\">/div&gt;</span></span><br><span class=\"line\"><span class=\"regexp\">        );</span></span><br><span class=\"line\"><span class=\"regexp\">&#125;</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"用户行为日志记录方案设计","date":"2019-04-21T16:00:00.000Z","_content":"* 设计思路\n\t* spark streaming作业根据不同的kafka topic进行消费\n\t* Topics\n\t\t* minions_songplay\n\t\t* minions_songlike\n\t\t* minions_songcomment\n \n* 前端埋点设计\n\n```javascript\n//播放歌曲记录\n    songplay(songInfo){\n\n        openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songplay',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('添加歌曲播放记录成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n\n    //收藏歌曲\n    likesong(songInfo){\n\n        //openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songlike',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('收藏成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n\n    //评论歌曲\n    comment(songInfo){\n\n        //openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songcomment',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('评论成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n```\n\n* 服务端记录\n\n`logger.info(\"topic:\" + k_topic + \" songID:\" + songID);`\n\n* log4j\n\n```properties\nlog4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender\nlog4j.appender.flume.Hostname = hadoop000\nlog4j.appender.flume.Port = 41415\nlog4j.appender.flume.UnsafeMode = true\nlog4j.appender.flume.layout=org.apache.log4j.PatternLayout\nlog4j.appender.flume.layout.ConversionPattern= %d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n\n```\n`2019-04-22 13:58:04,419 [http-nio-8080-exec-9] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songlike songID:13`\n\n* spark streaming作业根据不同kafka topic划分\n\n\n\n\n","source":"_posts/user-action-logger-design.md","raw":"---\ntitle: 用户行为日志记录方案设计\ndate: 2019/04/22\ncategories:\n    - 技术总结\ntags:\n    - 开发相关\n    - 毕业设计\n---\n* 设计思路\n\t* spark streaming作业根据不同的kafka topic进行消费\n\t* Topics\n\t\t* minions_songplay\n\t\t* minions_songlike\n\t\t* minions_songcomment\n \n* 前端埋点设计\n\n```javascript\n//播放歌曲记录\n    songplay(songInfo){\n\n        openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songplay',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('添加歌曲播放记录成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n\n    //收藏歌曲\n    likesong(songInfo){\n\n        //openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songlike',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('收藏成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n\n    //评论歌曲\n    comment(songInfo){\n\n        //openNotification(songInfo);\n\n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", {\n            params:{\n                K_topic: 'minions_songcomment',\n                songId: songInfo.songID\n            }\n        }).then(function (response) {\n            if (response.data === 1){\n                message.success('评论成功');\n                console.log(\"日志记录成功\")\n            } else console.log(\"日志记录错误\")\n        }).catch(function (error) {\n            console.log(error)\n        })\n    }\n```\n\n* 服务端记录\n\n`logger.info(\"topic:\" + k_topic + \" songID:\" + songID);`\n\n* log4j\n\n```properties\nlog4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender\nlog4j.appender.flume.Hostname = hadoop000\nlog4j.appender.flume.Port = 41415\nlog4j.appender.flume.UnsafeMode = true\nlog4j.appender.flume.layout=org.apache.log4j.PatternLayout\nlog4j.appender.flume.layout.ConversionPattern= %d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n\n```\n`2019-04-22 13:58:04,419 [http-nio-8080-exec-9] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songlike songID:13`\n\n* spark streaming作业根据不同kafka topic划分\n\n\n\n\n","slug":"user-action-logger-design","published":1,"updated":"2019-06-04T01:06:18.984Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwer001348upa10cchxy","content":"<ul>\n<li><p>设计思路</p>\n<ul>\n<li>spark streaming作业根据不同的kafka topic进行消费</li>\n<li>Topics<ul>\n<li>minions_songplay</li>\n<li>minions_songlike</li>\n<li>minions_songcomment</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>前端埋点设计</p>\n</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//播放歌曲记录</span></span><br><span class=\"line\">    songplay(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        openNotification(songInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songplay'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'添加歌曲播放记录成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//收藏歌曲</span></span><br><span class=\"line\">    likesong(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//openNotification(songInfo);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songlike'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'收藏成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//评论歌曲</span></span><br><span class=\"line\">    comment(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//openNotification(songInfo);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songcomment'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'评论成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>服务端记录</li>\n</ul>\n<p><code>logger.info(&quot;topic:&quot; + k_topic + &quot; songID:&quot; + songID);</code></p>\n<ul>\n<li>log4j</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class=\"line\">log4j.appender.flume.Hostname = hadoop000</span><br><span class=\"line\">log4j.appender.flume.Port = 41415</span><br><span class=\"line\">log4j.appender.flume.UnsafeMode = true</span><br><span class=\"line\">log4j.appender.flume.layout=org.apache.log4j.PatternLayout</span><br><span class=\"line\">log4j.appender.flume.layout.ConversionPattern= %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure>\n<p><code>2019-04-22 13:58:04,419 [http-nio-8080-exec-9] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songlike songID:13</code></p>\n<ul>\n<li>spark streaming作业根据不同kafka topic划分</li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<ul>\n<li><p>设计思路</p>\n<ul>\n<li>spark streaming作业根据不同的kafka topic进行消费</li>\n<li>Topics<ul>\n<li>minions_songplay</li>\n<li>minions_songlike</li>\n<li>minions_songcomment</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>前端埋点设计</p>\n</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//播放歌曲记录</span></span><br><span class=\"line\">    songplay(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        openNotification(songInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songplay'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'添加歌曲播放记录成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//收藏歌曲</span></span><br><span class=\"line\">    likesong(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//openNotification(songInfo);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songlike'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'收藏成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//评论歌曲</span></span><br><span class=\"line\">    comment(songInfo)&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//openNotification(songInfo);</span></span><br><span class=\"line\"></span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123;</span><br><span class=\"line\">            params:&#123;</span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songcomment'</span>,</span><br><span class=\"line\">                songId: songInfo.songID</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;).then(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">                message.success(<span class=\"string\">'评论成功'</span>);</span><br><span class=\"line\">                <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录成功\"</span>)</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"built_in\">console</span>.log(<span class=\"string\">\"日志记录错误\"</span>)</span><br><span class=\"line\">        &#125;).catch(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">error</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"built_in\">console</span>.log(error)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>服务端记录</li>\n</ul>\n<p><code>logger.info(&quot;topic:&quot; + k_topic + &quot; songID:&quot; + songID);</code></p>\n<ul>\n<li>log4j</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class=\"line\">log4j.appender.flume.Hostname = hadoop000</span><br><span class=\"line\">log4j.appender.flume.Port = 41415</span><br><span class=\"line\">log4j.appender.flume.UnsafeMode = true</span><br><span class=\"line\">log4j.appender.flume.layout=org.apache.log4j.PatternLayout</span><br><span class=\"line\">log4j.appender.flume.layout.ConversionPattern= %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure>\n<p><code>2019-04-22 13:58:04,419 [http-nio-8080-exec-9] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songlike songID:13</code></p>\n<ul>\n<li>spark streaming作业根据不同kafka topic划分</li>\n</ul>\n"},{"title":"访问行为mock","date":"2019-04-21T16:00:00.000Z","_content":"模拟用户访问的Python脚本\n\n* generate_log.py\n\n```python\n#coding=UTF-8\n\nimport random\nimport time\n\nurl_paths = {\n\t\"class/112.html\":10,\n\t\"class/128.html\":5,\n\t\"class/145.html\":8,\n\t\"class/146.html\":10,\n\t\"class/131.html\":3,\n\t\"class/130.html\":2,\n\t\"learn/821\":1,\n\t\"course/list\":1\n}\n\nip_slices = [132,156,124,10,29,167,143,187,30,46,55,63,72,87,98,168]\n\nhttp_referers = [\n\t\"http://www.baidu.com/s?wd={query}\",\n\t\"https://www.sogou.com/web?query={query}\",\n\t\"http://cn.bing.com/search?q={query}\",\n\t\"https://search.yahoo.com/search?p={query}\",\n]\n\nsearch_keyword = [\n\t\"新近发布\",\n\t\"为你推荐\",\n\t\"今日歌单\",\n\t\"瞩目艺人\",\n\t\"今日专辑\"\n]\n\nstatus_codes = [\"200\",\"404\",\"500\"]\n\ndef sample_url():\n\tall_data = []\n\tfor v, w in url_paths.items():\n\t\ttemp = []\n\t\tfor i in range(w):\n\t\t\ttemp.append(v)\n\t\tall_data.extend(temp)\n\n\treturn random.sample(all_data, 1)[0]\n\ndef sample_ip():\n\tslice = random.sample(ip_slices , 4)\n\treturn \".\".join([str(item) for item in slice])\n\ndef sample_referer():\n\tif random.uniform(0, 1) > 0.2:\n\t\treturn \"-\"\n\n\trefer_str = random.sample(http_referers, 1)\n\tquery_str = random.sample(search_keyword, 1)\n\treturn refer_str[0].format(query=query_str[0])\n\ndef sample_status_code():\n\treturn random.sample(status_codes, 1)[0]\n\ndef generate_log():\n\t\n\ttime_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tnow_hour = time.strftime(\"%H\", time.localtime())\n\t\n\tif now_hour < \"01\":\n\t\tcount = random.randint(80,100)\n\telif now_hour < \"02\":\n\t\tcount = random.randint(60,85)\n\telif now_hour < \"03\":\n\t\tcount = random.randint(50,75)\n\telif now_hour < \"05\":\n\t\tcount = random.randint(30,60)\n\telif now_hour < \"06\":\n\t\tcount = random.randint(50,65)\n\telif now_hour < \"08\":\n\t\tcount = random.randint(70,85)\n\telif now_hour < \"10\":\n\t\tcount = random.randint(70,95)\n\telif now_hour < \"11\":\n\t\tcount = random.randint(80,100)\n\telif now_hour < \"13\":\n\t\tcount = random.randint(95,150)\n\telif now_hour < \"15\":\n\t\tcount = random.randint(90,130)\n\telif now_hour < \"17\":\n\t\tcount = random.randint(80,120)\n\telif now_hour < \"18\":\n\t\tcount = random.randint(85,130)\t\n\telif now_hour < \"19\":\n\t\tcount = random.randint(100,120)\n\telse:\n\t\tcount = random.randint(110,160)\t\n\n\tf = open(\"/home/hadoop/data/project/logs/access.log\",\"w+\")\n\n\twhile count >= 1:\n\t\tquery_log = \"{ip}\\t{local_time}\\t\\\"GET /{url} HTTP/1.1\\\"\\t{status_code}\\t{referer}\".format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)\n\n\t\tf.write(query_log + \"\\n\")\n\n\t\tcount = count - 1 \n\nif __name__ == '__main__':\n\tgenerate_log()\n\t\n```\n\n* 定时执行脚本\n    * Linux crontab\n    * [工具网站](https://tool.lu/crontab/)","source":"_posts/mock-accesslog-python.md","raw":"---\ntitle: 访问行为mock\ndate: 2019/04/22\ncategories:\n    - 技术总结\ntags:\n    - 毕业设计\n---\n模拟用户访问的Python脚本\n\n* generate_log.py\n\n```python\n#coding=UTF-8\n\nimport random\nimport time\n\nurl_paths = {\n\t\"class/112.html\":10,\n\t\"class/128.html\":5,\n\t\"class/145.html\":8,\n\t\"class/146.html\":10,\n\t\"class/131.html\":3,\n\t\"class/130.html\":2,\n\t\"learn/821\":1,\n\t\"course/list\":1\n}\n\nip_slices = [132,156,124,10,29,167,143,187,30,46,55,63,72,87,98,168]\n\nhttp_referers = [\n\t\"http://www.baidu.com/s?wd={query}\",\n\t\"https://www.sogou.com/web?query={query}\",\n\t\"http://cn.bing.com/search?q={query}\",\n\t\"https://search.yahoo.com/search?p={query}\",\n]\n\nsearch_keyword = [\n\t\"新近发布\",\n\t\"为你推荐\",\n\t\"今日歌单\",\n\t\"瞩目艺人\",\n\t\"今日专辑\"\n]\n\nstatus_codes = [\"200\",\"404\",\"500\"]\n\ndef sample_url():\n\tall_data = []\n\tfor v, w in url_paths.items():\n\t\ttemp = []\n\t\tfor i in range(w):\n\t\t\ttemp.append(v)\n\t\tall_data.extend(temp)\n\n\treturn random.sample(all_data, 1)[0]\n\ndef sample_ip():\n\tslice = random.sample(ip_slices , 4)\n\treturn \".\".join([str(item) for item in slice])\n\ndef sample_referer():\n\tif random.uniform(0, 1) > 0.2:\n\t\treturn \"-\"\n\n\trefer_str = random.sample(http_referers, 1)\n\tquery_str = random.sample(search_keyword, 1)\n\treturn refer_str[0].format(query=query_str[0])\n\ndef sample_status_code():\n\treturn random.sample(status_codes, 1)[0]\n\ndef generate_log():\n\t\n\ttime_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tnow_hour = time.strftime(\"%H\", time.localtime())\n\t\n\tif now_hour < \"01\":\n\t\tcount = random.randint(80,100)\n\telif now_hour < \"02\":\n\t\tcount = random.randint(60,85)\n\telif now_hour < \"03\":\n\t\tcount = random.randint(50,75)\n\telif now_hour < \"05\":\n\t\tcount = random.randint(30,60)\n\telif now_hour < \"06\":\n\t\tcount = random.randint(50,65)\n\telif now_hour < \"08\":\n\t\tcount = random.randint(70,85)\n\telif now_hour < \"10\":\n\t\tcount = random.randint(70,95)\n\telif now_hour < \"11\":\n\t\tcount = random.randint(80,100)\n\telif now_hour < \"13\":\n\t\tcount = random.randint(95,150)\n\telif now_hour < \"15\":\n\t\tcount = random.randint(90,130)\n\telif now_hour < \"17\":\n\t\tcount = random.randint(80,120)\n\telif now_hour < \"18\":\n\t\tcount = random.randint(85,130)\t\n\telif now_hour < \"19\":\n\t\tcount = random.randint(100,120)\n\telse:\n\t\tcount = random.randint(110,160)\t\n\n\tf = open(\"/home/hadoop/data/project/logs/access.log\",\"w+\")\n\n\twhile count >= 1:\n\t\tquery_log = \"{ip}\\t{local_time}\\t\\\"GET /{url} HTTP/1.1\\\"\\t{status_code}\\t{referer}\".format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)\n\n\t\tf.write(query_log + \"\\n\")\n\n\t\tcount = count - 1 \n\nif __name__ == '__main__':\n\tgenerate_log()\n\t\n```\n\n* 定时执行脚本\n    * Linux crontab\n    * [工具网站](https://tool.lu/crontab/)","slug":"mock-accesslog-python","published":1,"updated":"2019-06-04T01:06:18.980Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwf3001648upcidqoanj","content":"<p>模拟用户访问的Python脚本</p>\n<ul>\n<li>generate_log.py</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#coding=UTF-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"></span><br><span class=\"line\">url_paths = &#123;</span><br><span class=\"line\">\t<span class=\"string\">\"class/112.html\"</span>:<span class=\"number\">10</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/128.html\"</span>:<span class=\"number\">5</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/145.html\"</span>:<span class=\"number\">8</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/146.html\"</span>:<span class=\"number\">10</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/131.html\"</span>:<span class=\"number\">3</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/130.html\"</span>:<span class=\"number\">2</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"learn/821\"</span>:<span class=\"number\">1</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"course/list\"</span>:<span class=\"number\">1</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">ip_slices = [<span class=\"number\">132</span>,<span class=\"number\">156</span>,<span class=\"number\">124</span>,<span class=\"number\">10</span>,<span class=\"number\">29</span>,<span class=\"number\">167</span>,<span class=\"number\">143</span>,<span class=\"number\">187</span>,<span class=\"number\">30</span>,<span class=\"number\">46</span>,<span class=\"number\">55</span>,<span class=\"number\">63</span>,<span class=\"number\">72</span>,<span class=\"number\">87</span>,<span class=\"number\">98</span>,<span class=\"number\">168</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">http_referers = [</span><br><span class=\"line\">\t<span class=\"string\">\"http://www.baidu.com/s?wd=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"https://www.sogou.com/web?query=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"http://cn.bing.com/search?q=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"https://search.yahoo.com/search?p=&#123;query&#125;\"</span>,</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">search_keyword = [</span><br><span class=\"line\">\t<span class=\"string\">\"新近发布\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"为你推荐\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"今日歌单\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"瞩目艺人\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"今日专辑\"</span></span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">status_codes = [<span class=\"string\">\"200\"</span>,<span class=\"string\">\"404\"</span>,<span class=\"string\">\"500\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_url</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\tall_data = []</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> v, w <span class=\"keyword\">in</span> url_paths.items():</span><br><span class=\"line\">\t\ttemp = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(w):</span><br><span class=\"line\">\t\t\ttemp.append(v)</span><br><span class=\"line\">\t\tall_data.extend(temp)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> random.sample(all_data, <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_ip</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\tslice = random.sample(ip_slices , <span class=\"number\">4</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"string\">\".\"</span>.join([str(item) <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> slice])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_referer</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> random.uniform(<span class=\"number\">0</span>, <span class=\"number\">1</span>) &gt; <span class=\"number\">0.2</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"-\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\trefer_str = random.sample(http_referers, <span class=\"number\">1</span>)</span><br><span class=\"line\">\tquery_str = random.sample(search_keyword, <span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> refer_str[<span class=\"number\">0</span>].format(query=query_str[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_status_code</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> random.sample(status_codes, <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_log</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\ttime_str = time.strftime(<span class=\"string\">\"%Y-%m-%d %H:%M:%S\"</span>, time.localtime())</span><br><span class=\"line\">\tnow_hour = time.strftime(<span class=\"string\">\"%H\"</span>, time.localtime())</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> now_hour &lt; <span class=\"string\">\"01\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">100</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"02\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">60</span>,<span class=\"number\">85</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"03\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">50</span>,<span class=\"number\">75</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"05\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">30</span>,<span class=\"number\">60</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"06\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">50</span>,<span class=\"number\">65</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"08\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">70</span>,<span class=\"number\">85</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"10\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">70</span>,<span class=\"number\">95</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"11\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">100</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"13\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">95</span>,<span class=\"number\">150</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"15\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">90</span>,<span class=\"number\">130</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"17\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">120</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"18\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">85</span>,<span class=\"number\">130</span>)\t</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"19\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">100</span>,<span class=\"number\">120</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">110</span>,<span class=\"number\">160</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\tf = open(<span class=\"string\">\"/home/hadoop/data/project/logs/access.log\"</span>,<span class=\"string\">\"w+\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> count &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\tquery_log = <span class=\"string\">\"&#123;ip&#125;\\t&#123;local_time&#125;\\t\\\"GET /&#123;url&#125; HTTP/1.1\\\"\\t&#123;status_code&#125;\\t&#123;referer&#125;\"</span>.format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tf.write(query_log + <span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcount = count - <span class=\"number\">1</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">\tgenerate_log()</span><br></pre></td></tr></table></figure>\n<ul>\n<li>定时执行脚本<ul>\n<li>Linux crontab</li>\n<li><a href=\"https://tool.lu/crontab/\" target=\"_blank\" rel=\"noopener\">工具网站</a></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p>模拟用户访问的Python脚本</p>\n<ul>\n<li>generate_log.py</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#coding=UTF-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"></span><br><span class=\"line\">url_paths = &#123;</span><br><span class=\"line\">\t<span class=\"string\">\"class/112.html\"</span>:<span class=\"number\">10</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/128.html\"</span>:<span class=\"number\">5</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/145.html\"</span>:<span class=\"number\">8</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/146.html\"</span>:<span class=\"number\">10</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/131.html\"</span>:<span class=\"number\">3</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"class/130.html\"</span>:<span class=\"number\">2</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"learn/821\"</span>:<span class=\"number\">1</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"course/list\"</span>:<span class=\"number\">1</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">ip_slices = [<span class=\"number\">132</span>,<span class=\"number\">156</span>,<span class=\"number\">124</span>,<span class=\"number\">10</span>,<span class=\"number\">29</span>,<span class=\"number\">167</span>,<span class=\"number\">143</span>,<span class=\"number\">187</span>,<span class=\"number\">30</span>,<span class=\"number\">46</span>,<span class=\"number\">55</span>,<span class=\"number\">63</span>,<span class=\"number\">72</span>,<span class=\"number\">87</span>,<span class=\"number\">98</span>,<span class=\"number\">168</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">http_referers = [</span><br><span class=\"line\">\t<span class=\"string\">\"http://www.baidu.com/s?wd=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"https://www.sogou.com/web?query=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"http://cn.bing.com/search?q=&#123;query&#125;\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"https://search.yahoo.com/search?p=&#123;query&#125;\"</span>,</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">search_keyword = [</span><br><span class=\"line\">\t<span class=\"string\">\"新近发布\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"为你推荐\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"今日歌单\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"瞩目艺人\"</span>,</span><br><span class=\"line\">\t<span class=\"string\">\"今日专辑\"</span></span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">status_codes = [<span class=\"string\">\"200\"</span>,<span class=\"string\">\"404\"</span>,<span class=\"string\">\"500\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_url</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\tall_data = []</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> v, w <span class=\"keyword\">in</span> url_paths.items():</span><br><span class=\"line\">\t\ttemp = []</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(w):</span><br><span class=\"line\">\t\t\ttemp.append(v)</span><br><span class=\"line\">\t\tall_data.extend(temp)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> random.sample(all_data, <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_ip</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\tslice = random.sample(ip_slices , <span class=\"number\">4</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"string\">\".\"</span>.join([str(item) <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> slice])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_referer</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> random.uniform(<span class=\"number\">0</span>, <span class=\"number\">1</span>) &gt; <span class=\"number\">0.2</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"-\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">\trefer_str = random.sample(http_referers, <span class=\"number\">1</span>)</span><br><span class=\"line\">\tquery_str = random.sample(search_keyword, <span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> refer_str[<span class=\"number\">0</span>].format(query=query_str[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_status_code</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> random.sample(status_codes, <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_log</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\ttime_str = time.strftime(<span class=\"string\">\"%Y-%m-%d %H:%M:%S\"</span>, time.localtime())</span><br><span class=\"line\">\tnow_hour = time.strftime(<span class=\"string\">\"%H\"</span>, time.localtime())</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> now_hour &lt; <span class=\"string\">\"01\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">100</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"02\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">60</span>,<span class=\"number\">85</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"03\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">50</span>,<span class=\"number\">75</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"05\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">30</span>,<span class=\"number\">60</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"06\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">50</span>,<span class=\"number\">65</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"08\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">70</span>,<span class=\"number\">85</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"10\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">70</span>,<span class=\"number\">95</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"11\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">100</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"13\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">95</span>,<span class=\"number\">150</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"15\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">90</span>,<span class=\"number\">130</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"17\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">80</span>,<span class=\"number\">120</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"18\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">85</span>,<span class=\"number\">130</span>)\t</span><br><span class=\"line\">\t<span class=\"keyword\">elif</span> now_hour &lt; <span class=\"string\">\"19\"</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">100</span>,<span class=\"number\">120</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\tcount = random.randint(<span class=\"number\">110</span>,<span class=\"number\">160</span>)\t</span><br><span class=\"line\"></span><br><span class=\"line\">\tf = open(<span class=\"string\">\"/home/hadoop/data/project/logs/access.log\"</span>,<span class=\"string\">\"w+\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> count &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\tquery_log = <span class=\"string\">\"&#123;ip&#125;\\t&#123;local_time&#125;\\t\\\"GET /&#123;url&#125; HTTP/1.1\\\"\\t&#123;status_code&#125;\\t&#123;referer&#125;\"</span>.format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tf.write(query_log + <span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcount = count - <span class=\"number\">1</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">\tgenerate_log()</span><br></pre></td></tr></table></figure>\n<ul>\n<li>定时执行脚本<ul>\n<li>Linux crontab</li>\n<li><a href=\"https://tool.lu/crontab/\" target=\"_blank\" rel=\"noopener\">工具网站</a></li>\n</ul>\n</li>\n</ul>\n"},{"title":"每周记录📝（190414）","date":"2019-04-13T16:00:00.000Z","_content":"![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g21zk7ocndj22482tq7wh.jpg)\n\n上一周没有周记，因为清明节回外婆家了。比较忙，也没有带电脑。\n所以说，真的好像要自己的一台电脑啊！\n\n另外也就是说，我的毕设在清明节之前的周四之后就没有弄过了！LeetCode也没有刷！真的很罪恶了。\n这周开始全身心投入！\n\n这里先定一下这周的计划：\n\n* 学习HBase RowKey的设计，以及java API的学习，记笔记。\n* 用BizCharts完成毕设页面浏览信息的时间维度细化。\n* 细化页面行为信息\n\nOK，展示一下目前毕设的状态：\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2207nk3atj21gn0u0tjq.jpg)\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2207nmb18j21gn0u0kcv.jpg)\n\n另外，喜欢上喝铁观音。自己一周之内被骗了2次，损失36元！谨记！\n\nbe humble！","source":"_posts/weekly-report-190414.md","raw":"---\ntitle: 每周记录📝（190414）\ndate: 2019/4/14\ncategories:\n    - 每周记录\n---\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g21zk7ocndj22482tq7wh.jpg)\n\n上一周没有周记，因为清明节回外婆家了。比较忙，也没有带电脑。\n所以说，真的好像要自己的一台电脑啊！\n\n另外也就是说，我的毕设在清明节之前的周四之后就没有弄过了！LeetCode也没有刷！真的很罪恶了。\n这周开始全身心投入！\n\n这里先定一下这周的计划：\n\n* 学习HBase RowKey的设计，以及java API的学习，记笔记。\n* 用BizCharts完成毕设页面浏览信息的时间维度细化。\n* 细化页面行为信息\n\nOK，展示一下目前毕设的状态：\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g2207nk3atj21gn0u0tjq.jpg)\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g2207nmb18j21gn0u0kcv.jpg)\n\n另外，喜欢上喝铁观音。自己一周之内被骗了2次，损失36元！谨记！\n\nbe humble！","slug":"weekly-report-190414","published":1,"updated":"2019-06-04T01:06:18.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwfc001a48upuevklzch","content":"<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g21zk7ocndj22482tq7wh.jpg\" alt></p>\n<p>上一周没有周记，因为清明节回外婆家了。比较忙，也没有带电脑。<br>所以说，真的好像要自己的一台电脑啊！</p>\n<p>另外也就是说，我的毕设在清明节之前的周四之后就没有弄过了！LeetCode也没有刷！真的很罪恶了。<br>这周开始全身心投入！</p>\n<p>这里先定一下这周的计划：</p>\n<ul>\n<li>学习HBase RowKey的设计，以及java API的学习，记笔记。</li>\n<li>用BizCharts完成毕设页面浏览信息的时间维度细化。</li>\n<li>细化页面行为信息</li>\n</ul>\n<p>OK，展示一下目前毕设的状态：</p>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2207nk3atj21gn0u0tjq.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2207nmb18j21gn0u0kcv.jpg\" alt></p>\n<p>另外，喜欢上喝铁观音。自己一周之内被骗了2次，损失36元！谨记！</p>\n<p>be humble！</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g21zk7ocndj22482tq7wh.jpg\" alt></p>\n<p>上一周没有周记，因为清明节回外婆家了。比较忙，也没有带电脑。<br>所以说，真的好像要自己的一台电脑啊！</p>\n<p>另外也就是说，我的毕设在清明节之前的周四之后就没有弄过了！LeetCode也没有刷！真的很罪恶了。<br>这周开始全身心投入！</p>\n<p>这里先定一下这周的计划：</p>\n<ul>\n<li>学习HBase RowKey的设计，以及java API的学习，记笔记。</li>\n<li>用BizCharts完成毕设页面浏览信息的时间维度细化。</li>\n<li>细化页面行为信息</li>\n</ul>\n<p>OK，展示一下目前毕设的状态：</p>\n<p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g2207nk3atj21gn0u0tjq.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g2207nmb18j21gn0u0kcv.jpg\" alt></p>\n<p>另外，喜欢上喝铁观音。自己一周之内被骗了2次，损失36元！谨记！</p>\n<p>be humble！</p>\n"},{"title":"每周记录📝（190330）","date":"2019-03-29T16:00:00.000Z","_content":"这周看了乔布斯的传记，其中说到乔布斯做产品的时候，追求完美。\n\n这周把毕设完善了一下：\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1l2zafpgwj21hc0u079v.jpg)\n\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1l2z9yalhj21hc0u0n9a.jpg)\n\n\n","source":"_posts/weeklyReport190330.md","raw":"---\ntitle: 每周记录📝（190330）\ndate: 2019/3/30\ncategories:\n    - 每周记录\n---\n这周看了乔布斯的传记，其中说到乔布斯做产品的时候，追求完美。\n\n这周把毕设完善了一下：\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1l2zafpgwj21hc0u079v.jpg)\n\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1l2z9yalhj21hc0u0n9a.jpg)\n\n\n","slug":"weeklyReport190330","published":1,"updated":"2019-06-04T01:06:18.987Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwfh001d48upj0lvysn3","content":"<p>这周看了乔布斯的传记，其中说到乔布斯做产品的时候，追求完美。</p>\n<p>这周把毕设完善了一下：<br><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1l2zafpgwj21hc0u079v.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1l2z9yalhj21hc0u0n9a.jpg\" alt></p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p>这周看了乔布斯的传记，其中说到乔布斯做产品的时候，追求完美。</p>\n<p>这周把毕设完善了一下：<br><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1l2zafpgwj21hc0u079v.jpg\" alt></p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1l2z9yalhj21hc0u0n9a.jpg\" alt></p>\n"},{"title":"每周记录📝（190420）","date":"2019-04-19T16:00:00.000Z","_content":"![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xszz4gj23402c0qv7.jpg)\n\n这周吃的铁板烧！大学里面最喜欢吃的东西，害怕以后就吃不到了。\n\n这周收到南洋理工CCA的offer了，IS被拒了。其实吧，我更想上IS，但是看了一下IS的课程，感觉除了个别及门，其他的在本科也有学过。\nCCA的话，就偏向控制之类的，我对这方面兴趣不大吧，以后就想做软件、大数据方面的。在贴吧上看到有学长说，找工作其实看自己的能力，确实也是的，所以CCA上也可以，主要我就是想出去。\n另外国立大学的申请还没有下来，有点捉急，因为国大有SO，如果国大有的话，我更愿意接受国大的，去签SO。\n\n这周前四天集中做毕设，效果如图：\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbiggj21i00ustmi.jpg)\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbic3j21i00us14u.jpg)\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xqa08vj21i00usgwg.jpg)\n\n维度细化了，界面交互也有了逻辑。有问题就是分类别统计30分钟窗口的时候，记录的rowkey设计有问题，所以在零点的左右会出现问题，我是按date在hbase中取值，如果window横跨零点左右，前一天就统计不到。\n\n另外，行为分析还要增添一点东西。","source":"_posts/weekly-report-190420.md","raw":"---\ntitle: 每周记录📝（190420）\ndate: 2019/4/20\ncategories:\n    - 每周记录\n---\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xszz4gj23402c0qv7.jpg)\n\n这周吃的铁板烧！大学里面最喜欢吃的东西，害怕以后就吃不到了。\n\n这周收到南洋理工CCA的offer了，IS被拒了。其实吧，我更想上IS，但是看了一下IS的课程，感觉除了个别及门，其他的在本科也有学过。\nCCA的话，就偏向控制之类的，我对这方面兴趣不大吧，以后就想做软件、大数据方面的。在贴吧上看到有学长说，找工作其实看自己的能力，确实也是的，所以CCA上也可以，主要我就是想出去。\n另外国立大学的申请还没有下来，有点捉急，因为国大有SO，如果国大有的话，我更愿意接受国大的，去签SO。\n\n这周前四天集中做毕设，效果如图：\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbiggj21i00ustmi.jpg)\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbic3j21i00us14u.jpg)\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xqa08vj21i00usgwg.jpg)\n\n维度细化了，界面交互也有了逻辑。有问题就是分类别统计30分钟窗口的时候，记录的rowkey设计有问题，所以在零点的左右会出现问题，我是按date在hbase中取值，如果window横跨零点左右，前一天就统计不到。\n\n另外，行为分析还要增添一点东西。","slug":"weekly-report-190420","published":1,"updated":"2019-06-04T01:06:18.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwfp001g48up6vnjz5hy","content":"<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xszz4gj23402c0qv7.jpg\" alt></p>\n<p>这周吃的铁板烧！大学里面最喜欢吃的东西，害怕以后就吃不到了。</p>\n<p>这周收到南洋理工CCA的offer了，IS被拒了。其实吧，我更想上IS，但是看了一下IS的课程，感觉除了个别及门，其他的在本科也有学过。<br>CCA的话，就偏向控制之类的，我对这方面兴趣不大吧，以后就想做软件、大数据方面的。在贴吧上看到有学长说，找工作其实看自己的能力，确实也是的，所以CCA上也可以，主要我就是想出去。<br>另外国立大学的申请还没有下来，有点捉急，因为国大有SO，如果国大有的话，我更愿意接受国大的，去签SO。</p>\n<p>这周前四天集中做毕设，效果如图：</p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbiggj21i00ustmi.jpg\" alt></p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbic3j21i00us14u.jpg\" alt></p>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xqa08vj21i00usgwg.jpg\" alt></p>\n<p>维度细化了，界面交互也有了逻辑。有问题就是分类别统计30分钟窗口的时候，记录的rowkey设计有问题，所以在零点的左右会出现问题，我是按date在hbase中取值，如果window横跨零点左右，前一天就统计不到。</p>\n<p>另外，行为分析还要增添一点东西。</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xszz4gj23402c0qv7.jpg\" alt></p>\n<p>这周吃的铁板烧！大学里面最喜欢吃的东西，害怕以后就吃不到了。</p>\n<p>这周收到南洋理工CCA的offer了，IS被拒了。其实吧，我更想上IS，但是看了一下IS的课程，感觉除了个别及门，其他的在本科也有学过。<br>CCA的话，就偏向控制之类的，我对这方面兴趣不大吧，以后就想做软件、大数据方面的。在贴吧上看到有学长说，找工作其实看自己的能力，确实也是的，所以CCA上也可以，主要我就是想出去。<br>另外国立大学的申请还没有下来，有点捉急，因为国大有SO，如果国大有的话，我更愿意接受国大的，去签SO。</p>\n<p>这周前四天集中做毕设，效果如图：</p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbiggj21i00ustmi.jpg\" alt></p>\n<p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g294xqbic3j21i00us14u.jpg\" alt></p>\n<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g294xqa08vj21i00usgwg.jpg\" alt></p>\n<p>维度细化了，界面交互也有了逻辑。有问题就是分类别统计30分钟窗口的时候，记录的rowkey设计有问题，所以在零点的左右会出现问题，我是按date在hbase中取值，如果window横跨零点左右，前一天就统计不到。</p>\n<p>另外，行为分析还要增添一点东西。</p>\n"},{"title":"微信小程序开发🚀","date":"2019-03-13T16:00:00.000Z","_content":"\n\n## 布局样式📱\n*\t[入门教程视频](https://www.imooc.com/learn/974)\n*  [基础代码教程](http://www.imooc.com/code/49)\n*  相关实用网站\n\t*  [菜鸟教程](http://www.runoob.com/css/css-tutorial.html)\n\t*  [WXML与WXSS](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#wxml-%E6%A8%A1%E6%9D%BF)\n\t*  [WXSS主要注意尺寸单位](https://developers.weixin.qq.com/miniprogram/dev/framework/view/wxss.html)\n\t*  [小程序的基本组件](https://developers.weixin.qq.com/miniprogram/dev/component/)\n\n## 逻辑🗯\n* [PWA](https://developers.weixin.qq.com/community/develop/article/doc/000aa057ca0a88dcd938b4d6656813)(了解）\n* [关于HTML5/小程序的产生](https://www.v2ex.com/t/427255)(了解）\n* [JS逻辑交互](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#js-%E4%BA%A4%E4%BA%92%E9%80%BB%E8%BE%91)\n* 相关实用网站\n\t* [小程序的API](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/framework.html#api)\n\t* [JavaScript](http://www.runoob.com/js/js-tutorial.html)\n\t* [JSON教程](http://www.runoob.com/json/json-tutorial.html)\n\n## 其他实用网站🕸\n*\t[官方社区](https://developers.weixin.qq.com/):有的坑可能别人已经踩过，可以搜索一下\n* [知晓程序](https://minapp.com/miniapp/)：可以看看别人产品的想法和思路\n* [WePY](https://tencent.github.io/wepy/)：开发小程序的一个框架。建议看看就好，还是以原生开发为主。等原生上手后再看看框架。\n* [工大思政课](https://github.com/Shincey/HFUTSZK):无网络交互，可以独立开发完成。\n* [AI小部落](https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram)：js逻辑中有网络交互，可以做参考。\n* 各种css样式配置web工具\n\t* [box、卡片的阴影效果](https://www.html.cn/tool/css3Preview/Box-Shadow.html)\n\t* ......\n\n## 建议🍻\n* 先学习基础**HTML/CSS/JavaScript**，不用特别抓细节，但是要知晓常见用法\n* 理清三个方面：\n\t* 如何布局、添加样式\n\t* 逻辑层与 ui 层如何交互，或者说如何用代码操控 ui 组件\n\t* 如何处理用户的交互动作\n* 先从简单的功能开始开发，如，点击按钮显示图片文本。再逐渐复杂逻辑\n* 小程序推荐用原生开发，先不要用框架\n* 看官方文档，也要结合搜索引擎。因为微信的官方文档，有坑\n* 每天新增一行代码都是进步😃加油！\n* 逐渐理解[生命周期](https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/page.html#%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F)\n","source":"_posts/微信小程序学习意见.md","raw":"---\ntitle: 微信小程序开发🚀\ndate: 2019/3/14\ntags:\n- 开发相关\n---\n\n\n## 布局样式📱\n*\t[入门教程视频](https://www.imooc.com/learn/974)\n*  [基础代码教程](http://www.imooc.com/code/49)\n*  相关实用网站\n\t*  [菜鸟教程](http://www.runoob.com/css/css-tutorial.html)\n\t*  [WXML与WXSS](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#wxml-%E6%A8%A1%E6%9D%BF)\n\t*  [WXSS主要注意尺寸单位](https://developers.weixin.qq.com/miniprogram/dev/framework/view/wxss.html)\n\t*  [小程序的基本组件](https://developers.weixin.qq.com/miniprogram/dev/component/)\n\n## 逻辑🗯\n* [PWA](https://developers.weixin.qq.com/community/develop/article/doc/000aa057ca0a88dcd938b4d6656813)(了解）\n* [关于HTML5/小程序的产生](https://www.v2ex.com/t/427255)(了解）\n* [JS逻辑交互](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#js-%E4%BA%A4%E4%BA%92%E9%80%BB%E8%BE%91)\n* 相关实用网站\n\t* [小程序的API](https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/framework.html#api)\n\t* [JavaScript](http://www.runoob.com/js/js-tutorial.html)\n\t* [JSON教程](http://www.runoob.com/json/json-tutorial.html)\n\n## 其他实用网站🕸\n*\t[官方社区](https://developers.weixin.qq.com/):有的坑可能别人已经踩过，可以搜索一下\n* [知晓程序](https://minapp.com/miniapp/)：可以看看别人产品的想法和思路\n* [WePY](https://tencent.github.io/wepy/)：开发小程序的一个框架。建议看看就好，还是以原生开发为主。等原生上手后再看看框架。\n* [工大思政课](https://github.com/Shincey/HFUTSZK):无网络交互，可以独立开发完成。\n* [AI小部落](https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram)：js逻辑中有网络交互，可以做参考。\n* 各种css样式配置web工具\n\t* [box、卡片的阴影效果](https://www.html.cn/tool/css3Preview/Box-Shadow.html)\n\t* ......\n\n## 建议🍻\n* 先学习基础**HTML/CSS/JavaScript**，不用特别抓细节，但是要知晓常见用法\n* 理清三个方面：\n\t* 如何布局、添加样式\n\t* 逻辑层与 ui 层如何交互，或者说如何用代码操控 ui 组件\n\t* 如何处理用户的交互动作\n* 先从简单的功能开始开发，如，点击按钮显示图片文本。再逐渐复杂逻辑\n* 小程序推荐用原生开发，先不要用框架\n* 看官方文档，也要结合搜索引擎。因为微信的官方文档，有坑\n* 每天新增一行代码都是进步😃加油！\n* 逐渐理解[生命周期](https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/page.html#%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F)\n","slug":"微信小程序学习意见","published":1,"updated":"2019-06-04T01:06:18.988Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwfy001k48upjawxi9d7","content":"<h2 id=\"布局样式📱\"><a href=\"#布局样式📱\" class=\"headerlink\" title=\"布局样式📱\"></a>布局样式📱</h2><ul>\n<li><a href=\"https://www.imooc.com/learn/974\" target=\"_blank\" rel=\"noopener\">入门教程视频</a></li>\n<li><a href=\"http://www.imooc.com/code/49\" target=\"_blank\" rel=\"noopener\">基础代码教程</a></li>\n<li>相关实用网站<ul>\n<li><a href=\"http://www.runoob.com/css/css-tutorial.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#wxml-%E6%A8%A1%E6%9D%BF\" target=\"_blank\" rel=\"noopener\">WXML与WXSS</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/framework/view/wxss.html\" target=\"_blank\" rel=\"noopener\">WXSS主要注意尺寸单位</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/component/\" target=\"_blank\" rel=\"noopener\">小程序的基本组件</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"逻辑🗯\"><a href=\"#逻辑🗯\" class=\"headerlink\" title=\"逻辑🗯\"></a>逻辑🗯</h2><ul>\n<li><a href=\"https://developers.weixin.qq.com/community/develop/article/doc/000aa057ca0a88dcd938b4d6656813\" target=\"_blank\" rel=\"noopener\">PWA</a>(了解）</li>\n<li><a href=\"https://www.v2ex.com/t/427255\" target=\"_blank\" rel=\"noopener\">关于HTML5/小程序的产生</a>(了解）</li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#js-%E4%BA%A4%E4%BA%92%E9%80%BB%E8%BE%91\" target=\"_blank\" rel=\"noopener\">JS逻辑交互</a></li>\n<li>相关实用网站<ul>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/framework.html#api\" target=\"_blank\" rel=\"noopener\">小程序的API</a></li>\n<li><a href=\"http://www.runoob.com/js/js-tutorial.html\" target=\"_blank\" rel=\"noopener\">JavaScript</a></li>\n<li><a href=\"http://www.runoob.com/json/json-tutorial.html\" target=\"_blank\" rel=\"noopener\">JSON教程</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"其他实用网站🕸\"><a href=\"#其他实用网站🕸\" class=\"headerlink\" title=\"其他实用网站🕸\"></a>其他实用网站🕸</h2><ul>\n<li><a href=\"https://developers.weixin.qq.com/\" target=\"_blank\" rel=\"noopener\">官方社区</a>:有的坑可能别人已经踩过，可以搜索一下</li>\n<li><a href=\"https://minapp.com/miniapp/\" target=\"_blank\" rel=\"noopener\">知晓程序</a>：可以看看别人产品的想法和思路</li>\n<li><a href=\"https://tencent.github.io/wepy/\" target=\"_blank\" rel=\"noopener\">WePY</a>：开发小程序的一个框架。建议看看就好，还是以原生开发为主。等原生上手后再看看框架。</li>\n<li><a href=\"https://github.com/Shincey/HFUTSZK\" target=\"_blank\" rel=\"noopener\">工大思政课</a>:无网络交互，可以独立开发完成。</li>\n<li><a href=\"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram\" target=\"_blank\" rel=\"noopener\">AI小部落</a>：js逻辑中有网络交互，可以做参考。</li>\n<li>各种css样式配置web工具<ul>\n<li><a href=\"https://www.html.cn/tool/css3Preview/Box-Shadow.html\" target=\"_blank\" rel=\"noopener\">box、卡片的阴影效果</a></li>\n<li>……</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"建议🍻\"><a href=\"#建议🍻\" class=\"headerlink\" title=\"建议🍻\"></a>建议🍻</h2><ul>\n<li>先学习基础<strong>HTML/CSS/JavaScript</strong>，不用特别抓细节，但是要知晓常见用法</li>\n<li>理清三个方面：<ul>\n<li>如何布局、添加样式</li>\n<li>逻辑层与 ui 层如何交互，或者说如何用代码操控 ui 组件</li>\n<li>如何处理用户的交互动作</li>\n</ul>\n</li>\n<li>先从简单的功能开始开发，如，点击按钮显示图片文本。再逐渐复杂逻辑</li>\n<li>小程序推荐用原生开发，先不要用框架</li>\n<li>看官方文档，也要结合搜索引擎。因为微信的官方文档，有坑</li>\n<li>每天新增一行代码都是进步😃加油！</li>\n<li>逐渐理解<a href=\"https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/page.html#%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F\" target=\"_blank\" rel=\"noopener\">生命周期</a></li>\n</ul>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"布局样式📱\"><a href=\"#布局样式📱\" class=\"headerlink\" title=\"布局样式📱\"></a>布局样式📱</h2><ul>\n<li><a href=\"https://www.imooc.com/learn/974\" target=\"_blank\" rel=\"noopener\">入门教程视频</a></li>\n<li><a href=\"http://www.imooc.com/code/49\" target=\"_blank\" rel=\"noopener\">基础代码教程</a></li>\n<li>相关实用网站<ul>\n<li><a href=\"http://www.runoob.com/css/css-tutorial.html\" target=\"_blank\" rel=\"noopener\">菜鸟教程</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#wxml-%E6%A8%A1%E6%9D%BF\" target=\"_blank\" rel=\"noopener\">WXML与WXSS</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/framework/view/wxss.html\" target=\"_blank\" rel=\"noopener\">WXSS主要注意尺寸单位</a></li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/component/\" target=\"_blank\" rel=\"noopener\">小程序的基本组件</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"逻辑🗯\"><a href=\"#逻辑🗯\" class=\"headerlink\" title=\"逻辑🗯\"></a>逻辑🗯</h2><ul>\n<li><a href=\"https://developers.weixin.qq.com/community/develop/article/doc/000aa057ca0a88dcd938b4d6656813\" target=\"_blank\" rel=\"noopener\">PWA</a>(了解）</li>\n<li><a href=\"https://www.v2ex.com/t/427255\" target=\"_blank\" rel=\"noopener\">关于HTML5/小程序的产生</a>(了解）</li>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/file.html#js-%E4%BA%A4%E4%BA%92%E9%80%BB%E8%BE%91\" target=\"_blank\" rel=\"noopener\">JS逻辑交互</a></li>\n<li>相关实用网站<ul>\n<li><a href=\"https://developers.weixin.qq.com/miniprogram/dev/quickstart/basic/framework.html#api\" target=\"_blank\" rel=\"noopener\">小程序的API</a></li>\n<li><a href=\"http://www.runoob.com/js/js-tutorial.html\" target=\"_blank\" rel=\"noopener\">JavaScript</a></li>\n<li><a href=\"http://www.runoob.com/json/json-tutorial.html\" target=\"_blank\" rel=\"noopener\">JSON教程</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"其他实用网站🕸\"><a href=\"#其他实用网站🕸\" class=\"headerlink\" title=\"其他实用网站🕸\"></a>其他实用网站🕸</h2><ul>\n<li><a href=\"https://developers.weixin.qq.com/\" target=\"_blank\" rel=\"noopener\">官方社区</a>:有的坑可能别人已经踩过，可以搜索一下</li>\n<li><a href=\"https://minapp.com/miniapp/\" target=\"_blank\" rel=\"noopener\">知晓程序</a>：可以看看别人产品的想法和思路</li>\n<li><a href=\"https://tencent.github.io/wepy/\" target=\"_blank\" rel=\"noopener\">WePY</a>：开发小程序的一个框架。建议看看就好，还是以原生开发为主。等原生上手后再看看框架。</li>\n<li><a href=\"https://github.com/Shincey/HFUTSZK\" target=\"_blank\" rel=\"noopener\">工大思政课</a>:无网络交互，可以独立开发完成。</li>\n<li><a href=\"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram\" target=\"_blank\" rel=\"noopener\">AI小部落</a>：js逻辑中有网络交互，可以做参考。</li>\n<li>各种css样式配置web工具<ul>\n<li><a href=\"https://www.html.cn/tool/css3Preview/Box-Shadow.html\" target=\"_blank\" rel=\"noopener\">box、卡片的阴影效果</a></li>\n<li>……</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"建议🍻\"><a href=\"#建议🍻\" class=\"headerlink\" title=\"建议🍻\"></a>建议🍻</h2><ul>\n<li>先学习基础<strong>HTML/CSS/JavaScript</strong>，不用特别抓细节，但是要知晓常见用法</li>\n<li>理清三个方面：<ul>\n<li>如何布局、添加样式</li>\n<li>逻辑层与 ui 层如何交互，或者说如何用代码操控 ui 组件</li>\n<li>如何处理用户的交互动作</li>\n</ul>\n</li>\n<li>先从简单的功能开始开发，如，点击按钮显示图片文本。再逐渐复杂逻辑</li>\n<li>小程序推荐用原生开发，先不要用框架</li>\n<li>看官方文档，也要结合搜索引擎。因为微信的官方文档，有坑</li>\n<li>每天新增一行代码都是进步😃加油！</li>\n<li>逐渐理解<a href=\"https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/page.html#%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F\" target=\"_blank\" rel=\"noopener\">生命周期</a></li>\n</ul>\n"},{"title":"每周记录📝（190316）","date":"2019-03-15T16:00:00.000Z","_content":"![学校的傍晚](https://wx1.sinaimg.cn/mw1024/6a49516fly1g14vxty5rbj21400u0e82.jpg)\n\n这一周将毕业设计的开头部分完成了链路打通，具体就是在前端埋点得到的数据能够在kafka中分topic消费。\n\n其实这之间走了很多弯路，自己反复查资料才发现一个正确的方法。现在才意识到做项目需要一个指导老师的重要性，同样企业需要技术的布道师也是这个道理。\n\n我的毕设是使用spark streaming进行网站流量的分析。其实对于毕设很尴尬的一点事，没有真实的业务数据，只能自己凭空制造。另外我觉得高校中的毕业设计，真的越来越水，就像是一个大一点的课程设计，自己独立完成的那种。\n\n这一周我首先想完成通过自己在前端埋点，收集到数据，并能够给后面的业务流程使用。我的前端使用react，埋点的数据收集打算用flume收集。最开始的思路是使用flume的HTTPSource直接收集来自前端POST请求中的数据，可是通过查阅文档发现，无法完成。因为HTTPSource接受的需要是flume的Event对象，前端直接发来的数据不符合要求。所以说前端的任务只要完成原始数据收集的工作就可以了，其他必要的转换由后端完成。\n\n还有一点就是，经别人提醒，如果说用flume直接收集网络上的数据还有安全的问题（见[V2ex帖子](https://www.v2ex.com/t/543592#reply4)）。所以有后端转发的必要。所以在这里我就有个疑惑，关于HTTPSource的使用场景到底是什么？\n\n进过反复的尝试，确定了方案是：前端埋点采集到的数据由后端接受，并发送log4j日志给flume，再由flume的interceptor进行过滤（分流），分成不同的topic给kafka的消费者使用，之后就可以给spark streaming使用啦。(解决方案[见文章](https://fangmiao97.github.io/2019/03/16/FluemToKafkaBaseOnDifferntTopic/))\n\n不过现在遗留下的问题就是，前端使用axios出现跨域访问的问题。这个问题下周开始解决。下周要完成的任务就是把前端的框架搭好，开始学习spark streaming深入一点的东西，然后找需求。目前的需求太单薄了，不怎么丰富。\n\n本周还做了点其他的事，总之效率没有那么高吧。这两天刷LeetCode也刷不起来，不知道为啥。所以这两天休息了一下，没刷。\n\n感觉自己有的时候总被一些东西打扰，自己的控制力不够。另外看书的速度也挺慢的。看了名人传记，感觉厉害的人为何有那么多精力呢？\n\n最近看到好玩的图，取名我的毕设，哈哈哈🤣\n![我的毕设](https://wx2.sinaimg.cn/mw1024/6a49516fly1g14w7kwjt8j20j60n4dks.jpg)\n\n","source":"_posts/weeklyReport190316.md","raw":"---\ntitle: 每周记录📝（190316）\ndate: 2019/3/16\ncategories:\n    - 每周记录\n---\n![学校的傍晚](https://wx1.sinaimg.cn/mw1024/6a49516fly1g14vxty5rbj21400u0e82.jpg)\n\n这一周将毕业设计的开头部分完成了链路打通，具体就是在前端埋点得到的数据能够在kafka中分topic消费。\n\n其实这之间走了很多弯路，自己反复查资料才发现一个正确的方法。现在才意识到做项目需要一个指导老师的重要性，同样企业需要技术的布道师也是这个道理。\n\n我的毕设是使用spark streaming进行网站流量的分析。其实对于毕设很尴尬的一点事，没有真实的业务数据，只能自己凭空制造。另外我觉得高校中的毕业设计，真的越来越水，就像是一个大一点的课程设计，自己独立完成的那种。\n\n这一周我首先想完成通过自己在前端埋点，收集到数据，并能够给后面的业务流程使用。我的前端使用react，埋点的数据收集打算用flume收集。最开始的思路是使用flume的HTTPSource直接收集来自前端POST请求中的数据，可是通过查阅文档发现，无法完成。因为HTTPSource接受的需要是flume的Event对象，前端直接发来的数据不符合要求。所以说前端的任务只要完成原始数据收集的工作就可以了，其他必要的转换由后端完成。\n\n还有一点就是，经别人提醒，如果说用flume直接收集网络上的数据还有安全的问题（见[V2ex帖子](https://www.v2ex.com/t/543592#reply4)）。所以有后端转发的必要。所以在这里我就有个疑惑，关于HTTPSource的使用场景到底是什么？\n\n进过反复的尝试，确定了方案是：前端埋点采集到的数据由后端接受，并发送log4j日志给flume，再由flume的interceptor进行过滤（分流），分成不同的topic给kafka的消费者使用，之后就可以给spark streaming使用啦。(解决方案[见文章](https://fangmiao97.github.io/2019/03/16/FluemToKafkaBaseOnDifferntTopic/))\n\n不过现在遗留下的问题就是，前端使用axios出现跨域访问的问题。这个问题下周开始解决。下周要完成的任务就是把前端的框架搭好，开始学习spark streaming深入一点的东西，然后找需求。目前的需求太单薄了，不怎么丰富。\n\n本周还做了点其他的事，总之效率没有那么高吧。这两天刷LeetCode也刷不起来，不知道为啥。所以这两天休息了一下，没刷。\n\n感觉自己有的时候总被一些东西打扰，自己的控制力不够。另外看书的速度也挺慢的。看了名人传记，感觉厉害的人为何有那么多精力呢？\n\n最近看到好玩的图，取名我的毕设，哈哈哈🤣\n![我的毕设](https://wx2.sinaimg.cn/mw1024/6a49516fly1g14w7kwjt8j20j60n4dks.jpg)\n\n","slug":"weeklyReport190316","published":1,"updated":"2019-06-04T01:06:18.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwg2001n48up6zwpnkvl","content":"<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g14vxty5rbj21400u0e82.jpg\" alt=\"学校的傍晚\"></p>\n<p>这一周将毕业设计的开头部分完成了链路打通，具体就是在前端埋点得到的数据能够在kafka中分topic消费。</p>\n<p>其实这之间走了很多弯路，自己反复查资料才发现一个正确的方法。现在才意识到做项目需要一个指导老师的重要性，同样企业需要技术的布道师也是这个道理。</p>\n<p>我的毕设是使用spark streaming进行网站流量的分析。其实对于毕设很尴尬的一点事，没有真实的业务数据，只能自己凭空制造。另外我觉得高校中的毕业设计，真的越来越水，就像是一个大一点的课程设计，自己独立完成的那种。</p>\n<p>这一周我首先想完成通过自己在前端埋点，收集到数据，并能够给后面的业务流程使用。我的前端使用react，埋点的数据收集打算用flume收集。最开始的思路是使用flume的HTTPSource直接收集来自前端POST请求中的数据，可是通过查阅文档发现，无法完成。因为HTTPSource接受的需要是flume的Event对象，前端直接发来的数据不符合要求。所以说前端的任务只要完成原始数据收集的工作就可以了，其他必要的转换由后端完成。</p>\n<p>还有一点就是，经别人提醒，如果说用flume直接收集网络上的数据还有安全的问题（见<a href=\"https://www.v2ex.com/t/543592#reply4\" target=\"_blank\" rel=\"noopener\">V2ex帖子</a>）。所以有后端转发的必要。所以在这里我就有个疑惑，关于HTTPSource的使用场景到底是什么？</p>\n<p>进过反复的尝试，确定了方案是：前端埋点采集到的数据由后端接受，并发送log4j日志给flume，再由flume的interceptor进行过滤（分流），分成不同的topic给kafka的消费者使用，之后就可以给spark streaming使用啦。(解决方案<a href=\"https://fangmiao97.github.io/2019/03/16/FluemToKafkaBaseOnDifferntTopic/\" target=\"_blank\" rel=\"noopener\">见文章</a>)</p>\n<p>不过现在遗留下的问题就是，前端使用axios出现跨域访问的问题。这个问题下周开始解决。下周要完成的任务就是把前端的框架搭好，开始学习spark streaming深入一点的东西，然后找需求。目前的需求太单薄了，不怎么丰富。</p>\n<p>本周还做了点其他的事，总之效率没有那么高吧。这两天刷LeetCode也刷不起来，不知道为啥。所以这两天休息了一下，没刷。</p>\n<p>感觉自己有的时候总被一些东西打扰，自己的控制力不够。另外看书的速度也挺慢的。看了名人传记，感觉厉害的人为何有那么多精力呢？</p>\n<p>最近看到好玩的图，取名我的毕设，哈哈哈🤣<br><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g14w7kwjt8j20j60n4dks.jpg\" alt=\"我的毕设\"></p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g14vxty5rbj21400u0e82.jpg\" alt=\"学校的傍晚\"></p>\n<p>这一周将毕业设计的开头部分完成了链路打通，具体就是在前端埋点得到的数据能够在kafka中分topic消费。</p>\n<p>其实这之间走了很多弯路，自己反复查资料才发现一个正确的方法。现在才意识到做项目需要一个指导老师的重要性，同样企业需要技术的布道师也是这个道理。</p>\n<p>我的毕设是使用spark streaming进行网站流量的分析。其实对于毕设很尴尬的一点事，没有真实的业务数据，只能自己凭空制造。另外我觉得高校中的毕业设计，真的越来越水，就像是一个大一点的课程设计，自己独立完成的那种。</p>\n<p>这一周我首先想完成通过自己在前端埋点，收集到数据，并能够给后面的业务流程使用。我的前端使用react，埋点的数据收集打算用flume收集。最开始的思路是使用flume的HTTPSource直接收集来自前端POST请求中的数据，可是通过查阅文档发现，无法完成。因为HTTPSource接受的需要是flume的Event对象，前端直接发来的数据不符合要求。所以说前端的任务只要完成原始数据收集的工作就可以了，其他必要的转换由后端完成。</p>\n<p>还有一点就是，经别人提醒，如果说用flume直接收集网络上的数据还有安全的问题（见<a href=\"https://www.v2ex.com/t/543592#reply4\" target=\"_blank\" rel=\"noopener\">V2ex帖子</a>）。所以有后端转发的必要。所以在这里我就有个疑惑，关于HTTPSource的使用场景到底是什么？</p>\n<p>进过反复的尝试，确定了方案是：前端埋点采集到的数据由后端接受，并发送log4j日志给flume，再由flume的interceptor进行过滤（分流），分成不同的topic给kafka的消费者使用，之后就可以给spark streaming使用啦。(解决方案<a href=\"https://fangmiao97.github.io/2019/03/16/FluemToKafkaBaseOnDifferntTopic/\" target=\"_blank\" rel=\"noopener\">见文章</a>)</p>\n<p>不过现在遗留下的问题就是，前端使用axios出现跨域访问的问题。这个问题下周开始解决。下周要完成的任务就是把前端的框架搭好，开始学习spark streaming深入一点的东西，然后找需求。目前的需求太单薄了，不怎么丰富。</p>\n<p>本周还做了点其他的事，总之效率没有那么高吧。这两天刷LeetCode也刷不起来，不知道为啥。所以这两天休息了一下，没刷。</p>\n<p>感觉自己有的时候总被一些东西打扰，自己的控制力不够。另外看书的速度也挺慢的。看了名人传记，感觉厉害的人为何有那么多精力呢？</p>\n<p>最近看到好玩的图，取名我的毕设，哈哈哈🤣<br><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g14w7kwjt8j20j60n4dks.jpg\" alt=\"我的毕设\"></p>\n"},{"title":"每周记录📝（190323）","date":"2019-03-22T16:00:00.000Z","_content":"![图书馆](https://wxt.sinaimg.cn/mw1024/6a49516fly1g1crb20idzj22c02c0b2a.jpg?tags=%5B%5D)\n\n雨后的图书馆。\n\n本周将毕业设计的前端后端，kafka与spark streaming进行了打通。前端页面也可展示HBase中的数据。\n\n关于ant-d Pro，我感觉肯定是有很多坑的。慢慢踩。\n\n对于毕设后续，下一周需要将模拟打点的页面加上，并把之前例子中的spark应用部署好，能够实现组件的动态渲染。\n\n本周首先解决了CORS问题，其实在后端加上标签即可。我的后端其实有一两点，首先是数据可视化的作用，另一个是模拟前端打点的应用。没有办法，只能这样用，模拟真实情况的服务。\n\n另外注意到Nginx，学习到其有两个作用：反向代理+负载均衡，以后用到在深入吧。\n\n对于HBase的scan操作，还需要深入学习一下。\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1cr9q61ixj21hc0u0n20.jpg)\n\n目前的可视化界面，没有logo，在今日数据概览中需要加一个显示当前时间的组件。这个Antd Pro的Pie组件真的很迷。\n\n历史数据查询需要增加日期选择控件。对于历史数据，我准备存在MySQL中，这里就要思考每日定时dump的问题。\n\n对于前端数据埋点的信息，先显示出来好了。\n\n刷LeetCode去了。","source":"_posts/weeklyReport190323.md","raw":"---\ntitle: 每周记录📝（190323）\ndate: 2019/3/23\ncategories:\n    - 每周记录\n---\n![图书馆](https://wxt.sinaimg.cn/mw1024/6a49516fly1g1crb20idzj22c02c0b2a.jpg?tags=%5B%5D)\n\n雨后的图书馆。\n\n本周将毕业设计的前端后端，kafka与spark streaming进行了打通。前端页面也可展示HBase中的数据。\n\n关于ant-d Pro，我感觉肯定是有很多坑的。慢慢踩。\n\n对于毕设后续，下一周需要将模拟打点的页面加上，并把之前例子中的spark应用部署好，能够实现组件的动态渲染。\n\n本周首先解决了CORS问题，其实在后端加上标签即可。我的后端其实有一两点，首先是数据可视化的作用，另一个是模拟前端打点的应用。没有办法，只能这样用，模拟真实情况的服务。\n\n另外注意到Nginx，学习到其有两个作用：反向代理+负载均衡，以后用到在深入吧。\n\n对于HBase的scan操作，还需要深入学习一下。\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1cr9q61ixj21hc0u0n20.jpg)\n\n目前的可视化界面，没有logo，在今日数据概览中需要加一个显示当前时间的组件。这个Antd Pro的Pie组件真的很迷。\n\n历史数据查询需要增加日期选择控件。对于历史数据，我准备存在MySQL中，这里就要思考每日定时dump的问题。\n\n对于前端数据埋点的信息，先显示出来好了。\n\n刷LeetCode去了。","slug":"weeklyReport190323","published":1,"updated":"2019-06-04T01:06:18.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwge001r48up7bo3mxlv","content":"<p><img src=\"https://wxt.sinaimg.cn/mw1024/6a49516fly1g1crb20idzj22c02c0b2a.jpg?tags=%5B%5D\" alt=\"图书馆\"></p>\n<p>雨后的图书馆。</p>\n<p>本周将毕业设计的前端后端，kafka与spark streaming进行了打通。前端页面也可展示HBase中的数据。</p>\n<p>关于ant-d Pro，我感觉肯定是有很多坑的。慢慢踩。</p>\n<p>对于毕设后续，下一周需要将模拟打点的页面加上，并把之前例子中的spark应用部署好，能够实现组件的动态渲染。</p>\n<p>本周首先解决了CORS问题，其实在后端加上标签即可。我的后端其实有一两点，首先是数据可视化的作用，另一个是模拟前端打点的应用。没有办法，只能这样用，模拟真实情况的服务。</p>\n<p>另外注意到Nginx，学习到其有两个作用：反向代理+负载均衡，以后用到在深入吧。</p>\n<p>对于HBase的scan操作，还需要深入学习一下。</p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1cr9q61ixj21hc0u0n20.jpg\" alt></p>\n<p>目前的可视化界面，没有logo，在今日数据概览中需要加一个显示当前时间的组件。这个Antd Pro的Pie组件真的很迷。</p>\n<p>历史数据查询需要增加日期选择控件。对于历史数据，我准备存在MySQL中，这里就要思考每日定时dump的问题。</p>\n<p>对于前端数据埋点的信息，先显示出来好了。</p>\n<p>刷LeetCode去了。</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p><img src=\"https://wxt.sinaimg.cn/mw1024/6a49516fly1g1crb20idzj22c02c0b2a.jpg?tags=%5B%5D\" alt=\"图书馆\"></p>\n<p>雨后的图书馆。</p>\n<p>本周将毕业设计的前端后端，kafka与spark streaming进行了打通。前端页面也可展示HBase中的数据。</p>\n<p>关于ant-d Pro，我感觉肯定是有很多坑的。慢慢踩。</p>\n<p>对于毕设后续，下一周需要将模拟打点的页面加上，并把之前例子中的spark应用部署好，能够实现组件的动态渲染。</p>\n<p>本周首先解决了CORS问题，其实在后端加上标签即可。我的后端其实有一两点，首先是数据可视化的作用，另一个是模拟前端打点的应用。没有办法，只能这样用，模拟真实情况的服务。</p>\n<p>另外注意到Nginx，学习到其有两个作用：反向代理+负载均衡，以后用到在深入吧。</p>\n<p>对于HBase的scan操作，还需要深入学习一下。</p>\n<p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1cr9q61ixj21hc0u0n20.jpg\" alt></p>\n<p>目前的可视化界面，没有logo，在今日数据概览中需要加一个显示当前时间的组件。这个Antd Pro的Pie组件真的很迷。</p>\n<p>历史数据查询需要增加日期选择控件。对于历史数据，我准备存在MySQL中，这里就要思考每日定时dump的问题。</p>\n<p>对于前端数据埋点的信息，先显示出来好了。</p>\n<p>刷LeetCode去了。</p>\n"},{"title":"胡适：写给大学毕业生的一封信","date":"2019-06-27T16:00:00.000Z","_content":"## context\n今天早上看了央视新闻关于毕业的[夜读文章](https://weibo.com/ttarticle/p/show?id=2309404387916615943710#_0)\n里面提到了胡适先生的《写给大学毕业生的一封信》，看了很有感触，所以摘抄至此，与未来的自己分享。\n\n本文是胡适先生1932年6月27日所作。虽然30年代那个血雨腥风的时代已经过去，现在的时代已经与当时不可同日而语，但是，读来还是感觉受益匪浅，胡适先生的谆谆教导之情溢于言表。本文中，胡适先生认为，大学生毕业有三条路可走：继续做学术研究；寻着相当的职业；做官，办党，革命。文中分析了大学毕业后遇到的“陷阱堕落的方式”，并给出了三个方子。\n\n这一两个星期里，各地的大学都有毕业的班次，都有很多的毕业生离开学校去开始他们的成人事业。 \n\n学生的生活是一种享有特殊优待的生活，不妨幼稚一点，不妨吵吵闹闹，社会都能纵容他们，不肯严格地要他们负行为的责任。现在他们要撑起自己的肩膀来挑他们自己的担子了。在这个国难最紧急的年头，他们的担子真不轻!我们祝他们的成功，同时也不忍不依据自己的经验，赠他们几句送行的赠言——虽未必是救命毫毛，也许做个防身的锦囊罢! \n\n你们毕业之后，可走的路不出这几条：绝少数的人还可以在国内或国外的研究院继续做学术研究；少数的人可以寻着相当的职业；此外还有做官，办党，革命三条路；再有就是在家享福或者失业亲居了。 \n\n走其余几条路的人，都不能没有堕落的危险。堕落的方式很多，总括起来，约有这两大类：\n\n第一是容易抛弃学生时代求知识的欲望。你们到了实际社会里，往往学非所用，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里即使向来抱有求知识学问的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的：书籍，实验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人能容易办到的。没有做学问的环境，又谁能怪我们抛弃学问呢? \n\n第二是容易抛弃学生时代理想的人生的追求。少年人初次和冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事了。渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就熔化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲抛兵而走，或者做了恶势的俘虏。你在那牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦!从此以后，你就甘心放弃理想人生的追求，甘心做现在社会的顺民了。要防御这两方面的堕落，一面要保持我们求知识的欲望，一面要保持我们对人生的追求。 \n\n有什么好方子呢?依我个人的观察和经验，有三种防身的药方是值得一试的。\n\n第一个方子只有一句话：“总得时时寻一两个值得研究的问题!”问题是知识学问的老祖宗：古往今来一切知识的产生与积聚，都是因为要解答问题——要解答实用上的困难和理论上的疑难。所谓“为知识而求知识”，其实也只是一种好奇心追求某种问题的解答，不过因为那种问题的性质不必是直接应用的，人们就觉得这是无所谓的求知识了。\n\n我们出学校之后，离开了做学问的环境，如果没有一两个值得解答的问题在脑子里盘旋，就很难保持求学问的热心。可是，如果你有了一个真有趣的问题逗你去想它，天天引诱你去解决它，天天对你挑衅你无可奈何它——这时候，你就会同恋爱一个女子发了疯一样，坐也坐不下，睡也睡不安，没工夫也得偷出工夫去陪她，没钱也得缩衣节食去巴结她。没有书，你自会变卖家私去买书；没有仪器，你自会典押衣物去置办仪器；没有师友，你自会不远千里去寻师访友。你只要有疑难问题来逼你时时用脑子，你自然会保持发展你对学问的兴趣，即使在最贫乏的知识中，你也会慢慢地，聚起一个小图书馆来，或者设置起一所小试验室来。所以我说，第一要寻问题。脑子里没有问题之日，就是你知识生活寿终正寝之时!古人说，“待文王而兴者，凡民也。若夫豪杰之士，虽无文王犹兴。”试想伽利略和牛顿有多少藏书?有多少仪器?他们不过是有问题而已。有了问题而后他们自会造出仪器来解决他们的问题。没有问题的人们，关在图书馆里也不会用书，锁在试验室里也不会有什么发现。\n\n第二个方子也只有一句话：“总得多发展一点非职业的兴趣。”离开学校之后，大家总是寻个吃饭的职业。可是你寻得的职业未必就是你所学的，未必是你所心喜的，或者是你所学的而和你性情不相近的。在这种情况之下，工作往往成了苦工，就感觉不到兴趣了。为糊口而做那种非“性之所近而力之所能勉”的工作，就很难保持求知的兴趣的生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。\n\n一个人应该有他的职业，也应该有他非职业的玩艺儿，可以叫作业余活动。往往他的业余活动比他的职业还更重要，因为一个人成就怎样，往往靠他怎样利用他的闲暇时间。他用他的闲暇来打麻将，他就成了个赌徒；你用你的闲暇来做社会服务，你也许成个社会改革者；或者你用你的闲暇去研究历史，你也许成个史学家。你的闲暇往往定你的终身。英国19世纪的两个哲人，弥儿终身做东印度公司的秘书，然而他的业余工作使他在哲学上、经济学上、政治思想史上都占一个很高的位置；斯宾塞是一个测量工程师，然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人，几乎没有一个不善用他的闲暇时间的。职业不容易适合我们的性情，我们要想生活不苦痛不堕落，只有多方发展。\n\n有了这种心爱的玩艺儿，你就做六个钟头抹桌子工作也不会感觉烦闷了。因为你知道，抹了六个钟头的桌子之后，你可以回家做你的化学研究，或画完你的大幅山水，或写你的小说戏曲，或继续你的历史考据，或做你的社会改革事业。你有了这种称心如意的活动，生活就不枯寂了，精神也就不会烦闷了。\n\n第三个方子也只有一句话：“你得有一点信心。”我们生当这个不幸的时代，眼中所见，耳中所闻，无非是叫我们悲观失望的。特别是在这个年头毕业的你们，眼见自己的国家民族沉沦到这步田地，眼看世界只是强权的世界，望极天边好像看不见一线的光明——在这个年头不发狂自杀，已算是万幸了，怎么还能够保持一点内心的镇定和理想的信任呢?我要对你们说：这时候正是我们要培养我们的信心的时候!只要我们有信心，我们还有救。 \n\n古人说：“信心可以移山。”又说：“只要功夫深，生铁磨成绣花针。”你不信吗?当拿破仑的军队征服普鲁士，占据柏林的时候，有一位教授叫作费希特的，天天在讲堂劝他的国人要有信心，要信仰他们的民族是有世界的特殊使命的，是必定要复兴的。费希特死的时候，谁也不能预料德意志统一帝国何时可以实现，然而不满５０年，新的统一的德意志帝国居然实现了。\n\n一个国家的强弱盛衰，都不是偶然的，都不能逃出因果的铁律的。我们今日所受的苦痛和耻辱，都只是过去种种恶因种下的恶果。我们要收获将来的善果，必须努力种现在新因。一粒一粒地种，必有满仓满屋的收，这是我们今日应有的信心。我们要深信：今日的失败，都由于过去的不努力。我们要深信：今日的努力，必定有将来的大收成。\n\n佛典里有一句话：“福不唐捐。”唐捐就是白白地丢了。我们也应该说：“功不唐捐!”没有一点努力是会白白地丢了的。在我们看不见想不到的时候，在我们看不见的方向，你瞧!你下的种子早已生根发叶开花结果了!你不信吗?法国被普鲁士打败之后，割了两省地，赔了５０万万法郎的赔款。这时候有一位刻苦的科学家巴斯德终日埋头在他的化学试验室里做他的化学试验和微菌学研究。他是一个最爱国的人，然而他深信只有科学可以救国。他用一生的精力证明了三个科学问题：(1)每一种发酵作用都是由于一种微菌的发展；(2)每一种传染病都是一种微菌在生物体内的发展；(3)传染病的微菌，在特殊的培养之下可以减轻毒力，使他们从病菌变成防病的药苗。\n\n这三个问题在表面上似乎都和救国大事业没有多大关系。然而从第一个问题的证明，巴斯德定出做醋酿酒的新法，使全国的酒醋业每年减除极大的损失。从第二个问题的证明，巴斯德教全国的蚕丝业怎样选种防病，教全国的畜牧农家怎样防止牛羊瘟疫，又教全世界怎样注重消毒以减少外科手术的死亡率。从第三个问题的证明，巴斯德发明了牲畜的脾热瘟的疗治药苗，每年替法国农家减除了２０００万法郎的大损失；又发明了疯狗咬毒的治疗法，救济了无数的生命。所以英国的科学家赫胥黎在皇家学会里称颂巴斯德的功绩道：“法国给了德国５０万万法郎的赔款，巴斯德先生一个人研究科学的成就足够还清这一笔赔款了。”巴斯德对于科学有绝大的信心，所以他在国家蒙奇辱大难的时候，终不肯抛弃他的显微镜与试验室。他绝不想他在显微镜底下能偿还５０万万法郎的赔款，然而在他看不见想不到的时候，他已收获了科学救国的奇迹。\n\n朋友们，在你最悲观失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。 \n\n（摘自《胡适文存》第4集第4卷《胡适教育论著选》，人民教育出版社)\n\n","source":"_posts/graduation-letter-from-Hushi.md","raw":"---\ntitle: 胡适：写给大学毕业生的一封信\ndate: 2019/06/28\ncategories: \n    - 毕业\n---\n## context\n今天早上看了央视新闻关于毕业的[夜读文章](https://weibo.com/ttarticle/p/show?id=2309404387916615943710#_0)\n里面提到了胡适先生的《写给大学毕业生的一封信》，看了很有感触，所以摘抄至此，与未来的自己分享。\n\n本文是胡适先生1932年6月27日所作。虽然30年代那个血雨腥风的时代已经过去，现在的时代已经与当时不可同日而语，但是，读来还是感觉受益匪浅，胡适先生的谆谆教导之情溢于言表。本文中，胡适先生认为，大学生毕业有三条路可走：继续做学术研究；寻着相当的职业；做官，办党，革命。文中分析了大学毕业后遇到的“陷阱堕落的方式”，并给出了三个方子。\n\n这一两个星期里，各地的大学都有毕业的班次，都有很多的毕业生离开学校去开始他们的成人事业。 \n\n学生的生活是一种享有特殊优待的生活，不妨幼稚一点，不妨吵吵闹闹，社会都能纵容他们，不肯严格地要他们负行为的责任。现在他们要撑起自己的肩膀来挑他们自己的担子了。在这个国难最紧急的年头，他们的担子真不轻!我们祝他们的成功，同时也不忍不依据自己的经验，赠他们几句送行的赠言——虽未必是救命毫毛，也许做个防身的锦囊罢! \n\n你们毕业之后，可走的路不出这几条：绝少数的人还可以在国内或国外的研究院继续做学术研究；少数的人可以寻着相当的职业；此外还有做官，办党，革命三条路；再有就是在家享福或者失业亲居了。 \n\n走其余几条路的人，都不能没有堕落的危险。堕落的方式很多，总括起来，约有这两大类：\n\n第一是容易抛弃学生时代求知识的欲望。你们到了实际社会里，往往学非所用，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里即使向来抱有求知识学问的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的：书籍，实验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人能容易办到的。没有做学问的环境，又谁能怪我们抛弃学问呢? \n\n第二是容易抛弃学生时代理想的人生的追求。少年人初次和冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事了。渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就熔化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲抛兵而走，或者做了恶势的俘虏。你在那牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦!从此以后，你就甘心放弃理想人生的追求，甘心做现在社会的顺民了。要防御这两方面的堕落，一面要保持我们求知识的欲望，一面要保持我们对人生的追求。 \n\n有什么好方子呢?依我个人的观察和经验，有三种防身的药方是值得一试的。\n\n第一个方子只有一句话：“总得时时寻一两个值得研究的问题!”问题是知识学问的老祖宗：古往今来一切知识的产生与积聚，都是因为要解答问题——要解答实用上的困难和理论上的疑难。所谓“为知识而求知识”，其实也只是一种好奇心追求某种问题的解答，不过因为那种问题的性质不必是直接应用的，人们就觉得这是无所谓的求知识了。\n\n我们出学校之后，离开了做学问的环境，如果没有一两个值得解答的问题在脑子里盘旋，就很难保持求学问的热心。可是，如果你有了一个真有趣的问题逗你去想它，天天引诱你去解决它，天天对你挑衅你无可奈何它——这时候，你就会同恋爱一个女子发了疯一样，坐也坐不下，睡也睡不安，没工夫也得偷出工夫去陪她，没钱也得缩衣节食去巴结她。没有书，你自会变卖家私去买书；没有仪器，你自会典押衣物去置办仪器；没有师友，你自会不远千里去寻师访友。你只要有疑难问题来逼你时时用脑子，你自然会保持发展你对学问的兴趣，即使在最贫乏的知识中，你也会慢慢地，聚起一个小图书馆来，或者设置起一所小试验室来。所以我说，第一要寻问题。脑子里没有问题之日，就是你知识生活寿终正寝之时!古人说，“待文王而兴者，凡民也。若夫豪杰之士，虽无文王犹兴。”试想伽利略和牛顿有多少藏书?有多少仪器?他们不过是有问题而已。有了问题而后他们自会造出仪器来解决他们的问题。没有问题的人们，关在图书馆里也不会用书，锁在试验室里也不会有什么发现。\n\n第二个方子也只有一句话：“总得多发展一点非职业的兴趣。”离开学校之后，大家总是寻个吃饭的职业。可是你寻得的职业未必就是你所学的，未必是你所心喜的，或者是你所学的而和你性情不相近的。在这种情况之下，工作往往成了苦工，就感觉不到兴趣了。为糊口而做那种非“性之所近而力之所能勉”的工作，就很难保持求知的兴趣的生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。\n\n一个人应该有他的职业，也应该有他非职业的玩艺儿，可以叫作业余活动。往往他的业余活动比他的职业还更重要，因为一个人成就怎样，往往靠他怎样利用他的闲暇时间。他用他的闲暇来打麻将，他就成了个赌徒；你用你的闲暇来做社会服务，你也许成个社会改革者；或者你用你的闲暇去研究历史，你也许成个史学家。你的闲暇往往定你的终身。英国19世纪的两个哲人，弥儿终身做东印度公司的秘书，然而他的业余工作使他在哲学上、经济学上、政治思想史上都占一个很高的位置；斯宾塞是一个测量工程师，然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人，几乎没有一个不善用他的闲暇时间的。职业不容易适合我们的性情，我们要想生活不苦痛不堕落，只有多方发展。\n\n有了这种心爱的玩艺儿，你就做六个钟头抹桌子工作也不会感觉烦闷了。因为你知道，抹了六个钟头的桌子之后，你可以回家做你的化学研究，或画完你的大幅山水，或写你的小说戏曲，或继续你的历史考据，或做你的社会改革事业。你有了这种称心如意的活动，生活就不枯寂了，精神也就不会烦闷了。\n\n第三个方子也只有一句话：“你得有一点信心。”我们生当这个不幸的时代，眼中所见，耳中所闻，无非是叫我们悲观失望的。特别是在这个年头毕业的你们，眼见自己的国家民族沉沦到这步田地，眼看世界只是强权的世界，望极天边好像看不见一线的光明——在这个年头不发狂自杀，已算是万幸了，怎么还能够保持一点内心的镇定和理想的信任呢?我要对你们说：这时候正是我们要培养我们的信心的时候!只要我们有信心，我们还有救。 \n\n古人说：“信心可以移山。”又说：“只要功夫深，生铁磨成绣花针。”你不信吗?当拿破仑的军队征服普鲁士，占据柏林的时候，有一位教授叫作费希特的，天天在讲堂劝他的国人要有信心，要信仰他们的民族是有世界的特殊使命的，是必定要复兴的。费希特死的时候，谁也不能预料德意志统一帝国何时可以实现，然而不满５０年，新的统一的德意志帝国居然实现了。\n\n一个国家的强弱盛衰，都不是偶然的，都不能逃出因果的铁律的。我们今日所受的苦痛和耻辱，都只是过去种种恶因种下的恶果。我们要收获将来的善果，必须努力种现在新因。一粒一粒地种，必有满仓满屋的收，这是我们今日应有的信心。我们要深信：今日的失败，都由于过去的不努力。我们要深信：今日的努力，必定有将来的大收成。\n\n佛典里有一句话：“福不唐捐。”唐捐就是白白地丢了。我们也应该说：“功不唐捐!”没有一点努力是会白白地丢了的。在我们看不见想不到的时候，在我们看不见的方向，你瞧!你下的种子早已生根发叶开花结果了!你不信吗?法国被普鲁士打败之后，割了两省地，赔了５０万万法郎的赔款。这时候有一位刻苦的科学家巴斯德终日埋头在他的化学试验室里做他的化学试验和微菌学研究。他是一个最爱国的人，然而他深信只有科学可以救国。他用一生的精力证明了三个科学问题：(1)每一种发酵作用都是由于一种微菌的发展；(2)每一种传染病都是一种微菌在生物体内的发展；(3)传染病的微菌，在特殊的培养之下可以减轻毒力，使他们从病菌变成防病的药苗。\n\n这三个问题在表面上似乎都和救国大事业没有多大关系。然而从第一个问题的证明，巴斯德定出做醋酿酒的新法，使全国的酒醋业每年减除极大的损失。从第二个问题的证明，巴斯德教全国的蚕丝业怎样选种防病，教全国的畜牧农家怎样防止牛羊瘟疫，又教全世界怎样注重消毒以减少外科手术的死亡率。从第三个问题的证明，巴斯德发明了牲畜的脾热瘟的疗治药苗，每年替法国农家减除了２０００万法郎的大损失；又发明了疯狗咬毒的治疗法，救济了无数的生命。所以英国的科学家赫胥黎在皇家学会里称颂巴斯德的功绩道：“法国给了德国５０万万法郎的赔款，巴斯德先生一个人研究科学的成就足够还清这一笔赔款了。”巴斯德对于科学有绝大的信心，所以他在国家蒙奇辱大难的时候，终不肯抛弃他的显微镜与试验室。他绝不想他在显微镜底下能偿还５０万万法郎的赔款，然而在他看不见想不到的时候，他已收获了科学救国的奇迹。\n\n朋友们，在你最悲观失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。 \n\n（摘自《胡适文存》第4集第4卷《胡适教育论著选》，人民教育出版社)\n\n","slug":"graduation-letter-from-Hushi","published":1,"updated":"2019-06-28T02:30:24.930Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwi5001z48upx46yqewc","content":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p>今天早上看了央视新闻关于毕业的<a href=\"https://weibo.com/ttarticle/p/show?id=2309404387916615943710#_0\" target=\"_blank\" rel=\"noopener\">夜读文章</a><br>里面提到了胡适先生的《写给大学毕业生的一封信》，看了很有感触，所以摘抄至此，与未来的自己分享。</p>\n<p>本文是胡适先生1932年6月27日所作。虽然30年代那个血雨腥风的时代已经过去，现在的时代已经与当时不可同日而语，但是，读来还是感觉受益匪浅，胡适先生的谆谆教导之情溢于言表。本文中，胡适先生认为，大学生毕业有三条路可走：继续做学术研究；寻着相当的职业；做官，办党，革命。文中分析了大学毕业后遇到的“陷阱堕落的方式”，并给出了三个方子。</p>\n<p>这一两个星期里，各地的大学都有毕业的班次，都有很多的毕业生离开学校去开始他们的成人事业。 </p>\n<p>学生的生活是一种享有特殊优待的生活，不妨幼稚一点，不妨吵吵闹闹，社会都能纵容他们，不肯严格地要他们负行为的责任。现在他们要撑起自己的肩膀来挑他们自己的担子了。在这个国难最紧急的年头，他们的担子真不轻!我们祝他们的成功，同时也不忍不依据自己的经验，赠他们几句送行的赠言——虽未必是救命毫毛，也许做个防身的锦囊罢! </p>\n<p>你们毕业之后，可走的路不出这几条：绝少数的人还可以在国内或国外的研究院继续做学术研究；少数的人可以寻着相当的职业；此外还有做官，办党，革命三条路；再有就是在家享福或者失业亲居了。 </p>\n<p>走其余几条路的人，都不能没有堕落的危险。堕落的方式很多，总括起来，约有这两大类：</p>\n<p>第一是容易抛弃学生时代求知识的欲望。你们到了实际社会里，往往学非所用，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里即使向来抱有求知识学问的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的：书籍，实验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人能容易办到的。没有做学问的环境，又谁能怪我们抛弃学问呢? </p>\n<p>第二是容易抛弃学生时代理想的人生的追求。少年人初次和冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事了。渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就熔化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲抛兵而走，或者做了恶势的俘虏。你在那牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦!从此以后，你就甘心放弃理想人生的追求，甘心做现在社会的顺民了。要防御这两方面的堕落，一面要保持我们求知识的欲望，一面要保持我们对人生的追求。 </p>\n<p>有什么好方子呢?依我个人的观察和经验，有三种防身的药方是值得一试的。</p>\n<p>第一个方子只有一句话：“总得时时寻一两个值得研究的问题!”问题是知识学问的老祖宗：古往今来一切知识的产生与积聚，都是因为要解答问题——要解答实用上的困难和理论上的疑难。所谓“为知识而求知识”，其实也只是一种好奇心追求某种问题的解答，不过因为那种问题的性质不必是直接应用的，人们就觉得这是无所谓的求知识了。</p>\n<p>我们出学校之后，离开了做学问的环境，如果没有一两个值得解答的问题在脑子里盘旋，就很难保持求学问的热心。可是，如果你有了一个真有趣的问题逗你去想它，天天引诱你去解决它，天天对你挑衅你无可奈何它——这时候，你就会同恋爱一个女子发了疯一样，坐也坐不下，睡也睡不安，没工夫也得偷出工夫去陪她，没钱也得缩衣节食去巴结她。没有书，你自会变卖家私去买书；没有仪器，你自会典押衣物去置办仪器；没有师友，你自会不远千里去寻师访友。你只要有疑难问题来逼你时时用脑子，你自然会保持发展你对学问的兴趣，即使在最贫乏的知识中，你也会慢慢地，聚起一个小图书馆来，或者设置起一所小试验室来。所以我说，第一要寻问题。脑子里没有问题之日，就是你知识生活寿终正寝之时!古人说，“待文王而兴者，凡民也。若夫豪杰之士，虽无文王犹兴。”试想伽利略和牛顿有多少藏书?有多少仪器?他们不过是有问题而已。有了问题而后他们自会造出仪器来解决他们的问题。没有问题的人们，关在图书馆里也不会用书，锁在试验室里也不会有什么发现。</p>\n<p>第二个方子也只有一句话：“总得多发展一点非职业的兴趣。”离开学校之后，大家总是寻个吃饭的职业。可是你寻得的职业未必就是你所学的，未必是你所心喜的，或者是你所学的而和你性情不相近的。在这种情况之下，工作往往成了苦工，就感觉不到兴趣了。为糊口而做那种非“性之所近而力之所能勉”的工作，就很难保持求知的兴趣的生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。</p>\n<p>一个人应该有他的职业，也应该有他非职业的玩艺儿，可以叫作业余活动。往往他的业余活动比他的职业还更重要，因为一个人成就怎样，往往靠他怎样利用他的闲暇时间。他用他的闲暇来打麻将，他就成了个赌徒；你用你的闲暇来做社会服务，你也许成个社会改革者；或者你用你的闲暇去研究历史，你也许成个史学家。你的闲暇往往定你的终身。英国19世纪的两个哲人，弥儿终身做东印度公司的秘书，然而他的业余工作使他在哲学上、经济学上、政治思想史上都占一个很高的位置；斯宾塞是一个测量工程师，然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人，几乎没有一个不善用他的闲暇时间的。职业不容易适合我们的性情，我们要想生活不苦痛不堕落，只有多方发展。</p>\n<p>有了这种心爱的玩艺儿，你就做六个钟头抹桌子工作也不会感觉烦闷了。因为你知道，抹了六个钟头的桌子之后，你可以回家做你的化学研究，或画完你的大幅山水，或写你的小说戏曲，或继续你的历史考据，或做你的社会改革事业。你有了这种称心如意的活动，生活就不枯寂了，精神也就不会烦闷了。</p>\n<p>第三个方子也只有一句话：“你得有一点信心。”我们生当这个不幸的时代，眼中所见，耳中所闻，无非是叫我们悲观失望的。特别是在这个年头毕业的你们，眼见自己的国家民族沉沦到这步田地，眼看世界只是强权的世界，望极天边好像看不见一线的光明——在这个年头不发狂自杀，已算是万幸了，怎么还能够保持一点内心的镇定和理想的信任呢?我要对你们说：这时候正是我们要培养我们的信心的时候!只要我们有信心，我们还有救。 </p>\n<p>古人说：“信心可以移山。”又说：“只要功夫深，生铁磨成绣花针。”你不信吗?当拿破仑的军队征服普鲁士，占据柏林的时候，有一位教授叫作费希特的，天天在讲堂劝他的国人要有信心，要信仰他们的民族是有世界的特殊使命的，是必定要复兴的。费希特死的时候，谁也不能预料德意志统一帝国何时可以实现，然而不满５０年，新的统一的德意志帝国居然实现了。</p>\n<p>一个国家的强弱盛衰，都不是偶然的，都不能逃出因果的铁律的。我们今日所受的苦痛和耻辱，都只是过去种种恶因种下的恶果。我们要收获将来的善果，必须努力种现在新因。一粒一粒地种，必有满仓满屋的收，这是我们今日应有的信心。我们要深信：今日的失败，都由于过去的不努力。我们要深信：今日的努力，必定有将来的大收成。</p>\n<p>佛典里有一句话：“福不唐捐。”唐捐就是白白地丢了。我们也应该说：“功不唐捐!”没有一点努力是会白白地丢了的。在我们看不见想不到的时候，在我们看不见的方向，你瞧!你下的种子早已生根发叶开花结果了!你不信吗?法国被普鲁士打败之后，割了两省地，赔了５０万万法郎的赔款。这时候有一位刻苦的科学家巴斯德终日埋头在他的化学试验室里做他的化学试验和微菌学研究。他是一个最爱国的人，然而他深信只有科学可以救国。他用一生的精力证明了三个科学问题：(1)每一种发酵作用都是由于一种微菌的发展；(2)每一种传染病都是一种微菌在生物体内的发展；(3)传染病的微菌，在特殊的培养之下可以减轻毒力，使他们从病菌变成防病的药苗。</p>\n<p>这三个问题在表面上似乎都和救国大事业没有多大关系。然而从第一个问题的证明，巴斯德定出做醋酿酒的新法，使全国的酒醋业每年减除极大的损失。从第二个问题的证明，巴斯德教全国的蚕丝业怎样选种防病，教全国的畜牧农家怎样防止牛羊瘟疫，又教全世界怎样注重消毒以减少外科手术的死亡率。从第三个问题的证明，巴斯德发明了牲畜的脾热瘟的疗治药苗，每年替法国农家减除了２０００万法郎的大损失；又发明了疯狗咬毒的治疗法，救济了无数的生命。所以英国的科学家赫胥黎在皇家学会里称颂巴斯德的功绩道：“法国给了德国５０万万法郎的赔款，巴斯德先生一个人研究科学的成就足够还清这一笔赔款了。”巴斯德对于科学有绝大的信心，所以他在国家蒙奇辱大难的时候，终不肯抛弃他的显微镜与试验室。他绝不想他在显微镜底下能偿还５０万万法郎的赔款，然而在他看不见想不到的时候，他已收获了科学救国的奇迹。</p>\n<p>朋友们，在你最悲观失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。 </p>\n<p>（摘自《胡适文存》第4集第4卷《胡适教育论著选》，人民教育出版社)</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p>今天早上看了央视新闻关于毕业的<a href=\"https://weibo.com/ttarticle/p/show?id=2309404387916615943710#_0\" target=\"_blank\" rel=\"noopener\">夜读文章</a><br>里面提到了胡适先生的《写给大学毕业生的一封信》，看了很有感触，所以摘抄至此，与未来的自己分享。</p>\n<p>本文是胡适先生1932年6月27日所作。虽然30年代那个血雨腥风的时代已经过去，现在的时代已经与当时不可同日而语，但是，读来还是感觉受益匪浅，胡适先生的谆谆教导之情溢于言表。本文中，胡适先生认为，大学生毕业有三条路可走：继续做学术研究；寻着相当的职业；做官，办党，革命。文中分析了大学毕业后遇到的“陷阱堕落的方式”，并给出了三个方子。</p>\n<p>这一两个星期里，各地的大学都有毕业的班次，都有很多的毕业生离开学校去开始他们的成人事业。 </p>\n<p>学生的生活是一种享有特殊优待的生活，不妨幼稚一点，不妨吵吵闹闹，社会都能纵容他们，不肯严格地要他们负行为的责任。现在他们要撑起自己的肩膀来挑他们自己的担子了。在这个国难最紧急的年头，他们的担子真不轻!我们祝他们的成功，同时也不忍不依据自己的经验，赠他们几句送行的赠言——虽未必是救命毫毛，也许做个防身的锦囊罢! </p>\n<p>你们毕业之后，可走的路不出这几条：绝少数的人还可以在国内或国外的研究院继续做学术研究；少数的人可以寻着相当的职业；此外还有做官，办党，革命三条路；再有就是在家享福或者失业亲居了。 </p>\n<p>走其余几条路的人，都不能没有堕落的危险。堕落的方式很多，总括起来，约有这两大类：</p>\n<p>第一是容易抛弃学生时代求知识的欲望。你们到了实际社会里，往往学非所用，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里即使向来抱有求知识学问的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的：书籍，实验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人能容易办到的。没有做学问的环境，又谁能怪我们抛弃学问呢? </p>\n<p>第二是容易抛弃学生时代理想的人生的追求。少年人初次和冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事了。渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就熔化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲抛兵而走，或者做了恶势的俘虏。你在那牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦!从此以后，你就甘心放弃理想人生的追求，甘心做现在社会的顺民了。要防御这两方面的堕落，一面要保持我们求知识的欲望，一面要保持我们对人生的追求。 </p>\n<p>有什么好方子呢?依我个人的观察和经验，有三种防身的药方是值得一试的。</p>\n<p>第一个方子只有一句话：“总得时时寻一两个值得研究的问题!”问题是知识学问的老祖宗：古往今来一切知识的产生与积聚，都是因为要解答问题——要解答实用上的困难和理论上的疑难。所谓“为知识而求知识”，其实也只是一种好奇心追求某种问题的解答，不过因为那种问题的性质不必是直接应用的，人们就觉得这是无所谓的求知识了。</p>\n<p>我们出学校之后，离开了做学问的环境，如果没有一两个值得解答的问题在脑子里盘旋，就很难保持求学问的热心。可是，如果你有了一个真有趣的问题逗你去想它，天天引诱你去解决它，天天对你挑衅你无可奈何它——这时候，你就会同恋爱一个女子发了疯一样，坐也坐不下，睡也睡不安，没工夫也得偷出工夫去陪她，没钱也得缩衣节食去巴结她。没有书，你自会变卖家私去买书；没有仪器，你自会典押衣物去置办仪器；没有师友，你自会不远千里去寻师访友。你只要有疑难问题来逼你时时用脑子，你自然会保持发展你对学问的兴趣，即使在最贫乏的知识中，你也会慢慢地，聚起一个小图书馆来，或者设置起一所小试验室来。所以我说，第一要寻问题。脑子里没有问题之日，就是你知识生活寿终正寝之时!古人说，“待文王而兴者，凡民也。若夫豪杰之士，虽无文王犹兴。”试想伽利略和牛顿有多少藏书?有多少仪器?他们不过是有问题而已。有了问题而后他们自会造出仪器来解决他们的问题。没有问题的人们，关在图书馆里也不会用书，锁在试验室里也不会有什么发现。</p>\n<p>第二个方子也只有一句话：“总得多发展一点非职业的兴趣。”离开学校之后，大家总是寻个吃饭的职业。可是你寻得的职业未必就是你所学的，未必是你所心喜的，或者是你所学的而和你性情不相近的。在这种情况之下，工作往往成了苦工，就感觉不到兴趣了。为糊口而做那种非“性之所近而力之所能勉”的工作，就很难保持求知的兴趣的生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。</p>\n<p>一个人应该有他的职业，也应该有他非职业的玩艺儿，可以叫作业余活动。往往他的业余活动比他的职业还更重要，因为一个人成就怎样，往往靠他怎样利用他的闲暇时间。他用他的闲暇来打麻将，他就成了个赌徒；你用你的闲暇来做社会服务，你也许成个社会改革者；或者你用你的闲暇去研究历史，你也许成个史学家。你的闲暇往往定你的终身。英国19世纪的两个哲人，弥儿终身做东印度公司的秘书，然而他的业余工作使他在哲学上、经济学上、政治思想史上都占一个很高的位置；斯宾塞是一个测量工程师，然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人，几乎没有一个不善用他的闲暇时间的。职业不容易适合我们的性情，我们要想生活不苦痛不堕落，只有多方发展。</p>\n<p>有了这种心爱的玩艺儿，你就做六个钟头抹桌子工作也不会感觉烦闷了。因为你知道，抹了六个钟头的桌子之后，你可以回家做你的化学研究，或画完你的大幅山水，或写你的小说戏曲，或继续你的历史考据，或做你的社会改革事业。你有了这种称心如意的活动，生活就不枯寂了，精神也就不会烦闷了。</p>\n<p>第三个方子也只有一句话：“你得有一点信心。”我们生当这个不幸的时代，眼中所见，耳中所闻，无非是叫我们悲观失望的。特别是在这个年头毕业的你们，眼见自己的国家民族沉沦到这步田地，眼看世界只是强权的世界，望极天边好像看不见一线的光明——在这个年头不发狂自杀，已算是万幸了，怎么还能够保持一点内心的镇定和理想的信任呢?我要对你们说：这时候正是我们要培养我们的信心的时候!只要我们有信心，我们还有救。 </p>\n<p>古人说：“信心可以移山。”又说：“只要功夫深，生铁磨成绣花针。”你不信吗?当拿破仑的军队征服普鲁士，占据柏林的时候，有一位教授叫作费希特的，天天在讲堂劝他的国人要有信心，要信仰他们的民族是有世界的特殊使命的，是必定要复兴的。费希特死的时候，谁也不能预料德意志统一帝国何时可以实现，然而不满５０年，新的统一的德意志帝国居然实现了。</p>\n<p>一个国家的强弱盛衰，都不是偶然的，都不能逃出因果的铁律的。我们今日所受的苦痛和耻辱，都只是过去种种恶因种下的恶果。我们要收获将来的善果，必须努力种现在新因。一粒一粒地种，必有满仓满屋的收，这是我们今日应有的信心。我们要深信：今日的失败，都由于过去的不努力。我们要深信：今日的努力，必定有将来的大收成。</p>\n<p>佛典里有一句话：“福不唐捐。”唐捐就是白白地丢了。我们也应该说：“功不唐捐!”没有一点努力是会白白地丢了的。在我们看不见想不到的时候，在我们看不见的方向，你瞧!你下的种子早已生根发叶开花结果了!你不信吗?法国被普鲁士打败之后，割了两省地，赔了５０万万法郎的赔款。这时候有一位刻苦的科学家巴斯德终日埋头在他的化学试验室里做他的化学试验和微菌学研究。他是一个最爱国的人，然而他深信只有科学可以救国。他用一生的精力证明了三个科学问题：(1)每一种发酵作用都是由于一种微菌的发展；(2)每一种传染病都是一种微菌在生物体内的发展；(3)传染病的微菌，在特殊的培养之下可以减轻毒力，使他们从病菌变成防病的药苗。</p>\n<p>这三个问题在表面上似乎都和救国大事业没有多大关系。然而从第一个问题的证明，巴斯德定出做醋酿酒的新法，使全国的酒醋业每年减除极大的损失。从第二个问题的证明，巴斯德教全国的蚕丝业怎样选种防病，教全国的畜牧农家怎样防止牛羊瘟疫，又教全世界怎样注重消毒以减少外科手术的死亡率。从第三个问题的证明，巴斯德发明了牲畜的脾热瘟的疗治药苗，每年替法国农家减除了２０００万法郎的大损失；又发明了疯狗咬毒的治疗法，救济了无数的生命。所以英国的科学家赫胥黎在皇家学会里称颂巴斯德的功绩道：“法国给了德国５０万万法郎的赔款，巴斯德先生一个人研究科学的成就足够还清这一笔赔款了。”巴斯德对于科学有绝大的信心，所以他在国家蒙奇辱大难的时候，终不肯抛弃他的显微镜与试验室。他绝不想他在显微镜底下能偿还５０万万法郎的赔款，然而在他看不见想不到的时候，他已收获了科学救国的奇迹。</p>\n<p>朋友们，在你最悲观失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。 </p>\n<p>（摘自《胡适文存》第4集第4卷《胡适教育论著选》，人民教育出版社)</p>\n"},{"title":"翻译-新手向大型网络应用扩展","date":"2019-04-08T16:00:00.000Z","_content":"## 前言\n上个星期就准备翻译出这篇文章来着，但是毕设答辩加上清明节假期就导致拖到了今天。\n\n话不多说，这篇文章是在阮一峰老师的每周记录中看到的。因为之前看了顶天的《大型网络扩展和JAVA中间件》这本书（2/3），所以对这个话题比较熟悉。加上这篇外文有很好看的插图，清晰易懂，没有技术性的阅读障碍，是通过逻辑性的思维进行网站应用扩展的讲解，所以我把它翻译出来，复习使用。\n\n* 原文地址: [Scaling webapps for newbs & non-techies](https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/)\n\n## 正文\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1web0xb1ej20zk0k0wrd.jpg)\n\n这篇入门总结了将单服务器扩展到百万级别用户大型网站**扩展**的基本原则。这篇文章针对的是，技术领域的新手或者非开发人员的，所以如果你正在部署更高大上的应用服务的话，这篇文章并不适合你。\n\n如果这正是你想要的，那么开始吧。\n\n### 什么是扩展\n\n你把你编写的网站，网上商店、社交网络等，部署上线，一切都运行的很美好：每天都有几百用户访问、请求响应及时、调用立即完成。一切都有条不紊地不断运行。\n\n但是一些\"不好\"的事情发生了：你大获成功！\n\n几百、上千、上万的用户每时每秒不断涌入，并且在不断增加。这看起来是商业上最棒的消息了，但同时也在对你的基础设施（服务）说：亲爱的，你大事不妙。因为对于网站的服务来说必须要扩展了，因为它能够：\n\n* 同时为更多的用户提供服务\n* 全天在线，不停机\n* 服务全世界用户\n\n### 扩展是怎么工作的\n以前，这篇文章要以讨论\"纵向\"扩展（\"vertical\"）和\"横向\"（\"horizontal\"）扩展来开头。总之，纵向扩展指的是在更强大的机器上运行相同的应用，而横向扩展是并行运行多个服务进程。\n\n现在，已经没有人纵向扩展了，理由如下：\n* 计算机的价格和性能成指数增加的关系\n* 单机可以运行很快，但是在纵向上的扩展是有极限的\n* 多核CPU意味着单机也能有效的并行运算，那不如一开始就并行化好了\n\n那么横向扩展的必要步骤有那些呢？\n\n### 1. 单服务器+数据库\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1webi6rjsj20jg0bumy3.jpg)\n你的网站后端可能一开始就是这个样子的。一个应用服务器执行你的业务逻辑，数据库存储数据。事情简单而美好，但是想要其高可用，只能用更好的机器--这并不好。\n\n### 2. 增加反向代理\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g1webuwgw7j20jg0al3zf.jpg)\n\n为了使你的架构能够适应更大的扩展，第一步要加一个\"反向代理\"。它就相当于酒店的前台。当然，你可以让客人直接去自己的房间；但是，你真正需要的事一个中间人，能够确认这位客人是否允许进入、到底有没有订房或者正去往一个正确房间的路上。同时你还要要告知客户，他要的房间可不可用、能不能去，以免尴尬的徘徊。以上就是反向代理的工作。代理就是用来接收和转发请求的。通常情况下，请求是从我们的服务器发出到互联网上的。但是这次，是从互联网上路由到我们的服务器上，所以我们\"反向代理\"。\n\n一个代理需要完成以下任务：\n\n* 运行状态检查，确保我们的服务器还在运行\n* 将请求路由（定向）到正确的终点\n* 认证，确保请求的发送方是有许可的\n* 防火墙，确保请求方访问的是服务器上有权限的内容\n\n### 3. 引入负载均衡\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wec96qloj20jg0csabd.jpg)\n\n许多反向代理还能够负载均衡。负载均衡器是一个很简单的概念：想象一下，在一分钟内有一百个用户要向你的网上商店付款，很不幸的是，你的支付系统只能同时处理50个支付请求。怎么解决？同时运行两个支付系统服务器。\n\n现在负载均衡器的作用就是将支付请求分摊到这两个服务器上，用户1到左边的服务器，用户2区右边的服务器，用户3去左边的...以此类推。\n\n如果同时🈶500个支付请求怎么办呢？加到10个支付系统服务器，然后用负载均衡分摊请求。\n\n### 4. 扩展数据库\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wecqv3woj20jg0bwdh3.jpg)\n\n运用负载均衡可以将压力分在各个服务器上。但是你发现问题了么？虽然我们可能有成百上千个服务器，用来处理请求，但是只有一个数据库存取数据。\n\n所以，我们可以以同样的方式来扩展数据库嘛？很不幸的是，不行。关键在于一致性的问题。我们系统中的每个部分都要确保使用数据的一致性。不一致的数据导致各种棘手的问题：订单为重复执行2次，从余额100的账户里扣除2次90元的付款等等。所以我们怎样才能扩展数据库，同时又保证数据的一致性呢?\n\n首先我们要把数据库分成不同的部分。一部分专门用来接收和存储数据，其他部分专门用来读取数据。这种方案有时称为主/从魔石，有时称为使用只读副本写入。而且假定服务器读数据比写数据的次数更多。\n这样的解决方法就能保证一致性，因为数据是从单个接口写入的，而且数据流动是单向的，从写到读。\n缺点就是数据输入还是由单个数据库完成，这对于中小型的应用来说是OK的，但是对于像Facebook那样级别的就不行。更进一步的数据库扩展将在之后讨论。\n\n### 5. 微服务\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wed4e3tyj20jg0c3tag.jpg)\n\n到现在，我们都用一个服务器去完成所有的工作：处理支付、订单、库存、提供web服务、管理用户账号等。\n\n这未必不是一件坏事，单个服务器意味着更低的复杂度（结构上的），对我们开发者来说没那么头疼。\n但是随着扩展的增加，事情开始变得复杂和低效：\n\n* 不同的服务用在不同的范围。对于用户登录这个流程，可能在很多页面都有，涉及到很多的资源。但是这一些都是在一个服务器上完成的。\n* 我们的代发团队随着应用而成长。但是越来越多的开发者在一个服务器上工作，不可避免的就会干扰到别人。\n* 所有服务都在一起，意味着每次发新版都要将所有服务都停掉。这会导致一个很严重的相互依赖关系，一个团队已经做完了，等着发布上线，但是另一个团队可能才完成一半。\n\n解决这个问题的方法是一个结构范式，它已经掀起了开发界的一大浪潮：微服务。思想很简单，将服务器分成多个功能单元，将他们部署成独立的、互联的迷你服务器。\n这样做有几大好处：\n\n* 每个服务都可以独立扩展，让我们能够机动的适应新需求。\n* 每个开发团队独立开发，对自己的微服务生命周期负责（创建、部署、更新等）\n* 每个微服务运用自己的资源，如数据库（一定程度上缓解了4中的问题）\n\n### 6. 缓存和内容分发网络CDN\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wedhmbk3j20jg0c876a.jpg)\n\n怎么才能工作更高效呢？不要什么都干！\n我们的web应用很大一部分都是静态资源，像图片、js脚本、css文件、预渲染的进入页面等。\n所以相比较每次都重新请求数据资源，我们不如将一些结果放在简单的缓存之中，谁需要就去拿，不用干扰到后面的服务器。\n\n缓存的大哥叫内容分发网络CDN，是一组分布在全世界范围内的缓存。这让我们能够从用户就近的节点向他们分发内容，不需要每次都翻山越岭了。\n\n### 7. 消息队列\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g1wedx2oaxj20jg0bdjtb.jpg)\n\n你去过游乐园吗？你是不是每次都直接去售票厅就能买到票呢？大部分情况都要排队吧应该。\n政府机构、邮局和游乐园都是\"sub-capacity parallelism\"很好的例子。他们是并行的--多个售票厅同时售票。\n但是貌似不可能有足够的售票厅能让每个人都能立即买到票，所以队列就形成了。\n\n在大型网站上同样可以套用这个概念。成千上万的图片每时每刻上传到Instagram、Facebook上面，每张图片都要被处理、调整、分析和打标，这是一个耗时的过程。\n所以为了使用户不用等待所有流程都结束，服务器接收到图片之后，只完成下面3件事：\n\n* 存储原始图像数据\n* 告诉用户上传成功了\n* 向一个大堆增加虚拟标签，说明接下来需要做些什么\n\n接下来这些标签会被许多的服务器拿到，这些服务器会完成标签上需要完成的任务，确认之后再返回标签，直到所有的任务都完成。\n系统将这一堆标签叫做\"消息队列\"，运用这个队列有几大好处：\n\n* 它将任务和处理器解耦。有时许多图片需要处理，有时很少；有时有很多可以进行处理的节点，有时可用的很少。\n通过在代办事务中增加任务标签的形式，而不是直接送过去处理，可以保证我们的系统是响应的，且没有任务丢失。\n\n* 它使我们能够按需扩展。因为启动服务器是耗时的，如果已经有很多用户上传了图片需要处理，而图片是直接传给服务器的，那么再启动等多的服务器就已经迟了。相反，如果我们有消息队列，就可以扩展处理能力来处理那些需要处理的任务。\n\n---\n👌如果我们的系统已经经过了以上的扩展，那么已经可以应对大流量了。但是如果我们想要更大更强呢？以下是几个选项。\n\n### 8. 分片/分区\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1wee9hj6jj20jg0bf401.jpg)\n\n什么是分区？定义如下：\n\n\"Sharding是一种通过将应用程序的堆栈分成多个单元来并行化应用程序堆栈的技术，每个单元负责某个键或命名空间\"\n\n额，所以到底什么是分区？其实很简单：需要访问20亿Facebook用户的档案，那就把Facebook分成26个小Facebook，每个Facebook服务不同姓名首字母大写的用户。\n\n分区不一定按照首字母划分，可以是地区、使用频率（头部用户给更好的服务）等。\n你可以将服务器分区、数据库分许，或者任何你应用系统中的一个方面，只要满足你的需求。\n\n### 9. 对负载均衡器进行负载均衡\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1weelr6h4j20jg0bf403.jpg)\n\n单个负载均衡器只能做那么多，即使你购买超贵超牛的负载均衡器，性能都是有瓶颈的。\n\n很幸运的是，有一个全球性的、分散的、超级稳定的层次，能够在流量到达负载均衡器之前进行分流操作。那就是\"域名系统\"，DNS。全球域名注册表将\"github.com\"映射到\"XXX.XXX.XXX.XXX\"这个IP地址上，同时也允许我们将特定域名映射到多个IP地址上，这样就能到达多个不同的负载均衡器了。\n\n---\n\n好了，我们做的已经很多了，希望这篇对你有所帮助。但是如果你是IT行业的一员，那么你肯定会问，\"到底啥是云服务？\"\n\n### 云计算和ServerLess（无服务器）\n\n什么是云服务？它是对于上面提到的所有问题一个便宜又高效的解决方法，简单来说就是，别自己解决他们。\n\n而是让云服务厂商根据你的系统和需求为你提供扩展服务，你不用考虑任何复杂的问题，不要再造轮子。\n\n后面略...（介绍下一篇文章的东西）\n\n\n","source":"_posts/transfer-scaling-webapps-for-newbs-and-non-techies.md","raw":"---\ntitle: 翻译-新手向大型网络应用扩展\ndate: 2019/4/09\ncategories: \n    - 技术总结\ntags: \n    - 翻译\n    - 大型网络扩展\n---\n## 前言\n上个星期就准备翻译出这篇文章来着，但是毕设答辩加上清明节假期就导致拖到了今天。\n\n话不多说，这篇文章是在阮一峰老师的每周记录中看到的。因为之前看了顶天的《大型网络扩展和JAVA中间件》这本书（2/3），所以对这个话题比较熟悉。加上这篇外文有很好看的插图，清晰易懂，没有技术性的阅读障碍，是通过逻辑性的思维进行网站应用扩展的讲解，所以我把它翻译出来，复习使用。\n\n* 原文地址: [Scaling webapps for newbs & non-techies](https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/)\n\n## 正文\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1web0xb1ej20zk0k0wrd.jpg)\n\n这篇入门总结了将单服务器扩展到百万级别用户大型网站**扩展**的基本原则。这篇文章针对的是，技术领域的新手或者非开发人员的，所以如果你正在部署更高大上的应用服务的话，这篇文章并不适合你。\n\n如果这正是你想要的，那么开始吧。\n\n### 什么是扩展\n\n你把你编写的网站，网上商店、社交网络等，部署上线，一切都运行的很美好：每天都有几百用户访问、请求响应及时、调用立即完成。一切都有条不紊地不断运行。\n\n但是一些\"不好\"的事情发生了：你大获成功！\n\n几百、上千、上万的用户每时每秒不断涌入，并且在不断增加。这看起来是商业上最棒的消息了，但同时也在对你的基础设施（服务）说：亲爱的，你大事不妙。因为对于网站的服务来说必须要扩展了，因为它能够：\n\n* 同时为更多的用户提供服务\n* 全天在线，不停机\n* 服务全世界用户\n\n### 扩展是怎么工作的\n以前，这篇文章要以讨论\"纵向\"扩展（\"vertical\"）和\"横向\"（\"horizontal\"）扩展来开头。总之，纵向扩展指的是在更强大的机器上运行相同的应用，而横向扩展是并行运行多个服务进程。\n\n现在，已经没有人纵向扩展了，理由如下：\n* 计算机的价格和性能成指数增加的关系\n* 单机可以运行很快，但是在纵向上的扩展是有极限的\n* 多核CPU意味着单机也能有效的并行运算，那不如一开始就并行化好了\n\n那么横向扩展的必要步骤有那些呢？\n\n### 1. 单服务器+数据库\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1webi6rjsj20jg0bumy3.jpg)\n你的网站后端可能一开始就是这个样子的。一个应用服务器执行你的业务逻辑，数据库存储数据。事情简单而美好，但是想要其高可用，只能用更好的机器--这并不好。\n\n### 2. 增加反向代理\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g1webuwgw7j20jg0al3zf.jpg)\n\n为了使你的架构能够适应更大的扩展，第一步要加一个\"反向代理\"。它就相当于酒店的前台。当然，你可以让客人直接去自己的房间；但是，你真正需要的事一个中间人，能够确认这位客人是否允许进入、到底有没有订房或者正去往一个正确房间的路上。同时你还要要告知客户，他要的房间可不可用、能不能去，以免尴尬的徘徊。以上就是反向代理的工作。代理就是用来接收和转发请求的。通常情况下，请求是从我们的服务器发出到互联网上的。但是这次，是从互联网上路由到我们的服务器上，所以我们\"反向代理\"。\n\n一个代理需要完成以下任务：\n\n* 运行状态检查，确保我们的服务器还在运行\n* 将请求路由（定向）到正确的终点\n* 认证，确保请求的发送方是有许可的\n* 防火墙，确保请求方访问的是服务器上有权限的内容\n\n### 3. 引入负载均衡\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wec96qloj20jg0csabd.jpg)\n\n许多反向代理还能够负载均衡。负载均衡器是一个很简单的概念：想象一下，在一分钟内有一百个用户要向你的网上商店付款，很不幸的是，你的支付系统只能同时处理50个支付请求。怎么解决？同时运行两个支付系统服务器。\n\n现在负载均衡器的作用就是将支付请求分摊到这两个服务器上，用户1到左边的服务器，用户2区右边的服务器，用户3去左边的...以此类推。\n\n如果同时🈶500个支付请求怎么办呢？加到10个支付系统服务器，然后用负载均衡分摊请求。\n\n### 4. 扩展数据库\n\n![](https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wecqv3woj20jg0bwdh3.jpg)\n\n运用负载均衡可以将压力分在各个服务器上。但是你发现问题了么？虽然我们可能有成百上千个服务器，用来处理请求，但是只有一个数据库存取数据。\n\n所以，我们可以以同样的方式来扩展数据库嘛？很不幸的是，不行。关键在于一致性的问题。我们系统中的每个部分都要确保使用数据的一致性。不一致的数据导致各种棘手的问题：订单为重复执行2次，从余额100的账户里扣除2次90元的付款等等。所以我们怎样才能扩展数据库，同时又保证数据的一致性呢?\n\n首先我们要把数据库分成不同的部分。一部分专门用来接收和存储数据，其他部分专门用来读取数据。这种方案有时称为主/从魔石，有时称为使用只读副本写入。而且假定服务器读数据比写数据的次数更多。\n这样的解决方法就能保证一致性，因为数据是从单个接口写入的，而且数据流动是单向的，从写到读。\n缺点就是数据输入还是由单个数据库完成，这对于中小型的应用来说是OK的，但是对于像Facebook那样级别的就不行。更进一步的数据库扩展将在之后讨论。\n\n### 5. 微服务\n\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wed4e3tyj20jg0c3tag.jpg)\n\n到现在，我们都用一个服务器去完成所有的工作：处理支付、订单、库存、提供web服务、管理用户账号等。\n\n这未必不是一件坏事，单个服务器意味着更低的复杂度（结构上的），对我们开发者来说没那么头疼。\n但是随着扩展的增加，事情开始变得复杂和低效：\n\n* 不同的服务用在不同的范围。对于用户登录这个流程，可能在很多页面都有，涉及到很多的资源。但是这一些都是在一个服务器上完成的。\n* 我们的代发团队随着应用而成长。但是越来越多的开发者在一个服务器上工作，不可避免的就会干扰到别人。\n* 所有服务都在一起，意味着每次发新版都要将所有服务都停掉。这会导致一个很严重的相互依赖关系，一个团队已经做完了，等着发布上线，但是另一个团队可能才完成一半。\n\n解决这个问题的方法是一个结构范式，它已经掀起了开发界的一大浪潮：微服务。思想很简单，将服务器分成多个功能单元，将他们部署成独立的、互联的迷你服务器。\n这样做有几大好处：\n\n* 每个服务都可以独立扩展，让我们能够机动的适应新需求。\n* 每个开发团队独立开发，对自己的微服务生命周期负责（创建、部署、更新等）\n* 每个微服务运用自己的资源，如数据库（一定程度上缓解了4中的问题）\n\n### 6. 缓存和内容分发网络CDN\n![](https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wedhmbk3j20jg0c876a.jpg)\n\n怎么才能工作更高效呢？不要什么都干！\n我们的web应用很大一部分都是静态资源，像图片、js脚本、css文件、预渲染的进入页面等。\n所以相比较每次都重新请求数据资源，我们不如将一些结果放在简单的缓存之中，谁需要就去拿，不用干扰到后面的服务器。\n\n缓存的大哥叫内容分发网络CDN，是一组分布在全世界范围内的缓存。这让我们能够从用户就近的节点向他们分发内容，不需要每次都翻山越岭了。\n\n### 7. 消息队列\n\n![](https://wx3.sinaimg.cn/mw1024/6a49516fly1g1wedx2oaxj20jg0bdjtb.jpg)\n\n你去过游乐园吗？你是不是每次都直接去售票厅就能买到票呢？大部分情况都要排队吧应该。\n政府机构、邮局和游乐园都是\"sub-capacity parallelism\"很好的例子。他们是并行的--多个售票厅同时售票。\n但是貌似不可能有足够的售票厅能让每个人都能立即买到票，所以队列就形成了。\n\n在大型网站上同样可以套用这个概念。成千上万的图片每时每刻上传到Instagram、Facebook上面，每张图片都要被处理、调整、分析和打标，这是一个耗时的过程。\n所以为了使用户不用等待所有流程都结束，服务器接收到图片之后，只完成下面3件事：\n\n* 存储原始图像数据\n* 告诉用户上传成功了\n* 向一个大堆增加虚拟标签，说明接下来需要做些什么\n\n接下来这些标签会被许多的服务器拿到，这些服务器会完成标签上需要完成的任务，确认之后再返回标签，直到所有的任务都完成。\n系统将这一堆标签叫做\"消息队列\"，运用这个队列有几大好处：\n\n* 它将任务和处理器解耦。有时许多图片需要处理，有时很少；有时有很多可以进行处理的节点，有时可用的很少。\n通过在代办事务中增加任务标签的形式，而不是直接送过去处理，可以保证我们的系统是响应的，且没有任务丢失。\n\n* 它使我们能够按需扩展。因为启动服务器是耗时的，如果已经有很多用户上传了图片需要处理，而图片是直接传给服务器的，那么再启动等多的服务器就已经迟了。相反，如果我们有消息队列，就可以扩展处理能力来处理那些需要处理的任务。\n\n---\n👌如果我们的系统已经经过了以上的扩展，那么已经可以应对大流量了。但是如果我们想要更大更强呢？以下是几个选项。\n\n### 8. 分片/分区\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1wee9hj6jj20jg0bf401.jpg)\n\n什么是分区？定义如下：\n\n\"Sharding是一种通过将应用程序的堆栈分成多个单元来并行化应用程序堆栈的技术，每个单元负责某个键或命名空间\"\n\n额，所以到底什么是分区？其实很简单：需要访问20亿Facebook用户的档案，那就把Facebook分成26个小Facebook，每个Facebook服务不同姓名首字母大写的用户。\n\n分区不一定按照首字母划分，可以是地区、使用频率（头部用户给更好的服务）等。\n你可以将服务器分区、数据库分许，或者任何你应用系统中的一个方面，只要满足你的需求。\n\n### 9. 对负载均衡器进行负载均衡\n\n![](https://wx4.sinaimg.cn/mw1024/6a49516fly1g1weelr6h4j20jg0bf403.jpg)\n\n单个负载均衡器只能做那么多，即使你购买超贵超牛的负载均衡器，性能都是有瓶颈的。\n\n很幸运的是，有一个全球性的、分散的、超级稳定的层次，能够在流量到达负载均衡器之前进行分流操作。那就是\"域名系统\"，DNS。全球域名注册表将\"github.com\"映射到\"XXX.XXX.XXX.XXX\"这个IP地址上，同时也允许我们将特定域名映射到多个IP地址上，这样就能到达多个不同的负载均衡器了。\n\n---\n\n好了，我们做的已经很多了，希望这篇对你有所帮助。但是如果你是IT行业的一员，那么你肯定会问，\"到底啥是云服务？\"\n\n### 云计算和ServerLess（无服务器）\n\n什么是云服务？它是对于上面提到的所有问题一个便宜又高效的解决方法，简单来说就是，别自己解决他们。\n\n而是让云服务厂商根据你的系统和需求为你提供扩展服务，你不用考虑任何复杂的问题，不要再造轮子。\n\n后面略...（介绍下一篇文章的东西）\n\n\n","slug":"transfer-scaling-webapps-for-newbs-and-non-techies","published":1,"updated":"2019-06-04T01:06:18.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwi7002048upmdlwgy0i","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>上个星期就准备翻译出这篇文章来着，但是毕设答辩加上清明节假期就导致拖到了今天。</p>\n<p>话不多说，这篇文章是在阮一峰老师的每周记录中看到的。因为之前看了顶天的《大型网络扩展和JAVA中间件》这本书（2/3），所以对这个话题比较熟悉。加上这篇外文有很好看的插图，清晰易懂，没有技术性的阅读障碍，是通过逻辑性的思维进行网站应用扩展的讲解，所以我把它翻译出来，复习使用。</p>\n<ul>\n<li>原文地址: <a href=\"https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/\" target=\"_blank\" rel=\"noopener\">Scaling webapps for newbs &amp; non-techies</a></li>\n</ul>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1web0xb1ej20zk0k0wrd.jpg\" alt></p>\n<p>这篇入门总结了将单服务器扩展到百万级别用户大型网站<strong>扩展</strong>的基本原则。这篇文章针对的是，技术领域的新手或者非开发人员的，所以如果你正在部署更高大上的应用服务的话，这篇文章并不适合你。</p>\n<p>如果这正是你想要的，那么开始吧。</p>\n<h3 id=\"什么是扩展\"><a href=\"#什么是扩展\" class=\"headerlink\" title=\"什么是扩展\"></a>什么是扩展</h3><p>你把你编写的网站，网上商店、社交网络等，部署上线，一切都运行的很美好：每天都有几百用户访问、请求响应及时、调用立即完成。一切都有条不紊地不断运行。</p>\n<p>但是一些”不好”的事情发生了：你大获成功！</p>\n<p>几百、上千、上万的用户每时每秒不断涌入，并且在不断增加。这看起来是商业上最棒的消息了，但同时也在对你的基础设施（服务）说：亲爱的，你大事不妙。因为对于网站的服务来说必须要扩展了，因为它能够：</p>\n<ul>\n<li>同时为更多的用户提供服务</li>\n<li>全天在线，不停机</li>\n<li>服务全世界用户</li>\n</ul>\n<h3 id=\"扩展是怎么工作的\"><a href=\"#扩展是怎么工作的\" class=\"headerlink\" title=\"扩展是怎么工作的\"></a>扩展是怎么工作的</h3><p>以前，这篇文章要以讨论”纵向”扩展（”vertical”）和”横向”（”horizontal”）扩展来开头。总之，纵向扩展指的是在更强大的机器上运行相同的应用，而横向扩展是并行运行多个服务进程。</p>\n<p>现在，已经没有人纵向扩展了，理由如下：</p>\n<ul>\n<li>计算机的价格和性能成指数增加的关系</li>\n<li>单机可以运行很快，但是在纵向上的扩展是有极限的</li>\n<li>多核CPU意味着单机也能有效的并行运算，那不如一开始就并行化好了</li>\n</ul>\n<p>那么横向扩展的必要步骤有那些呢？</p>\n<h3 id=\"1-单服务器-数据库\"><a href=\"#1-单服务器-数据库\" class=\"headerlink\" title=\"1. 单服务器+数据库\"></a>1. 单服务器+数据库</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1webi6rjsj20jg0bumy3.jpg\" alt><br>你的网站后端可能一开始就是这个样子的。一个应用服务器执行你的业务逻辑，数据库存储数据。事情简单而美好，但是想要其高可用，只能用更好的机器–这并不好。</p>\n<h3 id=\"2-增加反向代理\"><a href=\"#2-增加反向代理\" class=\"headerlink\" title=\"2. 增加反向代理\"></a>2. 增加反向代理</h3><p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g1webuwgw7j20jg0al3zf.jpg\" alt></p>\n<p>为了使你的架构能够适应更大的扩展，第一步要加一个”反向代理”。它就相当于酒店的前台。当然，你可以让客人直接去自己的房间；但是，你真正需要的事一个中间人，能够确认这位客人是否允许进入、到底有没有订房或者正去往一个正确房间的路上。同时你还要要告知客户，他要的房间可不可用、能不能去，以免尴尬的徘徊。以上就是反向代理的工作。代理就是用来接收和转发请求的。通常情况下，请求是从我们的服务器发出到互联网上的。但是这次，是从互联网上路由到我们的服务器上，所以我们”反向代理”。</p>\n<p>一个代理需要完成以下任务：</p>\n<ul>\n<li>运行状态检查，确保我们的服务器还在运行</li>\n<li>将请求路由（定向）到正确的终点</li>\n<li>认证，确保请求的发送方是有许可的</li>\n<li>防火墙，确保请求方访问的是服务器上有权限的内容</li>\n</ul>\n<h3 id=\"3-引入负载均衡\"><a href=\"#3-引入负载均衡\" class=\"headerlink\" title=\"3. 引入负载均衡\"></a>3. 引入负载均衡</h3><p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wec96qloj20jg0csabd.jpg\" alt></p>\n<p>许多反向代理还能够负载均衡。负载均衡器是一个很简单的概念：想象一下，在一分钟内有一百个用户要向你的网上商店付款，很不幸的是，你的支付系统只能同时处理50个支付请求。怎么解决？同时运行两个支付系统服务器。</p>\n<p>现在负载均衡器的作用就是将支付请求分摊到这两个服务器上，用户1到左边的服务器，用户2区右边的服务器，用户3去左边的…以此类推。</p>\n<p>如果同时🈶500个支付请求怎么办呢？加到10个支付系统服务器，然后用负载均衡分摊请求。</p>\n<h3 id=\"4-扩展数据库\"><a href=\"#4-扩展数据库\" class=\"headerlink\" title=\"4. 扩展数据库\"></a>4. 扩展数据库</h3><p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wecqv3woj20jg0bwdh3.jpg\" alt></p>\n<p>运用负载均衡可以将压力分在各个服务器上。但是你发现问题了么？虽然我们可能有成百上千个服务器，用来处理请求，但是只有一个数据库存取数据。</p>\n<p>所以，我们可以以同样的方式来扩展数据库嘛？很不幸的是，不行。关键在于一致性的问题。我们系统中的每个部分都要确保使用数据的一致性。不一致的数据导致各种棘手的问题：订单为重复执行2次，从余额100的账户里扣除2次90元的付款等等。所以我们怎样才能扩展数据库，同时又保证数据的一致性呢?</p>\n<p>首先我们要把数据库分成不同的部分。一部分专门用来接收和存储数据，其他部分专门用来读取数据。这种方案有时称为主/从魔石，有时称为使用只读副本写入。而且假定服务器读数据比写数据的次数更多。<br>这样的解决方法就能保证一致性，因为数据是从单个接口写入的，而且数据流动是单向的，从写到读。<br>缺点就是数据输入还是由单个数据库完成，这对于中小型的应用来说是OK的，但是对于像Facebook那样级别的就不行。更进一步的数据库扩展将在之后讨论。</p>\n<h3 id=\"5-微服务\"><a href=\"#5-微服务\" class=\"headerlink\" title=\"5. 微服务\"></a>5. 微服务</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wed4e3tyj20jg0c3tag.jpg\" alt></p>\n<p>到现在，我们都用一个服务器去完成所有的工作：处理支付、订单、库存、提供web服务、管理用户账号等。</p>\n<p>这未必不是一件坏事，单个服务器意味着更低的复杂度（结构上的），对我们开发者来说没那么头疼。<br>但是随着扩展的增加，事情开始变得复杂和低效：</p>\n<ul>\n<li>不同的服务用在不同的范围。对于用户登录这个流程，可能在很多页面都有，涉及到很多的资源。但是这一些都是在一个服务器上完成的。</li>\n<li>我们的代发团队随着应用而成长。但是越来越多的开发者在一个服务器上工作，不可避免的就会干扰到别人。</li>\n<li>所有服务都在一起，意味着每次发新版都要将所有服务都停掉。这会导致一个很严重的相互依赖关系，一个团队已经做完了，等着发布上线，但是另一个团队可能才完成一半。</li>\n</ul>\n<p>解决这个问题的方法是一个结构范式，它已经掀起了开发界的一大浪潮：微服务。思想很简单，将服务器分成多个功能单元，将他们部署成独立的、互联的迷你服务器。<br>这样做有几大好处：</p>\n<ul>\n<li>每个服务都可以独立扩展，让我们能够机动的适应新需求。</li>\n<li>每个开发团队独立开发，对自己的微服务生命周期负责（创建、部署、更新等）</li>\n<li>每个微服务运用自己的资源，如数据库（一定程度上缓解了4中的问题）</li>\n</ul>\n<h3 id=\"6-缓存和内容分发网络CDN\"><a href=\"#6-缓存和内容分发网络CDN\" class=\"headerlink\" title=\"6. 缓存和内容分发网络CDN\"></a>6. 缓存和内容分发网络CDN</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wedhmbk3j20jg0c876a.jpg\" alt></p>\n<p>怎么才能工作更高效呢？不要什么都干！<br>我们的web应用很大一部分都是静态资源，像图片、js脚本、css文件、预渲染的进入页面等。<br>所以相比较每次都重新请求数据资源，我们不如将一些结果放在简单的缓存之中，谁需要就去拿，不用干扰到后面的服务器。</p>\n<p>缓存的大哥叫内容分发网络CDN，是一组分布在全世界范围内的缓存。这让我们能够从用户就近的节点向他们分发内容，不需要每次都翻山越岭了。</p>\n<h3 id=\"7-消息队列\"><a href=\"#7-消息队列\" class=\"headerlink\" title=\"7. 消息队列\"></a>7. 消息队列</h3><p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g1wedx2oaxj20jg0bdjtb.jpg\" alt></p>\n<p>你去过游乐园吗？你是不是每次都直接去售票厅就能买到票呢？大部分情况都要排队吧应该。<br>政府机构、邮局和游乐园都是”sub-capacity parallelism”很好的例子。他们是并行的–多个售票厅同时售票。<br>但是貌似不可能有足够的售票厅能让每个人都能立即买到票，所以队列就形成了。</p>\n<p>在大型网站上同样可以套用这个概念。成千上万的图片每时每刻上传到Instagram、Facebook上面，每张图片都要被处理、调整、分析和打标，这是一个耗时的过程。<br>所以为了使用户不用等待所有流程都结束，服务器接收到图片之后，只完成下面3件事：</p>\n<ul>\n<li>存储原始图像数据</li>\n<li>告诉用户上传成功了</li>\n<li>向一个大堆增加虚拟标签，说明接下来需要做些什么</li>\n</ul>\n<p>接下来这些标签会被许多的服务器拿到，这些服务器会完成标签上需要完成的任务，确认之后再返回标签，直到所有的任务都完成。<br>系统将这一堆标签叫做”消息队列”，运用这个队列有几大好处：</p>\n<ul>\n<li><p>它将任务和处理器解耦。有时许多图片需要处理，有时很少；有时有很多可以进行处理的节点，有时可用的很少。<br>通过在代办事务中增加任务标签的形式，而不是直接送过去处理，可以保证我们的系统是响应的，且没有任务丢失。</p>\n</li>\n<li><p>它使我们能够按需扩展。因为启动服务器是耗时的，如果已经有很多用户上传了图片需要处理，而图片是直接传给服务器的，那么再启动等多的服务器就已经迟了。相反，如果我们有消息队列，就可以扩展处理能力来处理那些需要处理的任务。</p>\n</li>\n</ul>\n<hr>\n<p>👌如果我们的系统已经经过了以上的扩展，那么已经可以应对大流量了。但是如果我们想要更大更强呢？以下是几个选项。</p>\n<h3 id=\"8-分片-分区\"><a href=\"#8-分片-分区\" class=\"headerlink\" title=\"8. 分片/分区\"></a>8. 分片/分区</h3><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1wee9hj6jj20jg0bf401.jpg\" alt></p>\n<p>什么是分区？定义如下：</p>\n<p>“Sharding是一种通过将应用程序的堆栈分成多个单元来并行化应用程序堆栈的技术，每个单元负责某个键或命名空间”</p>\n<p>额，所以到底什么是分区？其实很简单：需要访问20亿Facebook用户的档案，那就把Facebook分成26个小Facebook，每个Facebook服务不同姓名首字母大写的用户。</p>\n<p>分区不一定按照首字母划分，可以是地区、使用频率（头部用户给更好的服务）等。<br>你可以将服务器分区、数据库分许，或者任何你应用系统中的一个方面，只要满足你的需求。</p>\n<h3 id=\"9-对负载均衡器进行负载均衡\"><a href=\"#9-对负载均衡器进行负载均衡\" class=\"headerlink\" title=\"9. 对负载均衡器进行负载均衡\"></a>9. 对负载均衡器进行负载均衡</h3><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1weelr6h4j20jg0bf403.jpg\" alt></p>\n<p>单个负载均衡器只能做那么多，即使你购买超贵超牛的负载均衡器，性能都是有瓶颈的。</p>\n<p>很幸运的是，有一个全球性的、分散的、超级稳定的层次，能够在流量到达负载均衡器之前进行分流操作。那就是”域名系统”，DNS。全球域名注册表将”github.com”映射到”XXX.XXX.XXX.XXX”这个IP地址上，同时也允许我们将特定域名映射到多个IP地址上，这样就能到达多个不同的负载均衡器了。</p>\n<hr>\n<p>好了，我们做的已经很多了，希望这篇对你有所帮助。但是如果你是IT行业的一员，那么你肯定会问，”到底啥是云服务？”</p>\n<h3 id=\"云计算和ServerLess（无服务器）\"><a href=\"#云计算和ServerLess（无服务器）\" class=\"headerlink\" title=\"云计算和ServerLess（无服务器）\"></a>云计算和ServerLess（无服务器）</h3><p>什么是云服务？它是对于上面提到的所有问题一个便宜又高效的解决方法，简单来说就是，别自己解决他们。</p>\n<p>而是让云服务厂商根据你的系统和需求为你提供扩展服务，你不用考虑任何复杂的问题，不要再造轮子。</p>\n<p>后面略…（介绍下一篇文章的东西）</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>上个星期就准备翻译出这篇文章来着，但是毕设答辩加上清明节假期就导致拖到了今天。</p>\n<p>话不多说，这篇文章是在阮一峰老师的每周记录中看到的。因为之前看了顶天的《大型网络扩展和JAVA中间件》这本书（2/3），所以对这个话题比较熟悉。加上这篇外文有很好看的插图，清晰易懂，没有技术性的阅读障碍，是通过逻辑性的思维进行网站应用扩展的讲解，所以我把它翻译出来，复习使用。</p>\n<ul>\n<li>原文地址: <a href=\"https://arcentry.com/blog/scaling-webapps-for-newbs-and-non-techies/\" target=\"_blank\" rel=\"noopener\">Scaling webapps for newbs &amp; non-techies</a></li>\n</ul>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1web0xb1ej20zk0k0wrd.jpg\" alt></p>\n<p>这篇入门总结了将单服务器扩展到百万级别用户大型网站<strong>扩展</strong>的基本原则。这篇文章针对的是，技术领域的新手或者非开发人员的，所以如果你正在部署更高大上的应用服务的话，这篇文章并不适合你。</p>\n<p>如果这正是你想要的，那么开始吧。</p>\n<h3 id=\"什么是扩展\"><a href=\"#什么是扩展\" class=\"headerlink\" title=\"什么是扩展\"></a>什么是扩展</h3><p>你把你编写的网站，网上商店、社交网络等，部署上线，一切都运行的很美好：每天都有几百用户访问、请求响应及时、调用立即完成。一切都有条不紊地不断运行。</p>\n<p>但是一些”不好”的事情发生了：你大获成功！</p>\n<p>几百、上千、上万的用户每时每秒不断涌入，并且在不断增加。这看起来是商业上最棒的消息了，但同时也在对你的基础设施（服务）说：亲爱的，你大事不妙。因为对于网站的服务来说必须要扩展了，因为它能够：</p>\n<ul>\n<li>同时为更多的用户提供服务</li>\n<li>全天在线，不停机</li>\n<li>服务全世界用户</li>\n</ul>\n<h3 id=\"扩展是怎么工作的\"><a href=\"#扩展是怎么工作的\" class=\"headerlink\" title=\"扩展是怎么工作的\"></a>扩展是怎么工作的</h3><p>以前，这篇文章要以讨论”纵向”扩展（”vertical”）和”横向”（”horizontal”）扩展来开头。总之，纵向扩展指的是在更强大的机器上运行相同的应用，而横向扩展是并行运行多个服务进程。</p>\n<p>现在，已经没有人纵向扩展了，理由如下：</p>\n<ul>\n<li>计算机的价格和性能成指数增加的关系</li>\n<li>单机可以运行很快，但是在纵向上的扩展是有极限的</li>\n<li>多核CPU意味着单机也能有效的并行运算，那不如一开始就并行化好了</li>\n</ul>\n<p>那么横向扩展的必要步骤有那些呢？</p>\n<h3 id=\"1-单服务器-数据库\"><a href=\"#1-单服务器-数据库\" class=\"headerlink\" title=\"1. 单服务器+数据库\"></a>1. 单服务器+数据库</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1webi6rjsj20jg0bumy3.jpg\" alt><br>你的网站后端可能一开始就是这个样子的。一个应用服务器执行你的业务逻辑，数据库存储数据。事情简单而美好，但是想要其高可用，只能用更好的机器–这并不好。</p>\n<h3 id=\"2-增加反向代理\"><a href=\"#2-增加反向代理\" class=\"headerlink\" title=\"2. 增加反向代理\"></a>2. 增加反向代理</h3><p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g1webuwgw7j20jg0al3zf.jpg\" alt></p>\n<p>为了使你的架构能够适应更大的扩展，第一步要加一个”反向代理”。它就相当于酒店的前台。当然，你可以让客人直接去自己的房间；但是，你真正需要的事一个中间人，能够确认这位客人是否允许进入、到底有没有订房或者正去往一个正确房间的路上。同时你还要要告知客户，他要的房间可不可用、能不能去，以免尴尬的徘徊。以上就是反向代理的工作。代理就是用来接收和转发请求的。通常情况下，请求是从我们的服务器发出到互联网上的。但是这次，是从互联网上路由到我们的服务器上，所以我们”反向代理”。</p>\n<p>一个代理需要完成以下任务：</p>\n<ul>\n<li>运行状态检查，确保我们的服务器还在运行</li>\n<li>将请求路由（定向）到正确的终点</li>\n<li>认证，确保请求的发送方是有许可的</li>\n<li>防火墙，确保请求方访问的是服务器上有权限的内容</li>\n</ul>\n<h3 id=\"3-引入负载均衡\"><a href=\"#3-引入负载均衡\" class=\"headerlink\" title=\"3. 引入负载均衡\"></a>3. 引入负载均衡</h3><p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wec96qloj20jg0csabd.jpg\" alt></p>\n<p>许多反向代理还能够负载均衡。负载均衡器是一个很简单的概念：想象一下，在一分钟内有一百个用户要向你的网上商店付款，很不幸的是，你的支付系统只能同时处理50个支付请求。怎么解决？同时运行两个支付系统服务器。</p>\n<p>现在负载均衡器的作用就是将支付请求分摊到这两个服务器上，用户1到左边的服务器，用户2区右边的服务器，用户3去左边的…以此类推。</p>\n<p>如果同时🈶500个支付请求怎么办呢？加到10个支付系统服务器，然后用负载均衡分摊请求。</p>\n<h3 id=\"4-扩展数据库\"><a href=\"#4-扩展数据库\" class=\"headerlink\" title=\"4. 扩展数据库\"></a>4. 扩展数据库</h3><p><img src=\"https://wx2.sinaimg.cn/mw1024/6a49516fly1g1wecqv3woj20jg0bwdh3.jpg\" alt></p>\n<p>运用负载均衡可以将压力分在各个服务器上。但是你发现问题了么？虽然我们可能有成百上千个服务器，用来处理请求，但是只有一个数据库存取数据。</p>\n<p>所以，我们可以以同样的方式来扩展数据库嘛？很不幸的是，不行。关键在于一致性的问题。我们系统中的每个部分都要确保使用数据的一致性。不一致的数据导致各种棘手的问题：订单为重复执行2次，从余额100的账户里扣除2次90元的付款等等。所以我们怎样才能扩展数据库，同时又保证数据的一致性呢?</p>\n<p>首先我们要把数据库分成不同的部分。一部分专门用来接收和存储数据，其他部分专门用来读取数据。这种方案有时称为主/从魔石，有时称为使用只读副本写入。而且假定服务器读数据比写数据的次数更多。<br>这样的解决方法就能保证一致性，因为数据是从单个接口写入的，而且数据流动是单向的，从写到读。<br>缺点就是数据输入还是由单个数据库完成，这对于中小型的应用来说是OK的，但是对于像Facebook那样级别的就不行。更进一步的数据库扩展将在之后讨论。</p>\n<h3 id=\"5-微服务\"><a href=\"#5-微服务\" class=\"headerlink\" title=\"5. 微服务\"></a>5. 微服务</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wed4e3tyj20jg0c3tag.jpg\" alt></p>\n<p>到现在，我们都用一个服务器去完成所有的工作：处理支付、订单、库存、提供web服务、管理用户账号等。</p>\n<p>这未必不是一件坏事，单个服务器意味着更低的复杂度（结构上的），对我们开发者来说没那么头疼。<br>但是随着扩展的增加，事情开始变得复杂和低效：</p>\n<ul>\n<li>不同的服务用在不同的范围。对于用户登录这个流程，可能在很多页面都有，涉及到很多的资源。但是这一些都是在一个服务器上完成的。</li>\n<li>我们的代发团队随着应用而成长。但是越来越多的开发者在一个服务器上工作，不可避免的就会干扰到别人。</li>\n<li>所有服务都在一起，意味着每次发新版都要将所有服务都停掉。这会导致一个很严重的相互依赖关系，一个团队已经做完了，等着发布上线，但是另一个团队可能才完成一半。</li>\n</ul>\n<p>解决这个问题的方法是一个结构范式，它已经掀起了开发界的一大浪潮：微服务。思想很简单，将服务器分成多个功能单元，将他们部署成独立的、互联的迷你服务器。<br>这样做有几大好处：</p>\n<ul>\n<li>每个服务都可以独立扩展，让我们能够机动的适应新需求。</li>\n<li>每个开发团队独立开发，对自己的微服务生命周期负责（创建、部署、更新等）</li>\n<li>每个微服务运用自己的资源，如数据库（一定程度上缓解了4中的问题）</li>\n</ul>\n<h3 id=\"6-缓存和内容分发网络CDN\"><a href=\"#6-缓存和内容分发网络CDN\" class=\"headerlink\" title=\"6. 缓存和内容分发网络CDN\"></a>6. 缓存和内容分发网络CDN</h3><p><img src=\"https://wx1.sinaimg.cn/mw1024/6a49516fly1g1wedhmbk3j20jg0c876a.jpg\" alt></p>\n<p>怎么才能工作更高效呢？不要什么都干！<br>我们的web应用很大一部分都是静态资源，像图片、js脚本、css文件、预渲染的进入页面等。<br>所以相比较每次都重新请求数据资源，我们不如将一些结果放在简单的缓存之中，谁需要就去拿，不用干扰到后面的服务器。</p>\n<p>缓存的大哥叫内容分发网络CDN，是一组分布在全世界范围内的缓存。这让我们能够从用户就近的节点向他们分发内容，不需要每次都翻山越岭了。</p>\n<h3 id=\"7-消息队列\"><a href=\"#7-消息队列\" class=\"headerlink\" title=\"7. 消息队列\"></a>7. 消息队列</h3><p><img src=\"https://wx3.sinaimg.cn/mw1024/6a49516fly1g1wedx2oaxj20jg0bdjtb.jpg\" alt></p>\n<p>你去过游乐园吗？你是不是每次都直接去售票厅就能买到票呢？大部分情况都要排队吧应该。<br>政府机构、邮局和游乐园都是”sub-capacity parallelism”很好的例子。他们是并行的–多个售票厅同时售票。<br>但是貌似不可能有足够的售票厅能让每个人都能立即买到票，所以队列就形成了。</p>\n<p>在大型网站上同样可以套用这个概念。成千上万的图片每时每刻上传到Instagram、Facebook上面，每张图片都要被处理、调整、分析和打标，这是一个耗时的过程。<br>所以为了使用户不用等待所有流程都结束，服务器接收到图片之后，只完成下面3件事：</p>\n<ul>\n<li>存储原始图像数据</li>\n<li>告诉用户上传成功了</li>\n<li>向一个大堆增加虚拟标签，说明接下来需要做些什么</li>\n</ul>\n<p>接下来这些标签会被许多的服务器拿到，这些服务器会完成标签上需要完成的任务，确认之后再返回标签，直到所有的任务都完成。<br>系统将这一堆标签叫做”消息队列”，运用这个队列有几大好处：</p>\n<ul>\n<li><p>它将任务和处理器解耦。有时许多图片需要处理，有时很少；有时有很多可以进行处理的节点，有时可用的很少。<br>通过在代办事务中增加任务标签的形式，而不是直接送过去处理，可以保证我们的系统是响应的，且没有任务丢失。</p>\n</li>\n<li><p>它使我们能够按需扩展。因为启动服务器是耗时的，如果已经有很多用户上传了图片需要处理，而图片是直接传给服务器的，那么再启动等多的服务器就已经迟了。相反，如果我们有消息队列，就可以扩展处理能力来处理那些需要处理的任务。</p>\n</li>\n</ul>\n<hr>\n<p>👌如果我们的系统已经经过了以上的扩展，那么已经可以应对大流量了。但是如果我们想要更大更强呢？以下是几个选项。</p>\n<h3 id=\"8-分片-分区\"><a href=\"#8-分片-分区\" class=\"headerlink\" title=\"8. 分片/分区\"></a>8. 分片/分区</h3><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1wee9hj6jj20jg0bf401.jpg\" alt></p>\n<p>什么是分区？定义如下：</p>\n<p>“Sharding是一种通过将应用程序的堆栈分成多个单元来并行化应用程序堆栈的技术，每个单元负责某个键或命名空间”</p>\n<p>额，所以到底什么是分区？其实很简单：需要访问20亿Facebook用户的档案，那就把Facebook分成26个小Facebook，每个Facebook服务不同姓名首字母大写的用户。</p>\n<p>分区不一定按照首字母划分，可以是地区、使用频率（头部用户给更好的服务）等。<br>你可以将服务器分区、数据库分许，或者任何你应用系统中的一个方面，只要满足你的需求。</p>\n<h3 id=\"9-对负载均衡器进行负载均衡\"><a href=\"#9-对负载均衡器进行负载均衡\" class=\"headerlink\" title=\"9. 对负载均衡器进行负载均衡\"></a>9. 对负载均衡器进行负载均衡</h3><p><img src=\"https://wx4.sinaimg.cn/mw1024/6a49516fly1g1weelr6h4j20jg0bf403.jpg\" alt></p>\n<p>单个负载均衡器只能做那么多，即使你购买超贵超牛的负载均衡器，性能都是有瓶颈的。</p>\n<p>很幸运的是，有一个全球性的、分散的、超级稳定的层次，能够在流量到达负载均衡器之前进行分流操作。那就是”域名系统”，DNS。全球域名注册表将”github.com”映射到”XXX.XXX.XXX.XXX”这个IP地址上，同时也允许我们将特定域名映射到多个IP地址上，这样就能到达多个不同的负载均衡器了。</p>\n<hr>\n<p>好了，我们做的已经很多了，希望这篇对你有所帮助。但是如果你是IT行业的一员，那么你肯定会问，”到底啥是云服务？”</p>\n<h3 id=\"云计算和ServerLess（无服务器）\"><a href=\"#云计算和ServerLess（无服务器）\" class=\"headerlink\" title=\"云计算和ServerLess（无服务器）\"></a>云计算和ServerLess（无服务器）</h3><p>什么是云服务？它是对于上面提到的所有问题一个便宜又高效的解决方法，简单来说就是，别自己解决他们。</p>\n<p>而是让云服务厂商根据你的系统和需求为你提供扩展服务，你不用考虑任何复杂的问题，不要再造轮子。</p>\n<p>后面略…（介绍下一篇文章的东西）</p>\n"},{"title":"LeetCode刷题总结（持续更新）","date":"2019-03-14T16:00:00.000Z","updated":"2019-03-16T16:00:00.000Z","_content":"我的答案仓库地址：[MyLeetCode](https://github.com/fangmiao97/MyLeetCode)\n## Tree与迭代、动态规划\n* [Maximum Binary Tree](https://leetcode.com/problems/maximum-binary-tree/)\n    * 思路：将创建最大二叉树，即根节点比所有叶子节点都大，的过程，分解成先寻找到当前数列中最大值，然后在创建左右最大子树的过程\n    * 退出情况是数列为一个数时返回null，即表示结束没有子树可以构造了\n\n```java\nTreeNode root = new TreeNode(nums[max_i]);\nroot.left = construct(nums, l, max_i);\nroot.right = construct(nums, max_i + 1, r);\n```\n* [Maximum Binary Tree II](https://leetcode.com/problems/maximum-binary-tree-ii/)\n    * 描述：在一棵现成的最大二叉树中，插入一个数，使得插入之后依然是最大二叉树\n    * 技巧点：比根节点小的数一律往根节点的右子树插。\n    * 思路：插入值与当前根节点的值比较，若大于根节点的值则创建节点，并将原根节点作为自己的左子结点，并返回新的根节点。否则的话插入值插入原根节点的右子树，并循环这个过程。若插入值比较到最后，即与null比较，则创建该节点并返回这个节点。\n\n```java\npublic TreeNode insertIntoMaxTree(TreeNode root, int val) {\n        if(root == null){\n            return new TreeNode(val);\n        }\n        if(root.val<val){\n            TreeNode head = new TreeNode(val);\n            head.left = root;\n            return head;\n        }\n        root.right = insertIntoMaxTree(root.right, val);\n        return root;\n    }\n```\n* [Second Minimum Node In a Binary Tree](https://leetcode.com/problems/second-minimum-node-in-a-binary-tree/)\n    * 需要注意的事，必须将所有的节点都遍历完全才能找到第二小的数字。因为有一个用例将第二小的数值藏在了最下面\n    * 采用DFS或BFS都可以。关键是判断第二小的时候，先要将第一小的数找到，如果之后有数字不是小于**等于**第一小的话，才可以比较是不是第二小。如果只是单纯的小于第一小，会让第二小也成为和第一小一样的数值。\n* BFS常用结构\n\n```java\n        Queue<TreeNode> a = new LinkedList<>();\n        a.offer(p);\n        while (!a.isEmpty()){\n            for (int sz = a.size(); sz > 0; --sz) {\n                TreeNode n = a.poll();\n                //do something\n                a.offer(n.left);\n                a.offer(n.right);\n            }\n```\n* [Sum of Root To Leaf Binary Numbers](https://leetcode.com/problems/sum-of-root-to-leaf-binary-numbers/)\n    * DFS\n    \n```java\n    public int sumRootToLeaf(TreeNode root) {\n        return dfs(root, 0);\n    }\n    public static int dfs(TreeNode root, int sum) {        \n        if(root == null) return 0;\n        sum = sum * 2 + root.val;\n        return root.left == root.right ? sum : dfs(root.left, sum) + dfs(root.right, sum);\n        \n    }\n    \n```\n## 字节/位处理\n* [Reverse Integer](https://leetcode.com/problems/reverse-integer/)\n    * java中不同数据类型的取值范围\n        * int 32\n        * short 16\n        * long 64\n        * float 32\n        * double 64\n    * 本题中关于溢出可能的判断\n   \n```java\n//方式一\n//正数溢出情况（2147483647）\nif (rev > Integer.MAX_VALUE/10 || (rev == Integer.MAX_VALUE / 10 && pop > 7)) return 0;\n//负数（-2147483648）\nif (rev < Integer.MIN_VALUE/10 || (rev == Integer.MIN_VALUE / 10 && pop < -8)) return 0;\n\n\n//方式二\nif((rev - pop) / 10 != org) return 0;//溢出的话肯定计算不出原来的数字了\n```\n\n* [Number of 1 Bits](https://leetcode.com/problems/number-of-1-bits/)\n    * 找一个int数的二进制中有多少个1，hamming weight\n    * & -- 位与运算 | -- 位或运算\n\n```java\nint res = 0;\n        for(int i = 31; i >= 0; i--) {\n            if((n & 1) == 1)\n                res++;\n            n >>= 1; //右移一位\n        }\n```\n* [java中的移位操作符](https://zhuanlan.zhihu.com/p/30108890)\n\n* [Reverse Bits](https://leetcode.com/problems/reverse-bits/)\n    * 翻转32位比特值，依次移动每一位。将每次需要移动的比特放在最后一位。与1进行位与操作后，将这一位移动到正确的位置后，与结果进行位或操作。\n    \n```java\nint ans = 0;\n        for(int i = 31; i >= 0; i--) {\n            ans = ans | ((n & 1) << i);\n            n >>= 1;\n        }\n```\n\n* [Counting Bits](https://leetcode.com/problems/counting-bits/)\n    * **DP** When it comes to even numbers, i.e, 2,4,6,8, their binary should be like '10', '100', '110', '1000' \n      so one notable difference is their unit digit is always 0, \n      which means when you call >> 1- shift one bit rightwards \n      and also means dividing by 2- would cause no change to the count of '1' in the binary string.\n      \n      Vice versa, when you meet odd numbers, shifting one bit rightwards always eliminates one '1' digit from original binary string,\n      that is why we should \"compensate\" one '1' character to the count.\n      \n      To sum up, when you meet even number the count of '1's is always the same as its half number,\n      on the other hand, remember to plus one to the odds' half number.\n   \n```java\n    int[] f = new int[num + 1];\n    for (int i=1; i<=num; i++) f[i] = f[i >> 1] + (i & 1);\n    return f;\n```\n\n## Two Pointers\n\n* [浅析经典面试算法题-two pointer的运用](https://chocoluffy.com/2016/12/04/浅析经典面试算法题-two-pointer的运用/)\n\n* Two Sum\n    * integer array已经过排序\n    * 两个pointers一头一尾。那么sum只有三种可能：\n        * sum == target，则返回\n        * sum < target，头指针向后走一个\n        * sum > target，尾指针向前走一个\n    * 循环条件，头 < 尾\n    \n```java\nint low = 0;\nint high = nums.length - 1;\nwhile(low < high) {\n    if(nums[low] + nums[high] == target)\n        //do something;\n        // low++ high--;\n    else if (nums[low] + nums[high] < target)\n        low++;\n    else\n        high--;\n} \n```\n\n* 3Sum\n    * 先将数列排序，再固定第一个数字，从剩下的数列中用2 sum的方法找。\n    * 注意一些要过滤的条件：\n        * 第一个数字在移动的过程中，如果与前一个一样的话，就再移一下\n        * low和high移动的道理也一样\n \n```java\n public List<List<Integer>> threeSum(int[] nums) {\n        Arrays.sort(nums);\n        List<List<Integer>> res = new ArrayList<>()\n        for(int i = 0; i < nums.length - 2; i++) {\n            if(i == 0 || nums[i] != nums[i - 1]) {\n                int low = i + 1;\n                int high = nums.length - 1;\n                int remain = 0 - nums[i];\n                while(low < high) {\n                    if(nums[low] + nums[high] == remain) {\n                        res.add(Arrays.asList(nums[i], nums[low], nums[high]));\n                        while(low < high && nums[low + 1] == nums[low])\n                            low++;\n                        while(low < high && nums[high - 1] == nums[high])\n                            high--;\n                        low++;\n                        high--;\n                    }else if(nums[low] + nums[high] < remain)\n                        low++;\n                    else\n                        high--;\n                }\n            }\n        }\n        return res;\n    }\n```\n\n* 3Sum closest\n    * 找最接近的sum（也可能相等）\n    * 增加判断条件Math.abs小的话，就要更新。\n\n* 3Sum With Multiplicity\n    * 每一中情况都要考虑到--排列组合\n\n```java\n  if (A[j] == A[k]) {\n    // If A[j...k] are all equal, then there are C(k - j + 1, 2) \n    // combinations that meet the requirement.取两个\n    res = (res + (k - j + 1) * (k - j) / 2) % m;\n    break;\n    }\n    int l = 1, r = 1;\n    while (j + l < k && A[j + l] == A[j]) { ++l; } // l: number of elements equal to A[j].\n    while (j < k - r && A[k - r] == A[k]) { ++r; } // r: number of elements equal to A[k].\n    res = (res + l * r) % m; // found l * r cases that meet the requirement.\n    j += l; // forward j by l steps.\n    k -= r; // backward k by r steps.\n```\n\n* Sum of Square Numbers\n    * c = a^2 + b^2\n    \n```java\npublic boolean judgeSquareSum(int c) {\n        int limit = (int)Math.sqrt(c);\n        int low = 0;\n        while(low<=limit){\n            int sum = low*low + limit*limit;\n            if(c==sum){\n                return true;\n            }\n            if(sum<c){\n               low++;\n            } else {\n               limit--; \n            }\n        }\n        return false;\n    }\n```\n## 基础字符串操作\n* reverse\n```java\n public String reverse(String s) {\n      StringBuilder res=new StringBuilder();\n        for (int i = 0; i < s.length(); i++)\n            res.insert(0,s.charAt(i));\n        return res.toString();\n    }\n```\n\n* split 对应API public String[] split(String regex, int limit)\n\n```java\npublic String[] split(String s) {\n        ArrayList < String > words = new ArrayList < > ();\n        StringBuilder word = new StringBuilder();\n        for (int i = 0; i < s.length(); i++) {\n            if (s.charAt(i) == ' ') {\n                words.add(word.toString());\n                word = new StringBuilder();\n            } else\n                word.append( s.charAt(i));\n        }\n        words.add(word.toString());\n        return words.toArray(new String[words.size()]);\n    }\n```\n* 大小写转换\n    * a - 97 0x61(0110 0001) A - 65 0x41(0100 0001)\n    \n```java\n    //法一\n    string.toLowerCase() or toUpperCase()\n    //法二\n    if ('A' <= a[i] && a[i] <= 'Z')\n        a[i] = (char) (a[i] - 'A' + 'a');\n    //法三，按位或，把第五位的1加上\n    char c = (char)(str.charAt(i) | (char)(32));\n```\n\n* 相同前缀\n    * indexOf（int，ch）：先看第一个indexOf它返回值是int，在看它的参数（int，ch）意思就是使用者可以给参数一个‘char’字符所代表的int值，然后去从前向后找到该字符在字符串中第一次出现处的索引，当然了我们不可能记得住每一个char的值所以我们在使用时直接用String s=abcdef;　int i=s.indexOf('d')\n    这种方式就可以了，char类型会自动提升为int类型，还有就是要注意如果返回值为-1，就说明索引越界了；\n    * indexOf（int ch，int，fromIndex）：这个方法就是说从指定位置往后找返回字符在该字符串中第一次出现处的索引，比如“woaizhongguo”indexOf（'o',2）那返回值就是6而不是1，也不是11；\n    * indexOf（Sting str）：这个方法基本就类似前面的了，只不过它是在参数里给一个子字符串，然后返回该子字符串在该字符串中第一次出现处的索引，比如\"woaixuexi\"要查\"ai\"这个子字符串在整个字符串中出现的索引位置那返回值就是2\n    * indexOf（String str，int fromIndex）这个方法不在累述\n    * lastIndexOf（int ch）：这个方法也是跟indexof相反，它是从后往前找返回字符在字符串中最后一次出现处的索引，也就是说找索引的时候是倒着找的但是返回值还是按照正的索引顺序返回的比如\"woaiwo\"用lastindexof查找‘w’返回的值是4而不是1\n    * lastIndexOf(int ch,fromindex)\n\n```java\n    public String longestCommonPrefix(String[] strs) {\n        if(strs.length == 0)\n            return \"\";\n        String prefix = strs[0];\n        for(int i = 0; i < strs.length; i++) {\n            while(strs[i].indexOf(prefix) != 0) {//前缀肯定是最短的，所以如果有这个相同前缀的话，肯定在每个string里面都有\n                prefix = prefix.substring(0, prefix.length() - 1);//前缀就从第一个字符串进行截取就可以了\n                if(prefix.isEmpty())\n                    return \"\";\n            }\n        }\n        return prefix;\n    }\n```\n## 非常规顺序操作\n\n* [ZigZag Conversion](https://leetcode.com/problems/zigzag-conversion/)\n    * 取相同数字位的进行操作：0123210123210..\n    * 法一：能够发现0之后都是加一，3之后都是减一\n    * 设置一个标志来判断是否加一还是减一\n    * 法二：变步长\n    \n```java\n    //1\n    if(curRow == 0 || curRow == numRows - 1) goingDown = !goingDown;\n    curRow += goingDown ? 1 : -1;\n    //2\n            int n = s.length();\n            int cycleLen = 2 * numRows - 2; //numRows = 4\n    \n            for (int i = 0; i < numRows; i++) {\n                for (int j = 0; j + i < n; j += cycleLen) {\n                    ret.append(s.charAt(j + i));\n                    if (i != 0 && i != numRows - 1 && j + cycleLen - i < n)\n                        ret.append(s.charAt(j + cycleLen - i));\n                }\n            }\n```\n\n## 数字计算\n\n* pow(x, n)\n    * n%2==0 -> x^n = x^(n/2) * x^(n/2) = (x*x)^(n/2)\n    * n%2==1 -> x^n = x*(x^(n/2) * x^(n/2)) = x * (x*x)^(n/2)\n\n```java\npublic double pow(double x, int n) {\n            if(n == 0)\n                return 1;\n            if(n == Integer.MIN_VALUE){//-2147483648不能直接换成正的，会溢出\n                return myPow(x*x, n/2);\n            }\n            if(n < 0){\n                x = 1/x;\n                n = -n;\n            }\n            if(n%2 == 1) \n                return myPow(x*x, n/2)*x;\n            else\n                return myPow(x*x, n/2);\n                }\n```\n\n* sqrt(x)\n    * I have seen many variants using Binary Search, the key difference is the search range. It seems easy to do it but actually there are some traps we need to take care. I made this just for a note for me.\n      Search range summary:\n      \n      * [1, Integer.MAX_VALUE](easy but not recommend)\n      * [1, x](recommended)\n      * [1, x/2](you need to do math to prove it)\n    * For case 2 and case 3, we need to take care of the corner case by making sure right >= left for [left, right], so:\n      2. x >= 1 for [1, x] => so we need to take care of the corner case: x < 1\n      3. x/2 >= 1 for [1, x/2]=> x >= 2 => so we need to take care of the corner case: x < 2\n   \n      \n```java\nclass Solution {\n    public int mySqrt(int x) {\n        long l=0,r=x; //in case of overflow\n        while(l<r){\n            long mid=l+(r-l)/2+1;\n            if(mid*mid>x) r=mid-1;\n            else l=mid;\n        }\n        return (int)l;\n    }\n}\n```\n\n* 计算加减式\n    * [basic-calculator](https://leetcode.com/problems/basic-calculator/)\n  \n```java\npublic int calculate(String s) {\n        Stack<Integer> stack = new Stack<Integer>();\n        int result = 0;\n        int number = 0;\n        int sign = 1;\n        for(int i = 0; i < s.length(); i++) {\n            char c = s.charAt(i);\n            if(Character.isDigit(c)) {\n                number = 10 * number + (int)(c  - '0');\n            }\n            else if(c == '+') {\n                number = sign * number;\n                result += number;\n                number = 0;\n                sign = 1;\n            } else if(c == '-') {\n                number = sign * number;\n                result += number;\n                number = 0;\n                sign = -1;\n            } else if( c == '(') {\n                stack.push(result);\n                stack.push(sign);\n                result = 0;\n                sign = 1;\n            } else if( c == ')') {\n                number = sign * number;\n                result += number;\n                number = 0; \n                result *= stack.pop();\n                result += stack.pop();\n            }\n        }\n        //最后以数字结尾的话\n        if(number != 0) return result += sign * number;\n        return result;\n    }\n```\n\n* 加减乘除\n\n```java\npublic int calculate(String s) {\n        Stack<Integer> stack = new Stack<Integer>();\n        int result = 0;\n        int number = 0;\n        char sign = '+';\n        for(int i = 0; i < s.length(); i++ ) {\n            \n            char c = s.charAt(i);\n            if(Character.isDigit(c)) {\n                number = number * 10 + (int)(c - '0');\n            }\n            if((!Character.isDigit(c) && c != ' ') || i == s.length()-1) {\n                if( sign == '+') {\n                    stack.push(number);\n                }\n                if(sign == '-') {\n                    stack.push(-number);\n                }\n                if(sign == '*') {\n                    stack.push(stack.pop() * number);\n                }\n                if(sign == '/') {\n                    stack.push(stack.pop() / number);\n                }\n                \n                sign = c;\n                number = 0;\n            }\n        }\n        \n        for(int i : stack) {\n            result += i;\n        }\n        \n        return result;\n    }\n```\n\n* [Missing Num](https://leetcode.com/problems/missing-number/)\n    * 在n长的数组中，包含0到n这n+1个数字中的n个。有个一数字[0, n]不在里面。\n    * 将数组的索引[0, n-1]想成抽屉的编号，如果是n缺少的话，每个抽屉其实都能装到自己的数字，n就单独出来了。\n    * 如果是[0, n-1]中的数字缺少了，说明n这个数字占了其中一个抽屉，那个数字（索引编号还贴在抽屉上）就单出来了。\n    * 使用异或操作可以将单独的数字找出来。\n\n```java\n     public int missingNumber(int[] nums) {   \n            \n            int missing = nums.length;\n            for(int i = 0; i < nums.length; i++) {\n                missing ^= i ^ nums[i];\n            }\n            \n            return missing;\n        }\n```\n\n## Int处理\n\n* Palindrome Number 回文数\n    * 转换成String 使用reverse()判断是否相同\n    * [reverse Integer](https://leetcode.com/problems/reverse-integer/)，再看是否相同，但是可能会溢出\n    * 只转换后一半的数字，与前面一半的数字比较：1221 --> 12 12 12321 --> 12 123/10\n\n```java\npublic boolean isPalindrome(int x) {\n    if(x < 0 || (x % 10 == 0 && x != 0))\n        return false;\n    \n    int rev = 0;\n    while(x > rev) { //当前面的数字小于等于后面翻转的数字时，就到中间了\n        rev = rev * 10 + x % 10;\n        x /= 10;\n    }\n    \n    return (x == rev || x == rev / 10);\n}\n```\n\n## LinkedList\n\n* Remove Linked List Elements\n\n```java\npublic ListNode removeElements(ListNode head, int val) {\n        ListNode dummy = new ListNode(0);\n        dummy.next = head;\n        head = dummy;\n        while(head.next != null) {\n            if(head.next.val == val)\n                head.next = head.next.next;\n            else\n                head = head.next;\n        }\n    }\n```\n\n* middle ListNode\n\n```java\npublic ListNode middleNode(ListNode head) {\n    ListNode slow = head, fast = head;\n    while(fast != null && fast.next != null) {\n        slow = slow.next;\n        fast = fast.next.next;\n    }\n    return slow;\n}\n```\n\n* reverse List\n    * 将链表翻转\n\n```java\npublic ListNode reverse(ListNode head) {\n    ListNode pre = null;\n    while(head != null) {\n        ListNode next = head.next;\n        head.next = pre;\n        pre = head;\n        head = next;\n    }\n    return pre;\n}\n```\n\n* 翻转部分List\n    * [reverse list II](https://leetcode.com/problems/reverse-linked-list-ii/)\n\n```java\npublic ListNode reverseBetween(ListNode head, int m, int n) {\n        \n        if(head == null || n == 1) return head;\n        \n        ListNode dummy = new ListNode(0);\n        dummy.next = head;\n        ListNode p = dummy;\n        while(m > 1) {\n            p = p.next;\n            m--;\n            n--;\n        }\n        ListNode tail = p.next;\n        \n        ListNode tmp = p;\n        while(n > 1) {\n            tmp = p.next;\n            p.next = tail.next;\n            tail.next = tail.next.next;\n            p.next.next = tmp;\n            n--;\n        }\n         \n        return dummy.next;\n    }\n```\n\n* 有环的LinkedList\n    * [Linked List CycleII](https://leetcode.com/problems/linked-list-cycle-ii/)\n    * two pointers看解释\n    * 延伸[287. Find the Duplicate Number](https://leetcode.com/problems/find-the-duplicate-number/)\n    * 将数组当成链表的索引表\n    * 鸽笼原理，一个数组[n+1]中有1-n个不相同的数，肯定有一个数字是重复的。如果把数组中的每个数当做是索引值的话，没有重复数字的情况下，就会形成一个没有环的链。如果中间有重复值的话，就一定会出现环。\n\n```java\n    public int findDuplicate(int[] nums) {\n            int slow = nums[0];\n            int fast = nums[0];\n            \n            do{\n                slow = nums[slow];\n                fast = nums[nums[fast]];\n            }while(slow != fast);\n            \n            int head = nums[0];\n            while(head != slow) {\n                head = nums[head];\n                slow = nums[slow];\n            }\n            \n            return head;\n        }\n```\n\n* [二进制加](https://leetcode.com/problems/add-binary/)\n```java\n    public String addBinary(String a, String b) {\n            StringBuilder sb = new StringBuilder();\n            int i = a.length(), j = b.lengthg(), carry = 0;\n            \n            while(i >= 0 || j >= 0) {\n                int sum = carry;\n                if(j >= 0) sum += b.charAt(j--) - '0';\n                if(i >= 0) sum += a.charAt(i--) - '0'；\n                sb.insert(0, sum % 2);\n                carry = sum / 2;\n            }\n            \n            if(carry ！= 0)\n                sb.insert(0, carry);\n            return sb.toString();\n        }\n```\n    \n## 指针\n\n* 将数组中的数当成指针\n    * [442. Find All Duplicates in an Array](https://leetcode.com/problems/find-all-duplicates-in-an-array)\n\n```java\npublic List<Integer> findDuplicates(int[] nums) {\n        List<Integer> res = new ArrayList<>();\n        \n        for (int i = 0; i < nums.length; ++i) {\n            int index = Math.abs(nums[i])-1;//要去绝对值 索引\n            if (nums[index] < 0)//是负的说明，之前遇到过\n                res.add(Math.abs(index+1));\n            nums[index] = -nums[index];\n        }\n        \n        return res;\n    }\n```\n   * 448. Find All Numbers Disappeared in an Array\n\n```java\n    public List<Integer> findDisappearedNumbers(int[] nums) {\n        \n        List<Integer> res = new ArrayList<>();\n        for(int i = 0; i < nums.length; i++) {\n            int val = Math.abs(nums[i]) - 1;\n            if(nums[val] > 0) {\n                nums[val] = - nums[val];\n            }\n        }\n        for(int i = 0; i < nums.length; i++) {\n            if(nums[i] > 0 ) {\n                res.add(i+1);\n            }\n        }\n        return res;\n    }\n```\n## 表达式校验\n\n* [Valid Parentheses](https://leetcode.com/problems/valid-parentheses/)\n    * 常规做法：用stack保存左括号，遇到右括号检验栈顶是不是对应的左括号，是的话就pop，不是就不正确\n    * 非常规，思想相同：\n\n```java\n    public boolean isValid(String s) {\n            Stack<Character> stack = new Stack<Character>();\n    \tfor (char c : s.toCharArray()) {\n    \t\tif (c == '(')\n    \t\t\tstack.push(')');\n    \t\telse if (c == '{')\n    \t\t\tstack.push('}');\n    \t\telse if (c == '[')\n    \t\t\tstack.push(']');\n    \t\telse if (stack.isEmpty() || stack.pop() != c)\n    \t\t\treturn false;\n    \t}\n    \treturn stack.isEmpty();\n```\n\n## 三分查找\n* [guess number](https://leetcode.com/problems/guess-number-higher-or-lower/solution/)\n\n```java\n    public int guessNumber(int n) {\n            int low = 1;\n            int high = n;\n            while (low <= high) {\n                int mid1 = low + (high - low) / 3;\n                int mid2 = high - (high - low) / 3;\n                int res1 = guess(mid1);\n                int res2 = guess(mid2);\n                if (res1 == 0)\n                    return mid1;\n                if (res2 == 0)\n                    return mid2;\n                else if (res1 < 0)\n                    high = mid1 - 1;\n                else if (res2 > 0)\n                    low = mid2 + 1;\n                else {\n                    low = mid1 + 1;\n                    high = mid2 - 1;\n                }\n            }\n            return -1;\n        }\n```","source":"_posts/MyLeetCodeSummarize.md","raw":"---\ntitle: LeetCode刷题总结（持续更新）\ndate: 2019/3/15\nupdated: 2019/3/17 \ntags:\n    - LeetCode\ncategories:\n    - 技术总结\n---\n我的答案仓库地址：[MyLeetCode](https://github.com/fangmiao97/MyLeetCode)\n## Tree与迭代、动态规划\n* [Maximum Binary Tree](https://leetcode.com/problems/maximum-binary-tree/)\n    * 思路：将创建最大二叉树，即根节点比所有叶子节点都大，的过程，分解成先寻找到当前数列中最大值，然后在创建左右最大子树的过程\n    * 退出情况是数列为一个数时返回null，即表示结束没有子树可以构造了\n\n```java\nTreeNode root = new TreeNode(nums[max_i]);\nroot.left = construct(nums, l, max_i);\nroot.right = construct(nums, max_i + 1, r);\n```\n* [Maximum Binary Tree II](https://leetcode.com/problems/maximum-binary-tree-ii/)\n    * 描述：在一棵现成的最大二叉树中，插入一个数，使得插入之后依然是最大二叉树\n    * 技巧点：比根节点小的数一律往根节点的右子树插。\n    * 思路：插入值与当前根节点的值比较，若大于根节点的值则创建节点，并将原根节点作为自己的左子结点，并返回新的根节点。否则的话插入值插入原根节点的右子树，并循环这个过程。若插入值比较到最后，即与null比较，则创建该节点并返回这个节点。\n\n```java\npublic TreeNode insertIntoMaxTree(TreeNode root, int val) {\n        if(root == null){\n            return new TreeNode(val);\n        }\n        if(root.val<val){\n            TreeNode head = new TreeNode(val);\n            head.left = root;\n            return head;\n        }\n        root.right = insertIntoMaxTree(root.right, val);\n        return root;\n    }\n```\n* [Second Minimum Node In a Binary Tree](https://leetcode.com/problems/second-minimum-node-in-a-binary-tree/)\n    * 需要注意的事，必须将所有的节点都遍历完全才能找到第二小的数字。因为有一个用例将第二小的数值藏在了最下面\n    * 采用DFS或BFS都可以。关键是判断第二小的时候，先要将第一小的数找到，如果之后有数字不是小于**等于**第一小的话，才可以比较是不是第二小。如果只是单纯的小于第一小，会让第二小也成为和第一小一样的数值。\n* BFS常用结构\n\n```java\n        Queue<TreeNode> a = new LinkedList<>();\n        a.offer(p);\n        while (!a.isEmpty()){\n            for (int sz = a.size(); sz > 0; --sz) {\n                TreeNode n = a.poll();\n                //do something\n                a.offer(n.left);\n                a.offer(n.right);\n            }\n```\n* [Sum of Root To Leaf Binary Numbers](https://leetcode.com/problems/sum-of-root-to-leaf-binary-numbers/)\n    * DFS\n    \n```java\n    public int sumRootToLeaf(TreeNode root) {\n        return dfs(root, 0);\n    }\n    public static int dfs(TreeNode root, int sum) {        \n        if(root == null) return 0;\n        sum = sum * 2 + root.val;\n        return root.left == root.right ? sum : dfs(root.left, sum) + dfs(root.right, sum);\n        \n    }\n    \n```\n## 字节/位处理\n* [Reverse Integer](https://leetcode.com/problems/reverse-integer/)\n    * java中不同数据类型的取值范围\n        * int 32\n        * short 16\n        * long 64\n        * float 32\n        * double 64\n    * 本题中关于溢出可能的判断\n   \n```java\n//方式一\n//正数溢出情况（2147483647）\nif (rev > Integer.MAX_VALUE/10 || (rev == Integer.MAX_VALUE / 10 && pop > 7)) return 0;\n//负数（-2147483648）\nif (rev < Integer.MIN_VALUE/10 || (rev == Integer.MIN_VALUE / 10 && pop < -8)) return 0;\n\n\n//方式二\nif((rev - pop) / 10 != org) return 0;//溢出的话肯定计算不出原来的数字了\n```\n\n* [Number of 1 Bits](https://leetcode.com/problems/number-of-1-bits/)\n    * 找一个int数的二进制中有多少个1，hamming weight\n    * & -- 位与运算 | -- 位或运算\n\n```java\nint res = 0;\n        for(int i = 31; i >= 0; i--) {\n            if((n & 1) == 1)\n                res++;\n            n >>= 1; //右移一位\n        }\n```\n* [java中的移位操作符](https://zhuanlan.zhihu.com/p/30108890)\n\n* [Reverse Bits](https://leetcode.com/problems/reverse-bits/)\n    * 翻转32位比特值，依次移动每一位。将每次需要移动的比特放在最后一位。与1进行位与操作后，将这一位移动到正确的位置后，与结果进行位或操作。\n    \n```java\nint ans = 0;\n        for(int i = 31; i >= 0; i--) {\n            ans = ans | ((n & 1) << i);\n            n >>= 1;\n        }\n```\n\n* [Counting Bits](https://leetcode.com/problems/counting-bits/)\n    * **DP** When it comes to even numbers, i.e, 2,4,6,8, their binary should be like '10', '100', '110', '1000' \n      so one notable difference is their unit digit is always 0, \n      which means when you call >> 1- shift one bit rightwards \n      and also means dividing by 2- would cause no change to the count of '1' in the binary string.\n      \n      Vice versa, when you meet odd numbers, shifting one bit rightwards always eliminates one '1' digit from original binary string,\n      that is why we should \"compensate\" one '1' character to the count.\n      \n      To sum up, when you meet even number the count of '1's is always the same as its half number,\n      on the other hand, remember to plus one to the odds' half number.\n   \n```java\n    int[] f = new int[num + 1];\n    for (int i=1; i<=num; i++) f[i] = f[i >> 1] + (i & 1);\n    return f;\n```\n\n## Two Pointers\n\n* [浅析经典面试算法题-two pointer的运用](https://chocoluffy.com/2016/12/04/浅析经典面试算法题-two-pointer的运用/)\n\n* Two Sum\n    * integer array已经过排序\n    * 两个pointers一头一尾。那么sum只有三种可能：\n        * sum == target，则返回\n        * sum < target，头指针向后走一个\n        * sum > target，尾指针向前走一个\n    * 循环条件，头 < 尾\n    \n```java\nint low = 0;\nint high = nums.length - 1;\nwhile(low < high) {\n    if(nums[low] + nums[high] == target)\n        //do something;\n        // low++ high--;\n    else if (nums[low] + nums[high] < target)\n        low++;\n    else\n        high--;\n} \n```\n\n* 3Sum\n    * 先将数列排序，再固定第一个数字，从剩下的数列中用2 sum的方法找。\n    * 注意一些要过滤的条件：\n        * 第一个数字在移动的过程中，如果与前一个一样的话，就再移一下\n        * low和high移动的道理也一样\n \n```java\n public List<List<Integer>> threeSum(int[] nums) {\n        Arrays.sort(nums);\n        List<List<Integer>> res = new ArrayList<>()\n        for(int i = 0; i < nums.length - 2; i++) {\n            if(i == 0 || nums[i] != nums[i - 1]) {\n                int low = i + 1;\n                int high = nums.length - 1;\n                int remain = 0 - nums[i];\n                while(low < high) {\n                    if(nums[low] + nums[high] == remain) {\n                        res.add(Arrays.asList(nums[i], nums[low], nums[high]));\n                        while(low < high && nums[low + 1] == nums[low])\n                            low++;\n                        while(low < high && nums[high - 1] == nums[high])\n                            high--;\n                        low++;\n                        high--;\n                    }else if(nums[low] + nums[high] < remain)\n                        low++;\n                    else\n                        high--;\n                }\n            }\n        }\n        return res;\n    }\n```\n\n* 3Sum closest\n    * 找最接近的sum（也可能相等）\n    * 增加判断条件Math.abs小的话，就要更新。\n\n* 3Sum With Multiplicity\n    * 每一中情况都要考虑到--排列组合\n\n```java\n  if (A[j] == A[k]) {\n    // If A[j...k] are all equal, then there are C(k - j + 1, 2) \n    // combinations that meet the requirement.取两个\n    res = (res + (k - j + 1) * (k - j) / 2) % m;\n    break;\n    }\n    int l = 1, r = 1;\n    while (j + l < k && A[j + l] == A[j]) { ++l; } // l: number of elements equal to A[j].\n    while (j < k - r && A[k - r] == A[k]) { ++r; } // r: number of elements equal to A[k].\n    res = (res + l * r) % m; // found l * r cases that meet the requirement.\n    j += l; // forward j by l steps.\n    k -= r; // backward k by r steps.\n```\n\n* Sum of Square Numbers\n    * c = a^2 + b^2\n    \n```java\npublic boolean judgeSquareSum(int c) {\n        int limit = (int)Math.sqrt(c);\n        int low = 0;\n        while(low<=limit){\n            int sum = low*low + limit*limit;\n            if(c==sum){\n                return true;\n            }\n            if(sum<c){\n               low++;\n            } else {\n               limit--; \n            }\n        }\n        return false;\n    }\n```\n## 基础字符串操作\n* reverse\n```java\n public String reverse(String s) {\n      StringBuilder res=new StringBuilder();\n        for (int i = 0; i < s.length(); i++)\n            res.insert(0,s.charAt(i));\n        return res.toString();\n    }\n```\n\n* split 对应API public String[] split(String regex, int limit)\n\n```java\npublic String[] split(String s) {\n        ArrayList < String > words = new ArrayList < > ();\n        StringBuilder word = new StringBuilder();\n        for (int i = 0; i < s.length(); i++) {\n            if (s.charAt(i) == ' ') {\n                words.add(word.toString());\n                word = new StringBuilder();\n            } else\n                word.append( s.charAt(i));\n        }\n        words.add(word.toString());\n        return words.toArray(new String[words.size()]);\n    }\n```\n* 大小写转换\n    * a - 97 0x61(0110 0001) A - 65 0x41(0100 0001)\n    \n```java\n    //法一\n    string.toLowerCase() or toUpperCase()\n    //法二\n    if ('A' <= a[i] && a[i] <= 'Z')\n        a[i] = (char) (a[i] - 'A' + 'a');\n    //法三，按位或，把第五位的1加上\n    char c = (char)(str.charAt(i) | (char)(32));\n```\n\n* 相同前缀\n    * indexOf（int，ch）：先看第一个indexOf它返回值是int，在看它的参数（int，ch）意思就是使用者可以给参数一个‘char’字符所代表的int值，然后去从前向后找到该字符在字符串中第一次出现处的索引，当然了我们不可能记得住每一个char的值所以我们在使用时直接用String s=abcdef;　int i=s.indexOf('d')\n    这种方式就可以了，char类型会自动提升为int类型，还有就是要注意如果返回值为-1，就说明索引越界了；\n    * indexOf（int ch，int，fromIndex）：这个方法就是说从指定位置往后找返回字符在该字符串中第一次出现处的索引，比如“woaizhongguo”indexOf（'o',2）那返回值就是6而不是1，也不是11；\n    * indexOf（Sting str）：这个方法基本就类似前面的了，只不过它是在参数里给一个子字符串，然后返回该子字符串在该字符串中第一次出现处的索引，比如\"woaixuexi\"要查\"ai\"这个子字符串在整个字符串中出现的索引位置那返回值就是2\n    * indexOf（String str，int fromIndex）这个方法不在累述\n    * lastIndexOf（int ch）：这个方法也是跟indexof相反，它是从后往前找返回字符在字符串中最后一次出现处的索引，也就是说找索引的时候是倒着找的但是返回值还是按照正的索引顺序返回的比如\"woaiwo\"用lastindexof查找‘w’返回的值是4而不是1\n    * lastIndexOf(int ch,fromindex)\n\n```java\n    public String longestCommonPrefix(String[] strs) {\n        if(strs.length == 0)\n            return \"\";\n        String prefix = strs[0];\n        for(int i = 0; i < strs.length; i++) {\n            while(strs[i].indexOf(prefix) != 0) {//前缀肯定是最短的，所以如果有这个相同前缀的话，肯定在每个string里面都有\n                prefix = prefix.substring(0, prefix.length() - 1);//前缀就从第一个字符串进行截取就可以了\n                if(prefix.isEmpty())\n                    return \"\";\n            }\n        }\n        return prefix;\n    }\n```\n## 非常规顺序操作\n\n* [ZigZag Conversion](https://leetcode.com/problems/zigzag-conversion/)\n    * 取相同数字位的进行操作：0123210123210..\n    * 法一：能够发现0之后都是加一，3之后都是减一\n    * 设置一个标志来判断是否加一还是减一\n    * 法二：变步长\n    \n```java\n    //1\n    if(curRow == 0 || curRow == numRows - 1) goingDown = !goingDown;\n    curRow += goingDown ? 1 : -1;\n    //2\n            int n = s.length();\n            int cycleLen = 2 * numRows - 2; //numRows = 4\n    \n            for (int i = 0; i < numRows; i++) {\n                for (int j = 0; j + i < n; j += cycleLen) {\n                    ret.append(s.charAt(j + i));\n                    if (i != 0 && i != numRows - 1 && j + cycleLen - i < n)\n                        ret.append(s.charAt(j + cycleLen - i));\n                }\n            }\n```\n\n## 数字计算\n\n* pow(x, n)\n    * n%2==0 -> x^n = x^(n/2) * x^(n/2) = (x*x)^(n/2)\n    * n%2==1 -> x^n = x*(x^(n/2) * x^(n/2)) = x * (x*x)^(n/2)\n\n```java\npublic double pow(double x, int n) {\n            if(n == 0)\n                return 1;\n            if(n == Integer.MIN_VALUE){//-2147483648不能直接换成正的，会溢出\n                return myPow(x*x, n/2);\n            }\n            if(n < 0){\n                x = 1/x;\n                n = -n;\n            }\n            if(n%2 == 1) \n                return myPow(x*x, n/2)*x;\n            else\n                return myPow(x*x, n/2);\n                }\n```\n\n* sqrt(x)\n    * I have seen many variants using Binary Search, the key difference is the search range. It seems easy to do it but actually there are some traps we need to take care. I made this just for a note for me.\n      Search range summary:\n      \n      * [1, Integer.MAX_VALUE](easy but not recommend)\n      * [1, x](recommended)\n      * [1, x/2](you need to do math to prove it)\n    * For case 2 and case 3, we need to take care of the corner case by making sure right >= left for [left, right], so:\n      2. x >= 1 for [1, x] => so we need to take care of the corner case: x < 1\n      3. x/2 >= 1 for [1, x/2]=> x >= 2 => so we need to take care of the corner case: x < 2\n   \n      \n```java\nclass Solution {\n    public int mySqrt(int x) {\n        long l=0,r=x; //in case of overflow\n        while(l<r){\n            long mid=l+(r-l)/2+1;\n            if(mid*mid>x) r=mid-1;\n            else l=mid;\n        }\n        return (int)l;\n    }\n}\n```\n\n* 计算加减式\n    * [basic-calculator](https://leetcode.com/problems/basic-calculator/)\n  \n```java\npublic int calculate(String s) {\n        Stack<Integer> stack = new Stack<Integer>();\n        int result = 0;\n        int number = 0;\n        int sign = 1;\n        for(int i = 0; i < s.length(); i++) {\n            char c = s.charAt(i);\n            if(Character.isDigit(c)) {\n                number = 10 * number + (int)(c  - '0');\n            }\n            else if(c == '+') {\n                number = sign * number;\n                result += number;\n                number = 0;\n                sign = 1;\n            } else if(c == '-') {\n                number = sign * number;\n                result += number;\n                number = 0;\n                sign = -1;\n            } else if( c == '(') {\n                stack.push(result);\n                stack.push(sign);\n                result = 0;\n                sign = 1;\n            } else if( c == ')') {\n                number = sign * number;\n                result += number;\n                number = 0; \n                result *= stack.pop();\n                result += stack.pop();\n            }\n        }\n        //最后以数字结尾的话\n        if(number != 0) return result += sign * number;\n        return result;\n    }\n```\n\n* 加减乘除\n\n```java\npublic int calculate(String s) {\n        Stack<Integer> stack = new Stack<Integer>();\n        int result = 0;\n        int number = 0;\n        char sign = '+';\n        for(int i = 0; i < s.length(); i++ ) {\n            \n            char c = s.charAt(i);\n            if(Character.isDigit(c)) {\n                number = number * 10 + (int)(c - '0');\n            }\n            if((!Character.isDigit(c) && c != ' ') || i == s.length()-1) {\n                if( sign == '+') {\n                    stack.push(number);\n                }\n                if(sign == '-') {\n                    stack.push(-number);\n                }\n                if(sign == '*') {\n                    stack.push(stack.pop() * number);\n                }\n                if(sign == '/') {\n                    stack.push(stack.pop() / number);\n                }\n                \n                sign = c;\n                number = 0;\n            }\n        }\n        \n        for(int i : stack) {\n            result += i;\n        }\n        \n        return result;\n    }\n```\n\n* [Missing Num](https://leetcode.com/problems/missing-number/)\n    * 在n长的数组中，包含0到n这n+1个数字中的n个。有个一数字[0, n]不在里面。\n    * 将数组的索引[0, n-1]想成抽屉的编号，如果是n缺少的话，每个抽屉其实都能装到自己的数字，n就单独出来了。\n    * 如果是[0, n-1]中的数字缺少了，说明n这个数字占了其中一个抽屉，那个数字（索引编号还贴在抽屉上）就单出来了。\n    * 使用异或操作可以将单独的数字找出来。\n\n```java\n     public int missingNumber(int[] nums) {   \n            \n            int missing = nums.length;\n            for(int i = 0; i < nums.length; i++) {\n                missing ^= i ^ nums[i];\n            }\n            \n            return missing;\n        }\n```\n\n## Int处理\n\n* Palindrome Number 回文数\n    * 转换成String 使用reverse()判断是否相同\n    * [reverse Integer](https://leetcode.com/problems/reverse-integer/)，再看是否相同，但是可能会溢出\n    * 只转换后一半的数字，与前面一半的数字比较：1221 --> 12 12 12321 --> 12 123/10\n\n```java\npublic boolean isPalindrome(int x) {\n    if(x < 0 || (x % 10 == 0 && x != 0))\n        return false;\n    \n    int rev = 0;\n    while(x > rev) { //当前面的数字小于等于后面翻转的数字时，就到中间了\n        rev = rev * 10 + x % 10;\n        x /= 10;\n    }\n    \n    return (x == rev || x == rev / 10);\n}\n```\n\n## LinkedList\n\n* Remove Linked List Elements\n\n```java\npublic ListNode removeElements(ListNode head, int val) {\n        ListNode dummy = new ListNode(0);\n        dummy.next = head;\n        head = dummy;\n        while(head.next != null) {\n            if(head.next.val == val)\n                head.next = head.next.next;\n            else\n                head = head.next;\n        }\n    }\n```\n\n* middle ListNode\n\n```java\npublic ListNode middleNode(ListNode head) {\n    ListNode slow = head, fast = head;\n    while(fast != null && fast.next != null) {\n        slow = slow.next;\n        fast = fast.next.next;\n    }\n    return slow;\n}\n```\n\n* reverse List\n    * 将链表翻转\n\n```java\npublic ListNode reverse(ListNode head) {\n    ListNode pre = null;\n    while(head != null) {\n        ListNode next = head.next;\n        head.next = pre;\n        pre = head;\n        head = next;\n    }\n    return pre;\n}\n```\n\n* 翻转部分List\n    * [reverse list II](https://leetcode.com/problems/reverse-linked-list-ii/)\n\n```java\npublic ListNode reverseBetween(ListNode head, int m, int n) {\n        \n        if(head == null || n == 1) return head;\n        \n        ListNode dummy = new ListNode(0);\n        dummy.next = head;\n        ListNode p = dummy;\n        while(m > 1) {\n            p = p.next;\n            m--;\n            n--;\n        }\n        ListNode tail = p.next;\n        \n        ListNode tmp = p;\n        while(n > 1) {\n            tmp = p.next;\n            p.next = tail.next;\n            tail.next = tail.next.next;\n            p.next.next = tmp;\n            n--;\n        }\n         \n        return dummy.next;\n    }\n```\n\n* 有环的LinkedList\n    * [Linked List CycleII](https://leetcode.com/problems/linked-list-cycle-ii/)\n    * two pointers看解释\n    * 延伸[287. Find the Duplicate Number](https://leetcode.com/problems/find-the-duplicate-number/)\n    * 将数组当成链表的索引表\n    * 鸽笼原理，一个数组[n+1]中有1-n个不相同的数，肯定有一个数字是重复的。如果把数组中的每个数当做是索引值的话，没有重复数字的情况下，就会形成一个没有环的链。如果中间有重复值的话，就一定会出现环。\n\n```java\n    public int findDuplicate(int[] nums) {\n            int slow = nums[0];\n            int fast = nums[0];\n            \n            do{\n                slow = nums[slow];\n                fast = nums[nums[fast]];\n            }while(slow != fast);\n            \n            int head = nums[0];\n            while(head != slow) {\n                head = nums[head];\n                slow = nums[slow];\n            }\n            \n            return head;\n        }\n```\n\n* [二进制加](https://leetcode.com/problems/add-binary/)\n```java\n    public String addBinary(String a, String b) {\n            StringBuilder sb = new StringBuilder();\n            int i = a.length(), j = b.lengthg(), carry = 0;\n            \n            while(i >= 0 || j >= 0) {\n                int sum = carry;\n                if(j >= 0) sum += b.charAt(j--) - '0';\n                if(i >= 0) sum += a.charAt(i--) - '0'；\n                sb.insert(0, sum % 2);\n                carry = sum / 2;\n            }\n            \n            if(carry ！= 0)\n                sb.insert(0, carry);\n            return sb.toString();\n        }\n```\n    \n## 指针\n\n* 将数组中的数当成指针\n    * [442. Find All Duplicates in an Array](https://leetcode.com/problems/find-all-duplicates-in-an-array)\n\n```java\npublic List<Integer> findDuplicates(int[] nums) {\n        List<Integer> res = new ArrayList<>();\n        \n        for (int i = 0; i < nums.length; ++i) {\n            int index = Math.abs(nums[i])-1;//要去绝对值 索引\n            if (nums[index] < 0)//是负的说明，之前遇到过\n                res.add(Math.abs(index+1));\n            nums[index] = -nums[index];\n        }\n        \n        return res;\n    }\n```\n   * 448. Find All Numbers Disappeared in an Array\n\n```java\n    public List<Integer> findDisappearedNumbers(int[] nums) {\n        \n        List<Integer> res = new ArrayList<>();\n        for(int i = 0; i < nums.length; i++) {\n            int val = Math.abs(nums[i]) - 1;\n            if(nums[val] > 0) {\n                nums[val] = - nums[val];\n            }\n        }\n        for(int i = 0; i < nums.length; i++) {\n            if(nums[i] > 0 ) {\n                res.add(i+1);\n            }\n        }\n        return res;\n    }\n```\n## 表达式校验\n\n* [Valid Parentheses](https://leetcode.com/problems/valid-parentheses/)\n    * 常规做法：用stack保存左括号，遇到右括号检验栈顶是不是对应的左括号，是的话就pop，不是就不正确\n    * 非常规，思想相同：\n\n```java\n    public boolean isValid(String s) {\n            Stack<Character> stack = new Stack<Character>();\n    \tfor (char c : s.toCharArray()) {\n    \t\tif (c == '(')\n    \t\t\tstack.push(')');\n    \t\telse if (c == '{')\n    \t\t\tstack.push('}');\n    \t\telse if (c == '[')\n    \t\t\tstack.push(']');\n    \t\telse if (stack.isEmpty() || stack.pop() != c)\n    \t\t\treturn false;\n    \t}\n    \treturn stack.isEmpty();\n```\n\n## 三分查找\n* [guess number](https://leetcode.com/problems/guess-number-higher-or-lower/solution/)\n\n```java\n    public int guessNumber(int n) {\n            int low = 1;\n            int high = n;\n            while (low <= high) {\n                int mid1 = low + (high - low) / 3;\n                int mid2 = high - (high - low) / 3;\n                int res1 = guess(mid1);\n                int res2 = guess(mid2);\n                if (res1 == 0)\n                    return mid1;\n                if (res2 == 0)\n                    return mid2;\n                else if (res1 < 0)\n                    high = mid1 - 1;\n                else if (res2 > 0)\n                    low = mid2 + 1;\n                else {\n                    low = mid1 + 1;\n                    high = mid2 - 1;\n                }\n            }\n            return -1;\n        }\n```","slug":"MyLeetCodeSummarize","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwj4002848upzw07sxip","content":"<p>我的答案仓库地址：<a href=\"https://github.com/fangmiao97/MyLeetCode\" target=\"_blank\" rel=\"noopener\">MyLeetCode</a></p>\n<h2 id=\"Tree与迭代、动态规划\"><a href=\"#Tree与迭代、动态规划\" class=\"headerlink\" title=\"Tree与迭代、动态规划\"></a>Tree与迭代、动态规划</h2><ul>\n<li><a href=\"https://leetcode.com/problems/maximum-binary-tree/\" target=\"_blank\" rel=\"noopener\">Maximum Binary Tree</a><ul>\n<li>思路：将创建最大二叉树，即根节点比所有叶子节点都大，的过程，分解成先寻找到当前数列中最大值，然后在创建左右最大子树的过程</li>\n<li>退出情况是数列为一个数时返回null，即表示结束没有子树可以构造了</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TreeNode root = <span class=\"keyword\">new</span> TreeNode(nums[max_i]);</span><br><span class=\"line\">root.left = construct(nums, l, max_i);</span><br><span class=\"line\">root.right = construct(nums, max_i + <span class=\"number\">1</span>, r);</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/maximum-binary-tree-ii/\" target=\"_blank\" rel=\"noopener\">Maximum Binary Tree II</a><ul>\n<li>描述：在一棵现成的最大二叉树中，插入一个数，使得插入之后依然是最大二叉树</li>\n<li>技巧点：比根节点小的数一律往根节点的右子树插。</li>\n<li>思路：插入值与当前根节点的值比较，若大于根节点的值则创建节点，并将原根节点作为自己的左子结点，并返回新的根节点。否则的话插入值插入原根节点的右子树，并循环这个过程。若插入值比较到最后，即与null比较，则创建该节点并返回这个节点。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> TreeNode <span class=\"title\">insertIntoMaxTree</span><span class=\"params\">(TreeNode root, <span class=\"keyword\">int</span> val)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(root == <span class=\"keyword\">null</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> TreeNode(val);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(root.val&lt;val)&#123;</span><br><span class=\"line\">            TreeNode head = <span class=\"keyword\">new</span> TreeNode(val);</span><br><span class=\"line\">            head.left = root;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        root.right = insertIntoMaxTree(root.right, val);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> root;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/second-minimum-node-in-a-binary-tree/\" target=\"_blank\" rel=\"noopener\">Second Minimum Node In a Binary Tree</a><ul>\n<li>需要注意的事，必须将所有的节点都遍历完全才能找到第二小的数字。因为有一个用例将第二小的数值藏在了最下面</li>\n<li>采用DFS或BFS都可以。关键是判断第二小的时候，先要将第一小的数找到，如果之后有数字不是小于<strong>等于</strong>第一小的话，才可以比较是不是第二小。如果只是单纯的小于第一小，会让第二小也成为和第一小一样的数值。</li>\n</ul>\n</li>\n<li>BFS常用结构</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Queue&lt;TreeNode&gt; a = <span class=\"keyword\">new</span> LinkedList&lt;&gt;();</span><br><span class=\"line\">a.offer(p);</span><br><span class=\"line\"><span class=\"keyword\">while</span> (!a.isEmpty())&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = a.size(); sz &gt; <span class=\"number\">0</span>; --sz) &#123;</span><br><span class=\"line\">        TreeNode n = a.poll();</span><br><span class=\"line\">        <span class=\"comment\">//do something</span></span><br><span class=\"line\">        a.offer(n.left);</span><br><span class=\"line\">        a.offer(n.right);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/sum-of-root-to-leaf-binary-numbers/\" target=\"_blank\" rel=\"noopener\">Sum of Root To Leaf Binary Numbers</a><ul>\n<li>DFS</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">sumRootToLeaf</span><span class=\"params\">(TreeNode root)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dfs(root, <span class=\"number\">0</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(TreeNode root, <span class=\"keyword\">int</span> sum)</span> </span>&#123;        </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    sum = sum * <span class=\"number\">2</span> + root.val;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> root.left == root.right ? sum : dfs(root.left, sum) + dfs(root.right, sum);</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"字节-位处理\"><a href=\"#字节-位处理\" class=\"headerlink\" title=\"字节/位处理\"></a>字节/位处理</h2><ul>\n<li><a href=\"https://leetcode.com/problems/reverse-integer/\" target=\"_blank\" rel=\"noopener\">Reverse Integer</a><ul>\n<li>java中不同数据类型的取值范围<ul>\n<li>int 32</li>\n<li>short 16</li>\n<li>long 64</li>\n<li>float 32</li>\n<li>double 64</li>\n</ul>\n</li>\n<li>本题中关于溢出可能的判断</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//方式一</span></span><br><span class=\"line\"><span class=\"comment\">//正数溢出情况（2147483647）</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (rev &gt; Integer.MAX_VALUE/<span class=\"number\">10</span> || (rev == Integer.MAX_VALUE / <span class=\"number\">10</span> &amp;&amp; pop &gt; <span class=\"number\">7</span>)) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"comment\">//负数（-2147483648）</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (rev &lt; Integer.MIN_VALUE/<span class=\"number\">10</span> || (rev == Integer.MIN_VALUE / <span class=\"number\">10</span> &amp;&amp; pop &lt; -<span class=\"number\">8</span>)) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//方式二</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>((rev - pop) / <span class=\"number\">10</span> != org) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;<span class=\"comment\">//溢出的话肯定计算不出原来的数字了</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/number-of-1-bits/\" target=\"_blank\" rel=\"noopener\">Number of 1 Bits</a><ul>\n<li>找一个int数的二进制中有多少个1，hamming weight</li>\n<li>&amp; – 位与运算 | – 位或运算</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">31</span>; i &gt;= <span class=\"number\">0</span>; i--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>((n &amp; <span class=\"number\">1</span>) == <span class=\"number\">1</span>)</span><br><span class=\"line\">                res++;</span><br><span class=\"line\">            n &gt;&gt;= <span class=\"number\">1</span>; <span class=\"comment\">//右移一位</span></span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/30108890\" target=\"_blank\" rel=\"noopener\">java中的移位操作符</a></p>\n</li>\n<li><p><a href=\"https://leetcode.com/problems/reverse-bits/\" target=\"_blank\" rel=\"noopener\">Reverse Bits</a></p>\n<ul>\n<li>翻转32位比特值，依次移动每一位。将每次需要移动的比特放在最后一位。与1进行位与操作后，将这一位移动到正确的位置后，与结果进行位或操作。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">31</span>; i &gt;= <span class=\"number\">0</span>; i--) &#123;</span><br><span class=\"line\">            ans = ans | ((n &amp; <span class=\"number\">1</span>) &lt;&lt; i);</span><br><span class=\"line\">            n &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><a href=\"https://leetcode.com/problems/counting-bits/\" target=\"_blank\" rel=\"noopener\">Counting Bits</a></p>\n<ul>\n<li><p><strong>DP</strong> When it comes to even numbers, i.e, 2,4,6,8, their binary should be like ‘10’, ‘100’, ‘110’, ‘1000’<br>so one notable difference is their unit digit is always 0,<br>which means when you call &gt;&gt; 1- shift one bit rightwards<br>and also means dividing by 2- would cause no change to the count of ‘1’ in the binary string.</p>\n<p>Vice versa, when you meet odd numbers, shifting one bit rightwards always eliminates one ‘1’ digit from original binary string,<br>that is why we should “compensate” one ‘1’ character to the count.</p>\n<p>To sum up, when you meet even number the count of ‘1’s is always the same as its half number,<br>on the other hand, remember to plus one to the odds’ half number.</p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span>[] f = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[num + <span class=\"number\">1</span>];</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>; i&lt;=num; i++) f[i] = f[i &gt;&gt; <span class=\"number\">1</span>] + (i &amp; <span class=\"number\">1</span>);</span><br><span class=\"line\"><span class=\"keyword\">return</span> f;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Two-Pointers\"><a href=\"#Two-Pointers\" class=\"headerlink\" title=\"Two Pointers\"></a>Two Pointers</h2><ul>\n<li><p><a href=\"https://chocoluffy.com/2016/12/04/浅析经典面试算法题-two-pointer的运用/\" target=\"_blank\" rel=\"noopener\">浅析经典面试算法题-two pointer的运用</a></p>\n</li>\n<li><p>Two Sum</p>\n<ul>\n<li>integer array已经过排序</li>\n<li>两个pointers一头一尾。那么sum只有三种可能：<ul>\n<li>sum == target，则返回</li>\n<li>sum &lt; target，头指针向后走一个</li>\n<li>sum &gt; target，尾指针向前走一个</li>\n</ul>\n</li>\n<li>循环条件，头 &lt; 尾</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> low = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> high = nums.length - <span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span>(low &lt; high) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nums[low] + nums[high] == target)</span><br><span class=\"line\">        <span class=\"comment\">//do something;</span></span><br><span class=\"line\">        <span class=\"comment\">// low++ high--;</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (nums[low] + nums[high] &lt; target)</span><br><span class=\"line\">        low++;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        high--;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>3Sum<ul>\n<li>先将数列排序，再固定第一个数字，从剩下的数列中用2 sum的方法找。</li>\n<li>注意一些要过滤的条件：<ul>\n<li>第一个数字在移动的过程中，如果与前一个一样的话，就再移一下</li>\n<li>low和high移动的道理也一样</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;List&lt;Integer&gt;&gt; threeSum(<span class=\"keyword\">int</span>[] nums) &#123;</span><br><span class=\"line\">       Arrays.sort(nums);</span><br><span class=\"line\">       List&lt;List&lt;Integer&gt;&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;()</span><br><span class=\"line\">       <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length - <span class=\"number\">2</span>; i++) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span>(i == <span class=\"number\">0</span> || nums[i] != nums[i - <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> low = i + <span class=\"number\">1</span>;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> high = nums.length - <span class=\"number\">1</span>;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> remain = <span class=\"number\">0</span> - nums[i];</span><br><span class=\"line\">               <span class=\"keyword\">while</span>(low &lt; high) &#123;</span><br><span class=\"line\">                   <span class=\"keyword\">if</span>(nums[low] + nums[high] == remain) &#123;</span><br><span class=\"line\">                       res.add(Arrays.asList(nums[i], nums[low], nums[high]));</span><br><span class=\"line\">                       <span class=\"keyword\">while</span>(low &lt; high &amp;&amp; nums[low + <span class=\"number\">1</span>] == nums[low])</span><br><span class=\"line\">                           low++;</span><br><span class=\"line\">                       <span class=\"keyword\">while</span>(low &lt; high &amp;&amp; nums[high - <span class=\"number\">1</span>] == nums[high])</span><br><span class=\"line\">                           high--;</span><br><span class=\"line\">                       low++;</span><br><span class=\"line\">                       high--;</span><br><span class=\"line\">                   &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(nums[low] + nums[high] &lt; remain)</span><br><span class=\"line\">                       low++;</span><br><span class=\"line\">                   <span class=\"keyword\">else</span></span><br><span class=\"line\">                       high--;</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>3Sum closest</p>\n<ul>\n<li>找最接近的sum（也可能相等）</li>\n<li>增加判断条件Math.abs小的话，就要更新。</li>\n</ul>\n</li>\n<li><p>3Sum With Multiplicity</p>\n<ul>\n<li>每一中情况都要考虑到–排列组合</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (A[j] == A[k]) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// If A[j...k] are all equal, then there are C(k - j + 1, 2) </span></span><br><span class=\"line\">  <span class=\"comment\">// combinations that meet the requirement.取两个</span></span><br><span class=\"line\">  res = (res + (k - j + <span class=\"number\">1</span>) * (k - j) / <span class=\"number\">2</span>) % m;</span><br><span class=\"line\">  <span class=\"keyword\">break</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">int</span> l = <span class=\"number\">1</span>, r = <span class=\"number\">1</span>;</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (j + l &lt; k &amp;&amp; A[j + l] == A[j]) &#123; ++l; &#125; <span class=\"comment\">// l: number of elements equal to A[j].</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (j &lt; k - r &amp;&amp; A[k - r] == A[k]) &#123; ++r; &#125; <span class=\"comment\">// r: number of elements equal to A[k].</span></span><br><span class=\"line\">  res = (res + l * r) % m; <span class=\"comment\">// found l * r cases that meet the requirement.</span></span><br><span class=\"line\">  j += l; <span class=\"comment\">// forward j by l steps.</span></span><br><span class=\"line\">  k -= r; <span class=\"comment\">// backward k by r steps.</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Sum of Square Numbers<ul>\n<li>c = a^2 + b^2</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">judgeSquareSum</span><span class=\"params\">(<span class=\"keyword\">int</span> c)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> limit = (<span class=\"keyword\">int</span>)Math.sqrt(c);</span><br><span class=\"line\">        <span class=\"keyword\">int</span> low = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(low&lt;=limit)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> sum = low*low + limit*limit;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(c==sum)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(sum&lt;c)&#123;</span><br><span class=\"line\">               low++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">               limit--; </span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"基础字符串操作\"><a href=\"#基础字符串操作\" class=\"headerlink\" title=\"基础字符串操作\"></a>基础字符串操作</h2><ul>\n<li><p>reverse</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">reverse</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">     StringBuilder res=<span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++)</span><br><span class=\"line\">           res.insert(<span class=\"number\">0</span>,s.charAt(i));</span><br><span class=\"line\">       <span class=\"keyword\">return</span> res.toString();</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>split 对应API public String[] split(String regex, int limit)</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> String[] split(String s) &#123;</span><br><span class=\"line\">        ArrayList &lt; String &gt; words = <span class=\"keyword\">new</span> ArrayList &lt; &gt; ();</span><br><span class=\"line\">        StringBuilder word = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (s.charAt(i) == <span class=\"string\">' '</span>) &#123;</span><br><span class=\"line\">                words.add(word.toString());</span><br><span class=\"line\">                word = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                word.append( s.charAt(i));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        words.add(word.toString());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> words.toArray(<span class=\"keyword\">new</span> String[words.size()]);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>大小写转换<ul>\n<li>a - 97 0x61(0110 0001) A - 65 0x41(0100 0001)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//法一</span></span><br><span class=\"line\">string.toLowerCase() <span class=\"function\">or <span class=\"title\">toUpperCase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"comment\">//法二</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">if</span> <span class=\"params\">(<span class=\"string\">'A'</span> &lt;= a[i] &amp;&amp; a[i] &lt;= <span class=\"string\">'Z'</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    a[i] </span>= (<span class=\"keyword\">char</span>) (a[i] - <span class=\"string\">'A'</span> + <span class=\"string\">'a'</span>);</span><br><span class=\"line\"><span class=\"comment\">//法三，按位或，把第五位的1加上</span></span><br><span class=\"line\"><span class=\"keyword\">char</span> c = (<span class=\"keyword\">char</span>)(str.charAt(i) | (<span class=\"keyword\">char</span>)(<span class=\"number\">32</span>));</span><br></pre></td></tr></table></figure>\n<ul>\n<li>相同前缀<ul>\n<li>indexOf（int，ch）：先看第一个indexOf它返回值是int，在看它的参数（int，ch）意思就是使用者可以给参数一个‘char’字符所代表的int值，然后去从前向后找到该字符在字符串中第一次出现处的索引，当然了我们不可能记得住每一个char的值所以我们在使用时直接用String s=abcdef;　int i=s.indexOf(‘d’)<br>这种方式就可以了，char类型会自动提升为int类型，还有就是要注意如果返回值为-1，就说明索引越界了；</li>\n<li>indexOf（int ch，int，fromIndex）：这个方法就是说从指定位置往后找返回字符在该字符串中第一次出现处的索引，比如“woaizhongguo”indexOf（’o’,2）那返回值就是6而不是1，也不是11；</li>\n<li>indexOf（Sting str）：这个方法基本就类似前面的了，只不过它是在参数里给一个子字符串，然后返回该子字符串在该字符串中第一次出现处的索引，比如”woaixuexi”要查”ai”这个子字符串在整个字符串中出现的索引位置那返回值就是2</li>\n<li>indexOf（String str，int fromIndex）这个方法不在累述</li>\n<li>lastIndexOf（int ch）：这个方法也是跟indexof相反，它是从后往前找返回字符在字符串中最后一次出现处的索引，也就是说找索引的时候是倒着找的但是返回值还是按照正的索引顺序返回的比如”woaiwo”用lastindexof查找‘w’返回的值是4而不是1</li>\n<li>lastIndexOf(int ch,fromindex)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">longestCommonPrefix</span><span class=\"params\">(String[] strs)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(strs.length == <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span>;</span><br><span class=\"line\">    String prefix = strs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; strs.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(strs[i].indexOf(prefix) != <span class=\"number\">0</span>) &#123;<span class=\"comment\">//前缀肯定是最短的，所以如果有这个相同前缀的话，肯定在每个string里面都有</span></span><br><span class=\"line\">            prefix = prefix.substring(<span class=\"number\">0</span>, prefix.length() - <span class=\"number\">1</span>);<span class=\"comment\">//前缀就从第一个字符串进行截取就可以了</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span>(prefix.isEmpty())</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> prefix;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"非常规顺序操作\"><a href=\"#非常规顺序操作\" class=\"headerlink\" title=\"非常规顺序操作\"></a>非常规顺序操作</h2><ul>\n<li><a href=\"https://leetcode.com/problems/zigzag-conversion/\" target=\"_blank\" rel=\"noopener\">ZigZag Conversion</a><ul>\n<li>取相同数字位的进行操作：0123210123210..</li>\n<li>法一：能够发现0之后都是加一，3之后都是减一</li>\n<li>设置一个标志来判断是否加一还是减一</li>\n<li>法二：变步长</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//1</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(curRow == <span class=\"number\">0</span> || curRow == numRows - <span class=\"number\">1</span>) goingDown = !goingDown;</span><br><span class=\"line\">curRow += goingDown ? <span class=\"number\">1</span> : -<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"comment\">//2</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = s.length();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> cycleLen = <span class=\"number\">2</span> * numRows - <span class=\"number\">2</span>; <span class=\"comment\">//numRows = 4</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; numRows; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j + i &lt; n; j += cycleLen) &#123;</span><br><span class=\"line\">                ret.append(s.charAt(j + i));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (i != <span class=\"number\">0</span> &amp;&amp; i != numRows - <span class=\"number\">1</span> &amp;&amp; j + cycleLen - i &lt; n)</span><br><span class=\"line\">                    ret.append(s.charAt(j + cycleLen - i));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"数字计算\"><a href=\"#数字计算\" class=\"headerlink\" title=\"数字计算\"></a>数字计算</h2><ul>\n<li>pow(x, n)<ul>\n<li>n%2==0 -&gt; x^n = x^(n/2) <em> x^(n/2) = (x</em>x)^(n/2)</li>\n<li>n%2==1 -&gt; x^n = x<em>(x^(n/2) </em> x^(n/2)) = x <em> (x</em>x)^(n/2)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">double</span> <span class=\"title\">pow</span><span class=\"params\">(<span class=\"keyword\">double</span> x, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n == Integer.MIN_VALUE)&#123;<span class=\"comment\">//-2147483648不能直接换成正的，会溢出</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n &lt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                x = <span class=\"number\">1</span>/x;</span><br><span class=\"line\">                n = -n;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n%<span class=\"number\">2</span> == <span class=\"number\">1</span>) </span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>)*x;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>);</span><br><span class=\"line\">                &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>sqrt(x)</p>\n<ul>\n<li><p>I have seen many variants using Binary Search, the key difference is the search range. It seems easy to do it but actually there are some traps we need to take care. I made this just for a note for me.<br>Search range summary:</p>\n<ul>\n<li><a href=\"easy but not recommend\">1, Integer.MAX_VALUE</a></li>\n<li><a href=\"recommended\">1, x</a></li>\n<li><a href=\"you need to do math to prove it\">1, x/2</a></li>\n</ul>\n</li>\n<li>For case 2 and case 3, we need to take care of the corner case by making sure right &gt;= left for [left, right], so:<ol start=\"2\">\n<li>x &gt;= 1 for [1, x] =&gt; so we need to take care of the corner case: x &lt; 1</li>\n<li>x/2 &gt;= 1 for [1, x/2]=&gt; x &gt;= 2 =&gt; so we need to take care of the corner case: x &lt; 2</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">mySqrt</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> l=<span class=\"number\">0</span>,r=x; <span class=\"comment\">//in case of overflow</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> mid=l+(r-l)/<span class=\"number\">2</span>+<span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(mid*mid&gt;x) r=mid-<span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> l=mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"keyword\">int</span>)l;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>计算加减式<ul>\n<li><a href=\"https://leetcode.com/problems/basic-calculator/\" target=\"_blank\" rel=\"noopener\">basic-calculator</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">calculate</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Integer&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Integer&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> result = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> number = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">char</span> c = s.charAt(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(Character.isDigit(c)) &#123;</span><br><span class=\"line\">                number = <span class=\"number\">10</span> * number + (<span class=\"keyword\">int</span>)(c  - <span class=\"string\">'0'</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(c == <span class=\"string\">'+'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(c == <span class=\"string\">'-'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = -<span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( c == <span class=\"string\">'('</span>) &#123;</span><br><span class=\"line\">                stack.push(result);</span><br><span class=\"line\">                stack.push(sign);</span><br><span class=\"line\">                result = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( c == <span class=\"string\">')'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>; </span><br><span class=\"line\">                result *= stack.pop();</span><br><span class=\"line\">                result += stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//最后以数字结尾的话</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(number != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> result += sign * number;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>加减乘除</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">calculate</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Integer&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Integer&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> result = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> number = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">char</span> sign = <span class=\"string\">'+'</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++ ) &#123;</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"keyword\">char</span> c = s.charAt(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(Character.isDigit(c)) &#123;</span><br><span class=\"line\">                number = number * <span class=\"number\">10</span> + (<span class=\"keyword\">int</span>)(c - <span class=\"string\">'0'</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>((!Character.isDigit(c) &amp;&amp; c != <span class=\"string\">' '</span>) || i == s.length()-<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>( sign == <span class=\"string\">'+'</span>) &#123;</span><br><span class=\"line\">                    stack.push(number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'-'</span>) &#123;</span><br><span class=\"line\">                    stack.push(-number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'*'</span>) &#123;</span><br><span class=\"line\">                    stack.push(stack.pop() * number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'/'</span>) &#123;</span><br><span class=\"line\">                    stack.push(stack.pop() / number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                </span><br><span class=\"line\">                sign = c;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i : stack) &#123;</span><br><span class=\"line\">            result += i;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/missing-number/\" target=\"_blank\" rel=\"noopener\">Missing Num</a><ul>\n<li>在n长的数组中，包含0到n这n+1个数字中的n个。有个一数字[0, n]不在里面。</li>\n<li>将数组的索引[0, n-1]想成抽屉的编号，如果是n缺少的话，每个抽屉其实都能装到自己的数字，n就单独出来了。</li>\n<li>如果是[0, n-1]中的数字缺少了，说明n这个数字占了其中一个抽屉，那个数字（索引编号还贴在抽屉上）就单出来了。</li>\n<li>使用异或操作可以将单独的数字找出来。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">missingNumber</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;   </span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"keyword\">int</span> missing = nums.length;</span><br><span class=\"line\">       <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">           missing ^= i ^ nums[i];</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"keyword\">return</span> missing;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Int处理\"><a href=\"#Int处理\" class=\"headerlink\" title=\"Int处理\"></a>Int处理</h2><ul>\n<li>Palindrome Number 回文数<ul>\n<li>转换成String 使用reverse()判断是否相同</li>\n<li><a href=\"https://leetcode.com/problems/reverse-integer/\" target=\"_blank\" rel=\"noopener\">reverse Integer</a>，再看是否相同，但是可能会溢出</li>\n<li>只转换后一半的数字，与前面一半的数字比较：1221 –&gt; 12 12 12321 –&gt; 12 123/10</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isPalindrome</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(x &lt; <span class=\"number\">0</span> || (x % <span class=\"number\">10</span> == <span class=\"number\">0</span> &amp;&amp; x != <span class=\"number\">0</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">int</span> rev = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(x &gt; rev) &#123; <span class=\"comment\">//当前面的数字小于等于后面翻转的数字时，就到中间了</span></span><br><span class=\"line\">        rev = rev * <span class=\"number\">10</span> + x % <span class=\"number\">10</span>;</span><br><span class=\"line\">        x /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x == rev || x == rev / <span class=\"number\">10</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"LinkedList\"><a href=\"#LinkedList\" class=\"headerlink\" title=\"LinkedList\"></a>LinkedList</h2><ul>\n<li>Remove Linked List Elements</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">removeElements</span><span class=\"params\">(ListNode head, <span class=\"keyword\">int</span> val)</span> </span>&#123;</span><br><span class=\"line\">        ListNode dummy = <span class=\"keyword\">new</span> ListNode(<span class=\"number\">0</span>);</span><br><span class=\"line\">        dummy.next = head;</span><br><span class=\"line\">        head = dummy;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(head.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(head.next.val == val)</span><br><span class=\"line\">                head.next = head.next.next;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                head = head.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>middle ListNode</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">middleNode</span><span class=\"params\">(ListNode head)</span> </span>&#123;</span><br><span class=\"line\">    ListNode slow = head, fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"keyword\">null</span> &amp;&amp; fast.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        slow = slow.next;</span><br><span class=\"line\">        fast = fast.next.next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> slow;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>reverse List<ul>\n<li>将链表翻转</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">reverse</span><span class=\"params\">(ListNode head)</span> </span>&#123;</span><br><span class=\"line\">    ListNode pre = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(head != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        ListNode next = head.next;</span><br><span class=\"line\">        head.next = pre;</span><br><span class=\"line\">        pre = head;</span><br><span class=\"line\">        head = next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pre;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>翻转部分List<ul>\n<li><a href=\"https://leetcode.com/problems/reverse-linked-list-ii/\" target=\"_blank\" rel=\"noopener\">reverse list II</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">reverseBetween</span><span class=\"params\">(ListNode head, <span class=\"keyword\">int</span> m, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(head == <span class=\"keyword\">null</span> || n == <span class=\"number\">1</span>) <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">        </span><br><span class=\"line\">        ListNode dummy = <span class=\"keyword\">new</span> ListNode(<span class=\"number\">0</span>);</span><br><span class=\"line\">        dummy.next = head;</span><br><span class=\"line\">        ListNode p = dummy;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(m &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">            m--;</span><br><span class=\"line\">            n--;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ListNode tail = p.next;</span><br><span class=\"line\">        </span><br><span class=\"line\">        ListNode tmp = p;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(n &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            tmp = p.next;</span><br><span class=\"line\">            p.next = tail.next;</span><br><span class=\"line\">            tail.next = tail.next.next;</span><br><span class=\"line\">            p.next.next = tmp;</span><br><span class=\"line\">            n--;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">return</span> dummy.next;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>有环的LinkedList<ul>\n<li><a href=\"https://leetcode.com/problems/linked-list-cycle-ii/\" target=\"_blank\" rel=\"noopener\">Linked List CycleII</a></li>\n<li>two pointers看解释</li>\n<li>延伸<a href=\"https://leetcode.com/problems/find-the-duplicate-number/\" target=\"_blank\" rel=\"noopener\">287. Find the Duplicate Number</a></li>\n<li>将数组当成链表的索引表</li>\n<li>鸽笼原理，一个数组[n+1]中有1-n个不相同的数，肯定有一个数字是重复的。如果把数组中的每个数当做是索引值的话，没有重复数字的情况下，就会形成一个没有环的链。如果中间有重复值的话，就一定会出现环。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">findDuplicate</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> slow = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> fast = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">do</span>&#123;</span><br><span class=\"line\">            slow = nums[slow];</span><br><span class=\"line\">            fast = nums[nums[fast]];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">while</span>(slow != fast);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">int</span> head = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(head != slow) &#123;</span><br><span class=\"line\">            head = nums[head];</span><br><span class=\"line\">            slow = nums[slow];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/add-binary/\" target=\"_blank\" rel=\"noopener\">二进制加</a><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">addBinary</span><span class=\"params\">(String a, String b)</span> </span>&#123;</span><br><span class=\"line\">        StringBuilder sb = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = a.length(), j = b.lengthg(), carry = <span class=\"number\">0</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &gt;= <span class=\"number\">0</span> || j &gt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> sum = carry;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j &gt;= <span class=\"number\">0</span>) sum += b.charAt(j--) - <span class=\"string\">'0'</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(i &gt;= <span class=\"number\">0</span>) sum += a.charAt(i--) - <span class=\"string\">'0'</span>；</span><br><span class=\"line\">            sb.insert(<span class=\"number\">0</span>, sum % <span class=\"number\">2</span>);</span><br><span class=\"line\">            carry = sum / <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(carry ！= <span class=\"number\">0</span>)</span><br><span class=\"line\">            sb.insert(<span class=\"number\">0</span>, carry);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sb.toString();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"指针\"><a href=\"#指针\" class=\"headerlink\" title=\"指针\"></a>指针</h2><ul>\n<li>将数组中的数当成指针<ul>\n<li><a href=\"https://leetcode.com/problems/find-all-duplicates-in-an-array\" target=\"_blank\" rel=\"noopener\">442. Find All Duplicates in an Array</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;Integer&gt; <span class=\"title\">findDuplicates</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">        List&lt;Integer&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> index = Math.abs(nums[i])-<span class=\"number\">1</span>;<span class=\"comment\">//要去绝对值 索引</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (nums[index] &lt; <span class=\"number\">0</span>)<span class=\"comment\">//是负的说明，之前遇到过</span></span><br><span class=\"line\">                res.add(Math.abs(index+<span class=\"number\">1</span>));</span><br><span class=\"line\">            nums[index] = -nums[index];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><ol start=\"448\">\n<li>Find All Numbers Disappeared in an Array</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;Integer&gt; <span class=\"title\">findDisappearedNumbers</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    List&lt;Integer&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> val = Math.abs(nums[i]) - <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[val] &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            nums[val] = - nums[val];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[i] &gt; <span class=\"number\">0</span> ) &#123;</span><br><span class=\"line\">            res.add(i+<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"表达式校验\"><a href=\"#表达式校验\" class=\"headerlink\" title=\"表达式校验\"></a>表达式校验</h2><ul>\n<li><a href=\"https://leetcode.com/problems/valid-parentheses/\" target=\"_blank\" rel=\"noopener\">Valid Parentheses</a><ul>\n<li>常规做法：用stack保存左括号，遇到右括号检验栈顶是不是对应的左括号，是的话就pop，不是就不正确</li>\n<li>非常规，思想相同：</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isValid</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Character&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Character&gt;();</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">char</span> c : s.toCharArray()) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (c == <span class=\"string\">'('</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">')'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (c == <span class=\"string\">'&#123;'</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">'&#125;'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (c == <span class=\"string\">'['</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">']'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (stack.isEmpty() || stack.pop() != c)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> stack.isEmpty();</span><br></pre></td></tr></table></figure>\n<h2 id=\"三分查找\"><a href=\"#三分查找\" class=\"headerlink\" title=\"三分查找\"></a>三分查找</h2><ul>\n<li><a href=\"https://leetcode.com/problems/guess-number-higher-or-lower/solution/\" target=\"_blank\" rel=\"noopener\">guess number</a></li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">guessNumber</span><span class=\"params\">(<span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> low = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> high = n;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (low &lt;= high) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid1 = low + (high - low) / <span class=\"number\">3</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid2 = high - (high - low) / <span class=\"number\">3</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> res1 = guess(mid1);</span><br><span class=\"line\">            <span class=\"keyword\">int</span> res2 = guess(mid2);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (res1 == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> mid1;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (res2 == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> mid2;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (res1 &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                high = mid1 - <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (res2 &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                low = mid2 + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                low = mid1 + <span class=\"number\">1</span>;</span><br><span class=\"line\">                high = mid2 - <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<p>我的答案仓库地址：<a href=\"https://github.com/fangmiao97/MyLeetCode\" target=\"_blank\" rel=\"noopener\">MyLeetCode</a></p>\n<h2 id=\"Tree与迭代、动态规划\"><a href=\"#Tree与迭代、动态规划\" class=\"headerlink\" title=\"Tree与迭代、动态规划\"></a>Tree与迭代、动态规划</h2><ul>\n<li><a href=\"https://leetcode.com/problems/maximum-binary-tree/\" target=\"_blank\" rel=\"noopener\">Maximum Binary Tree</a><ul>\n<li>思路：将创建最大二叉树，即根节点比所有叶子节点都大，的过程，分解成先寻找到当前数列中最大值，然后在创建左右最大子树的过程</li>\n<li>退出情况是数列为一个数时返回null，即表示结束没有子树可以构造了</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TreeNode root = <span class=\"keyword\">new</span> TreeNode(nums[max_i]);</span><br><span class=\"line\">root.left = construct(nums, l, max_i);</span><br><span class=\"line\">root.right = construct(nums, max_i + <span class=\"number\">1</span>, r);</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/maximum-binary-tree-ii/\" target=\"_blank\" rel=\"noopener\">Maximum Binary Tree II</a><ul>\n<li>描述：在一棵现成的最大二叉树中，插入一个数，使得插入之后依然是最大二叉树</li>\n<li>技巧点：比根节点小的数一律往根节点的右子树插。</li>\n<li>思路：插入值与当前根节点的值比较，若大于根节点的值则创建节点，并将原根节点作为自己的左子结点，并返回新的根节点。否则的话插入值插入原根节点的右子树，并循环这个过程。若插入值比较到最后，即与null比较，则创建该节点并返回这个节点。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> TreeNode <span class=\"title\">insertIntoMaxTree</span><span class=\"params\">(TreeNode root, <span class=\"keyword\">int</span> val)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(root == <span class=\"keyword\">null</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> TreeNode(val);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(root.val&lt;val)&#123;</span><br><span class=\"line\">            TreeNode head = <span class=\"keyword\">new</span> TreeNode(val);</span><br><span class=\"line\">            head.left = root;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        root.right = insertIntoMaxTree(root.right, val);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> root;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/second-minimum-node-in-a-binary-tree/\" target=\"_blank\" rel=\"noopener\">Second Minimum Node In a Binary Tree</a><ul>\n<li>需要注意的事，必须将所有的节点都遍历完全才能找到第二小的数字。因为有一个用例将第二小的数值藏在了最下面</li>\n<li>采用DFS或BFS都可以。关键是判断第二小的时候，先要将第一小的数找到，如果之后有数字不是小于<strong>等于</strong>第一小的话，才可以比较是不是第二小。如果只是单纯的小于第一小，会让第二小也成为和第一小一样的数值。</li>\n</ul>\n</li>\n<li>BFS常用结构</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Queue&lt;TreeNode&gt; a = <span class=\"keyword\">new</span> LinkedList&lt;&gt;();</span><br><span class=\"line\">a.offer(p);</span><br><span class=\"line\"><span class=\"keyword\">while</span> (!a.isEmpty())&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = a.size(); sz &gt; <span class=\"number\">0</span>; --sz) &#123;</span><br><span class=\"line\">        TreeNode n = a.poll();</span><br><span class=\"line\">        <span class=\"comment\">//do something</span></span><br><span class=\"line\">        a.offer(n.left);</span><br><span class=\"line\">        a.offer(n.right);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/sum-of-root-to-leaf-binary-numbers/\" target=\"_blank\" rel=\"noopener\">Sum of Root To Leaf Binary Numbers</a><ul>\n<li>DFS</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">sumRootToLeaf</span><span class=\"params\">(TreeNode root)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dfs(root, <span class=\"number\">0</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(TreeNode root, <span class=\"keyword\">int</span> sum)</span> </span>&#123;        </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    sum = sum * <span class=\"number\">2</span> + root.val;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> root.left == root.right ? sum : dfs(root.left, sum) + dfs(root.right, sum);</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"字节-位处理\"><a href=\"#字节-位处理\" class=\"headerlink\" title=\"字节/位处理\"></a>字节/位处理</h2><ul>\n<li><a href=\"https://leetcode.com/problems/reverse-integer/\" target=\"_blank\" rel=\"noopener\">Reverse Integer</a><ul>\n<li>java中不同数据类型的取值范围<ul>\n<li>int 32</li>\n<li>short 16</li>\n<li>long 64</li>\n<li>float 32</li>\n<li>double 64</li>\n</ul>\n</li>\n<li>本题中关于溢出可能的判断</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//方式一</span></span><br><span class=\"line\"><span class=\"comment\">//正数溢出情况（2147483647）</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (rev &gt; Integer.MAX_VALUE/<span class=\"number\">10</span> || (rev == Integer.MAX_VALUE / <span class=\"number\">10</span> &amp;&amp; pop &gt; <span class=\"number\">7</span>)) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"comment\">//负数（-2147483648）</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (rev &lt; Integer.MIN_VALUE/<span class=\"number\">10</span> || (rev == Integer.MIN_VALUE / <span class=\"number\">10</span> &amp;&amp; pop &lt; -<span class=\"number\">8</span>)) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//方式二</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>((rev - pop) / <span class=\"number\">10</span> != org) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;<span class=\"comment\">//溢出的话肯定计算不出原来的数字了</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/number-of-1-bits/\" target=\"_blank\" rel=\"noopener\">Number of 1 Bits</a><ul>\n<li>找一个int数的二进制中有多少个1，hamming weight</li>\n<li>&amp; – 位与运算 | – 位或运算</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">31</span>; i &gt;= <span class=\"number\">0</span>; i--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>((n &amp; <span class=\"number\">1</span>) == <span class=\"number\">1</span>)</span><br><span class=\"line\">                res++;</span><br><span class=\"line\">            n &gt;&gt;= <span class=\"number\">1</span>; <span class=\"comment\">//右移一位</span></span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/30108890\" target=\"_blank\" rel=\"noopener\">java中的移位操作符</a></p>\n</li>\n<li><p><a href=\"https://leetcode.com/problems/reverse-bits/\" target=\"_blank\" rel=\"noopener\">Reverse Bits</a></p>\n<ul>\n<li>翻转32位比特值，依次移动每一位。将每次需要移动的比特放在最后一位。与1进行位与操作后，将这一位移动到正确的位置后，与结果进行位或操作。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">31</span>; i &gt;= <span class=\"number\">0</span>; i--) &#123;</span><br><span class=\"line\">            ans = ans | ((n &amp; <span class=\"number\">1</span>) &lt;&lt; i);</span><br><span class=\"line\">            n &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><a href=\"https://leetcode.com/problems/counting-bits/\" target=\"_blank\" rel=\"noopener\">Counting Bits</a></p>\n<ul>\n<li><p><strong>DP</strong> When it comes to even numbers, i.e, 2,4,6,8, their binary should be like ‘10’, ‘100’, ‘110’, ‘1000’<br>so one notable difference is their unit digit is always 0,<br>which means when you call &gt;&gt; 1- shift one bit rightwards<br>and also means dividing by 2- would cause no change to the count of ‘1’ in the binary string.</p>\n<p>Vice versa, when you meet odd numbers, shifting one bit rightwards always eliminates one ‘1’ digit from original binary string,<br>that is why we should “compensate” one ‘1’ character to the count.</p>\n<p>To sum up, when you meet even number the count of ‘1’s is always the same as its half number,<br>on the other hand, remember to plus one to the odds’ half number.</p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span>[] f = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[num + <span class=\"number\">1</span>];</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>; i&lt;=num; i++) f[i] = f[i &gt;&gt; <span class=\"number\">1</span>] + (i &amp; <span class=\"number\">1</span>);</span><br><span class=\"line\"><span class=\"keyword\">return</span> f;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Two-Pointers\"><a href=\"#Two-Pointers\" class=\"headerlink\" title=\"Two Pointers\"></a>Two Pointers</h2><ul>\n<li><p><a href=\"https://chocoluffy.com/2016/12/04/浅析经典面试算法题-two-pointer的运用/\" target=\"_blank\" rel=\"noopener\">浅析经典面试算法题-two pointer的运用</a></p>\n</li>\n<li><p>Two Sum</p>\n<ul>\n<li>integer array已经过排序</li>\n<li>两个pointers一头一尾。那么sum只有三种可能：<ul>\n<li>sum == target，则返回</li>\n<li>sum &lt; target，头指针向后走一个</li>\n<li>sum &gt; target，尾指针向前走一个</li>\n</ul>\n</li>\n<li>循环条件，头 &lt; 尾</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> low = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> high = nums.length - <span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span>(low &lt; high) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nums[low] + nums[high] == target)</span><br><span class=\"line\">        <span class=\"comment\">//do something;</span></span><br><span class=\"line\">        <span class=\"comment\">// low++ high--;</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (nums[low] + nums[high] &lt; target)</span><br><span class=\"line\">        low++;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        high--;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>3Sum<ul>\n<li>先将数列排序，再固定第一个数字，从剩下的数列中用2 sum的方法找。</li>\n<li>注意一些要过滤的条件：<ul>\n<li>第一个数字在移动的过程中，如果与前一个一样的话，就再移一下</li>\n<li>low和high移动的道理也一样</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> List&lt;List&lt;Integer&gt;&gt; threeSum(<span class=\"keyword\">int</span>[] nums) &#123;</span><br><span class=\"line\">       Arrays.sort(nums);</span><br><span class=\"line\">       List&lt;List&lt;Integer&gt;&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;()</span><br><span class=\"line\">       <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length - <span class=\"number\">2</span>; i++) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span>(i == <span class=\"number\">0</span> || nums[i] != nums[i - <span class=\"number\">1</span>]) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> low = i + <span class=\"number\">1</span>;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> high = nums.length - <span class=\"number\">1</span>;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> remain = <span class=\"number\">0</span> - nums[i];</span><br><span class=\"line\">               <span class=\"keyword\">while</span>(low &lt; high) &#123;</span><br><span class=\"line\">                   <span class=\"keyword\">if</span>(nums[low] + nums[high] == remain) &#123;</span><br><span class=\"line\">                       res.add(Arrays.asList(nums[i], nums[low], nums[high]));</span><br><span class=\"line\">                       <span class=\"keyword\">while</span>(low &lt; high &amp;&amp; nums[low + <span class=\"number\">1</span>] == nums[low])</span><br><span class=\"line\">                           low++;</span><br><span class=\"line\">                       <span class=\"keyword\">while</span>(low &lt; high &amp;&amp; nums[high - <span class=\"number\">1</span>] == nums[high])</span><br><span class=\"line\">                           high--;</span><br><span class=\"line\">                       low++;</span><br><span class=\"line\">                       high--;</span><br><span class=\"line\">                   &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(nums[low] + nums[high] &lt; remain)</span><br><span class=\"line\">                       low++;</span><br><span class=\"line\">                   <span class=\"keyword\">else</span></span><br><span class=\"line\">                       high--;</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>3Sum closest</p>\n<ul>\n<li>找最接近的sum（也可能相等）</li>\n<li>增加判断条件Math.abs小的话，就要更新。</li>\n</ul>\n</li>\n<li><p>3Sum With Multiplicity</p>\n<ul>\n<li>每一中情况都要考虑到–排列组合</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (A[j] == A[k]) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// If A[j...k] are all equal, then there are C(k - j + 1, 2) </span></span><br><span class=\"line\">  <span class=\"comment\">// combinations that meet the requirement.取两个</span></span><br><span class=\"line\">  res = (res + (k - j + <span class=\"number\">1</span>) * (k - j) / <span class=\"number\">2</span>) % m;</span><br><span class=\"line\">  <span class=\"keyword\">break</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">int</span> l = <span class=\"number\">1</span>, r = <span class=\"number\">1</span>;</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (j + l &lt; k &amp;&amp; A[j + l] == A[j]) &#123; ++l; &#125; <span class=\"comment\">// l: number of elements equal to A[j].</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (j &lt; k - r &amp;&amp; A[k - r] == A[k]) &#123; ++r; &#125; <span class=\"comment\">// r: number of elements equal to A[k].</span></span><br><span class=\"line\">  res = (res + l * r) % m; <span class=\"comment\">// found l * r cases that meet the requirement.</span></span><br><span class=\"line\">  j += l; <span class=\"comment\">// forward j by l steps.</span></span><br><span class=\"line\">  k -= r; <span class=\"comment\">// backward k by r steps.</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Sum of Square Numbers<ul>\n<li>c = a^2 + b^2</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">judgeSquareSum</span><span class=\"params\">(<span class=\"keyword\">int</span> c)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> limit = (<span class=\"keyword\">int</span>)Math.sqrt(c);</span><br><span class=\"line\">        <span class=\"keyword\">int</span> low = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(low&lt;=limit)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> sum = low*low + limit*limit;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(c==sum)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(sum&lt;c)&#123;</span><br><span class=\"line\">               low++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">               limit--; </span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"基础字符串操作\"><a href=\"#基础字符串操作\" class=\"headerlink\" title=\"基础字符串操作\"></a>基础字符串操作</h2><ul>\n<li><p>reverse</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">reverse</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">     StringBuilder res=<span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++)</span><br><span class=\"line\">           res.insert(<span class=\"number\">0</span>,s.charAt(i));</span><br><span class=\"line\">       <span class=\"keyword\">return</span> res.toString();</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>split 对应API public String[] split(String regex, int limit)</p>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> String[] split(String s) &#123;</span><br><span class=\"line\">        ArrayList &lt; String &gt; words = <span class=\"keyword\">new</span> ArrayList &lt; &gt; ();</span><br><span class=\"line\">        StringBuilder word = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (s.charAt(i) == <span class=\"string\">' '</span>) &#123;</span><br><span class=\"line\">                words.add(word.toString());</span><br><span class=\"line\">                word = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                word.append( s.charAt(i));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        words.add(word.toString());</span><br><span class=\"line\">        <span class=\"keyword\">return</span> words.toArray(<span class=\"keyword\">new</span> String[words.size()]);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>大小写转换<ul>\n<li>a - 97 0x61(0110 0001) A - 65 0x41(0100 0001)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//法一</span></span><br><span class=\"line\">string.toLowerCase() <span class=\"function\">or <span class=\"title\">toUpperCase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"comment\">//法二</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">if</span> <span class=\"params\">(<span class=\"string\">'A'</span> &lt;= a[i] &amp;&amp; a[i] &lt;= <span class=\"string\">'Z'</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    a[i] </span>= (<span class=\"keyword\">char</span>) (a[i] - <span class=\"string\">'A'</span> + <span class=\"string\">'a'</span>);</span><br><span class=\"line\"><span class=\"comment\">//法三，按位或，把第五位的1加上</span></span><br><span class=\"line\"><span class=\"keyword\">char</span> c = (<span class=\"keyword\">char</span>)(str.charAt(i) | (<span class=\"keyword\">char</span>)(<span class=\"number\">32</span>));</span><br></pre></td></tr></table></figure>\n<ul>\n<li>相同前缀<ul>\n<li>indexOf（int，ch）：先看第一个indexOf它返回值是int，在看它的参数（int，ch）意思就是使用者可以给参数一个‘char’字符所代表的int值，然后去从前向后找到该字符在字符串中第一次出现处的索引，当然了我们不可能记得住每一个char的值所以我们在使用时直接用String s=abcdef;　int i=s.indexOf(‘d’)<br>这种方式就可以了，char类型会自动提升为int类型，还有就是要注意如果返回值为-1，就说明索引越界了；</li>\n<li>indexOf（int ch，int，fromIndex）：这个方法就是说从指定位置往后找返回字符在该字符串中第一次出现处的索引，比如“woaizhongguo”indexOf（’o’,2）那返回值就是6而不是1，也不是11；</li>\n<li>indexOf（Sting str）：这个方法基本就类似前面的了，只不过它是在参数里给一个子字符串，然后返回该子字符串在该字符串中第一次出现处的索引，比如”woaixuexi”要查”ai”这个子字符串在整个字符串中出现的索引位置那返回值就是2</li>\n<li>indexOf（String str，int fromIndex）这个方法不在累述</li>\n<li>lastIndexOf（int ch）：这个方法也是跟indexof相反，它是从后往前找返回字符在字符串中最后一次出现处的索引，也就是说找索引的时候是倒着找的但是返回值还是按照正的索引顺序返回的比如”woaiwo”用lastindexof查找‘w’返回的值是4而不是1</li>\n<li>lastIndexOf(int ch,fromindex)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">longestCommonPrefix</span><span class=\"params\">(String[] strs)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(strs.length == <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span>;</span><br><span class=\"line\">    String prefix = strs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; strs.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(strs[i].indexOf(prefix) != <span class=\"number\">0</span>) &#123;<span class=\"comment\">//前缀肯定是最短的，所以如果有这个相同前缀的话，肯定在每个string里面都有</span></span><br><span class=\"line\">            prefix = prefix.substring(<span class=\"number\">0</span>, prefix.length() - <span class=\"number\">1</span>);<span class=\"comment\">//前缀就从第一个字符串进行截取就可以了</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span>(prefix.isEmpty())</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"\"</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> prefix;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"非常规顺序操作\"><a href=\"#非常规顺序操作\" class=\"headerlink\" title=\"非常规顺序操作\"></a>非常规顺序操作</h2><ul>\n<li><a href=\"https://leetcode.com/problems/zigzag-conversion/\" target=\"_blank\" rel=\"noopener\">ZigZag Conversion</a><ul>\n<li>取相同数字位的进行操作：0123210123210..</li>\n<li>法一：能够发现0之后都是加一，3之后都是减一</li>\n<li>设置一个标志来判断是否加一还是减一</li>\n<li>法二：变步长</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//1</span></span><br><span class=\"line\"><span class=\"keyword\">if</span>(curRow == <span class=\"number\">0</span> || curRow == numRows - <span class=\"number\">1</span>) goingDown = !goingDown;</span><br><span class=\"line\">curRow += goingDown ? <span class=\"number\">1</span> : -<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"comment\">//2</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = s.length();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> cycleLen = <span class=\"number\">2</span> * numRows - <span class=\"number\">2</span>; <span class=\"comment\">//numRows = 4</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; numRows; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j + i &lt; n; j += cycleLen) &#123;</span><br><span class=\"line\">                ret.append(s.charAt(j + i));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (i != <span class=\"number\">0</span> &amp;&amp; i != numRows - <span class=\"number\">1</span> &amp;&amp; j + cycleLen - i &lt; n)</span><br><span class=\"line\">                    ret.append(s.charAt(j + cycleLen - i));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"数字计算\"><a href=\"#数字计算\" class=\"headerlink\" title=\"数字计算\"></a>数字计算</h2><ul>\n<li>pow(x, n)<ul>\n<li>n%2==0 -&gt; x^n = x^(n/2) <em> x^(n/2) = (x</em>x)^(n/2)</li>\n<li>n%2==1 -&gt; x^n = x<em>(x^(n/2) </em> x^(n/2)) = x <em> (x</em>x)^(n/2)</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">double</span> <span class=\"title\">pow</span><span class=\"params\">(<span class=\"keyword\">double</span> x, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n == Integer.MIN_VALUE)&#123;<span class=\"comment\">//-2147483648不能直接换成正的，会溢出</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n &lt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                x = <span class=\"number\">1</span>/x;</span><br><span class=\"line\">                n = -n;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(n%<span class=\"number\">2</span> == <span class=\"number\">1</span>) </span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>)*x;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span> myPow(x*x, n/<span class=\"number\">2</span>);</span><br><span class=\"line\">                &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>sqrt(x)</p>\n<ul>\n<li><p>I have seen many variants using Binary Search, the key difference is the search range. It seems easy to do it but actually there are some traps we need to take care. I made this just for a note for me.<br>Search range summary:</p>\n<ul>\n<li><a href=\"easy but not recommend\">1, Integer.MAX_VALUE</a></li>\n<li><a href=\"recommended\">1, x</a></li>\n<li><a href=\"you need to do math to prove it\">1, x/2</a></li>\n</ul>\n</li>\n<li>For case 2 and case 3, we need to take care of the corner case by making sure right &gt;= left for [left, right], so:<ol start=\"2\">\n<li>x &gt;= 1 for [1, x] =&gt; so we need to take care of the corner case: x &lt; 1</li>\n<li>x/2 &gt;= 1 for [1, x/2]=&gt; x &gt;= 2 =&gt; so we need to take care of the corner case: x &lt; 2</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">mySqrt</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> l=<span class=\"number\">0</span>,r=x; <span class=\"comment\">//in case of overflow</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(l&lt;r)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> mid=l+(r-l)/<span class=\"number\">2</span>+<span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(mid*mid&gt;x) r=mid-<span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> l=mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"keyword\">int</span>)l;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>计算加减式<ul>\n<li><a href=\"https://leetcode.com/problems/basic-calculator/\" target=\"_blank\" rel=\"noopener\">basic-calculator</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">calculate</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Integer&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Integer&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> result = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> number = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">char</span> c = s.charAt(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(Character.isDigit(c)) &#123;</span><br><span class=\"line\">                number = <span class=\"number\">10</span> * number + (<span class=\"keyword\">int</span>)(c  - <span class=\"string\">'0'</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(c == <span class=\"string\">'+'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(c == <span class=\"string\">'-'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = -<span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( c == <span class=\"string\">'('</span>) &#123;</span><br><span class=\"line\">                stack.push(result);</span><br><span class=\"line\">                stack.push(sign);</span><br><span class=\"line\">                result = <span class=\"number\">0</span>;</span><br><span class=\"line\">                sign = <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( c == <span class=\"string\">')'</span>) &#123;</span><br><span class=\"line\">                number = sign * number;</span><br><span class=\"line\">                result += number;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>; </span><br><span class=\"line\">                result *= stack.pop();</span><br><span class=\"line\">                result += stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//最后以数字结尾的话</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(number != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> result += sign * number;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>加减乘除</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">calculate</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Integer&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Integer&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> result = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> number = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">char</span> sign = <span class=\"string\">'+'</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++ ) &#123;</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"keyword\">char</span> c = s.charAt(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(Character.isDigit(c)) &#123;</span><br><span class=\"line\">                number = number * <span class=\"number\">10</span> + (<span class=\"keyword\">int</span>)(c - <span class=\"string\">'0'</span>);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>((!Character.isDigit(c) &amp;&amp; c != <span class=\"string\">' '</span>) || i == s.length()-<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>( sign == <span class=\"string\">'+'</span>) &#123;</span><br><span class=\"line\">                    stack.push(number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'-'</span>) &#123;</span><br><span class=\"line\">                    stack.push(-number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'*'</span>) &#123;</span><br><span class=\"line\">                    stack.push(stack.pop() * number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(sign == <span class=\"string\">'/'</span>) &#123;</span><br><span class=\"line\">                    stack.push(stack.pop() / number);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                </span><br><span class=\"line\">                sign = c;</span><br><span class=\"line\">                number = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i : stack) &#123;</span><br><span class=\"line\">            result += i;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/missing-number/\" target=\"_blank\" rel=\"noopener\">Missing Num</a><ul>\n<li>在n长的数组中，包含0到n这n+1个数字中的n个。有个一数字[0, n]不在里面。</li>\n<li>将数组的索引[0, n-1]想成抽屉的编号，如果是n缺少的话，每个抽屉其实都能装到自己的数字，n就单独出来了。</li>\n<li>如果是[0, n-1]中的数字缺少了，说明n这个数字占了其中一个抽屉，那个数字（索引编号还贴在抽屉上）就单出来了。</li>\n<li>使用异或操作可以将单独的数字找出来。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">missingNumber</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;   </span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"keyword\">int</span> missing = nums.length;</span><br><span class=\"line\">       <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">           missing ^= i ^ nums[i];</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"keyword\">return</span> missing;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Int处理\"><a href=\"#Int处理\" class=\"headerlink\" title=\"Int处理\"></a>Int处理</h2><ul>\n<li>Palindrome Number 回文数<ul>\n<li>转换成String 使用reverse()判断是否相同</li>\n<li><a href=\"https://leetcode.com/problems/reverse-integer/\" target=\"_blank\" rel=\"noopener\">reverse Integer</a>，再看是否相同，但是可能会溢出</li>\n<li>只转换后一半的数字，与前面一半的数字比较：1221 –&gt; 12 12 12321 –&gt; 12 123/10</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isPalindrome</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(x &lt; <span class=\"number\">0</span> || (x % <span class=\"number\">10</span> == <span class=\"number\">0</span> &amp;&amp; x != <span class=\"number\">0</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">int</span> rev = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(x &gt; rev) &#123; <span class=\"comment\">//当前面的数字小于等于后面翻转的数字时，就到中间了</span></span><br><span class=\"line\">        rev = rev * <span class=\"number\">10</span> + x % <span class=\"number\">10</span>;</span><br><span class=\"line\">        x /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x == rev || x == rev / <span class=\"number\">10</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"LinkedList\"><a href=\"#LinkedList\" class=\"headerlink\" title=\"LinkedList\"></a>LinkedList</h2><ul>\n<li>Remove Linked List Elements</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">removeElements</span><span class=\"params\">(ListNode head, <span class=\"keyword\">int</span> val)</span> </span>&#123;</span><br><span class=\"line\">        ListNode dummy = <span class=\"keyword\">new</span> ListNode(<span class=\"number\">0</span>);</span><br><span class=\"line\">        dummy.next = head;</span><br><span class=\"line\">        head = dummy;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(head.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(head.next.val == val)</span><br><span class=\"line\">                head.next = head.next.next;</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                head = head.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>middle ListNode</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">middleNode</span><span class=\"params\">(ListNode head)</span> </span>&#123;</span><br><span class=\"line\">    ListNode slow = head, fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"keyword\">null</span> &amp;&amp; fast.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        slow = slow.next;</span><br><span class=\"line\">        fast = fast.next.next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> slow;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>reverse List<ul>\n<li>将链表翻转</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">reverse</span><span class=\"params\">(ListNode head)</span> </span>&#123;</span><br><span class=\"line\">    ListNode pre = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(head != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">        ListNode next = head.next;</span><br><span class=\"line\">        head.next = pre;</span><br><span class=\"line\">        pre = head;</span><br><span class=\"line\">        head = next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pre;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>翻转部分List<ul>\n<li><a href=\"https://leetcode.com/problems/reverse-linked-list-ii/\" target=\"_blank\" rel=\"noopener\">reverse list II</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ListNode <span class=\"title\">reverseBetween</span><span class=\"params\">(ListNode head, <span class=\"keyword\">int</span> m, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(head == <span class=\"keyword\">null</span> || n == <span class=\"number\">1</span>) <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">        </span><br><span class=\"line\">        ListNode dummy = <span class=\"keyword\">new</span> ListNode(<span class=\"number\">0</span>);</span><br><span class=\"line\">        dummy.next = head;</span><br><span class=\"line\">        ListNode p = dummy;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(m &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">            m--;</span><br><span class=\"line\">            n--;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ListNode tail = p.next;</span><br><span class=\"line\">        </span><br><span class=\"line\">        ListNode tmp = p;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(n &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            tmp = p.next;</span><br><span class=\"line\">            p.next = tail.next;</span><br><span class=\"line\">            tail.next = tail.next.next;</span><br><span class=\"line\">            p.next.next = tmp;</span><br><span class=\"line\">            n--;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">return</span> dummy.next;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>有环的LinkedList<ul>\n<li><a href=\"https://leetcode.com/problems/linked-list-cycle-ii/\" target=\"_blank\" rel=\"noopener\">Linked List CycleII</a></li>\n<li>two pointers看解释</li>\n<li>延伸<a href=\"https://leetcode.com/problems/find-the-duplicate-number/\" target=\"_blank\" rel=\"noopener\">287. Find the Duplicate Number</a></li>\n<li>将数组当成链表的索引表</li>\n<li>鸽笼原理，一个数组[n+1]中有1-n个不相同的数，肯定有一个数字是重复的。如果把数组中的每个数当做是索引值的话，没有重复数字的情况下，就会形成一个没有环的链。如果中间有重复值的话，就一定会出现环。</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">findDuplicate</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> slow = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> fast = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">do</span>&#123;</span><br><span class=\"line\">            slow = nums[slow];</span><br><span class=\"line\">            fast = nums[nums[fast]];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">while</span>(slow != fast);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">int</span> head = nums[<span class=\"number\">0</span>];</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(head != slow) &#123;</span><br><span class=\"line\">            head = nums[head];</span><br><span class=\"line\">            slow = nums[slow];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> head;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><a href=\"https://leetcode.com/problems/add-binary/\" target=\"_blank\" rel=\"noopener\">二进制加</a><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">addBinary</span><span class=\"params\">(String a, String b)</span> </span>&#123;</span><br><span class=\"line\">        StringBuilder sb = <span class=\"keyword\">new</span> StringBuilder();</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = a.length(), j = b.lengthg(), carry = <span class=\"number\">0</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &gt;= <span class=\"number\">0</span> || j &gt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> sum = carry;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j &gt;= <span class=\"number\">0</span>) sum += b.charAt(j--) - <span class=\"string\">'0'</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(i &gt;= <span class=\"number\">0</span>) sum += a.charAt(i--) - <span class=\"string\">'0'</span>；</span><br><span class=\"line\">            sb.insert(<span class=\"number\">0</span>, sum % <span class=\"number\">2</span>);</span><br><span class=\"line\">            carry = sum / <span class=\"number\">2</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(carry ！= <span class=\"number\">0</span>)</span><br><span class=\"line\">            sb.insert(<span class=\"number\">0</span>, carry);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sb.toString();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"指针\"><a href=\"#指针\" class=\"headerlink\" title=\"指针\"></a>指针</h2><ul>\n<li>将数组中的数当成指针<ul>\n<li><a href=\"https://leetcode.com/problems/find-all-duplicates-in-an-array\" target=\"_blank\" rel=\"noopener\">442. Find All Duplicates in an Array</a></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;Integer&gt; <span class=\"title\">findDuplicates</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">        List&lt;Integer&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> index = Math.abs(nums[i])-<span class=\"number\">1</span>;<span class=\"comment\">//要去绝对值 索引</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (nums[index] &lt; <span class=\"number\">0</span>)<span class=\"comment\">//是负的说明，之前遇到过</span></span><br><span class=\"line\">                res.add(Math.abs(index+<span class=\"number\">1</span>));</span><br><span class=\"line\">            nums[index] = -nums[index];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><ol start=\"448\">\n<li>Find All Numbers Disappeared in an Array</li>\n</ol>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;Integer&gt; <span class=\"title\">findDisappearedNumbers</span><span class=\"params\">(<span class=\"keyword\">int</span>[] nums)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    List&lt;Integer&gt; res = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> val = Math.abs(nums[i]) - <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[val] &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            nums[val] = - nums[val];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(nums[i] &gt; <span class=\"number\">0</span> ) &#123;</span><br><span class=\"line\">            res.add(i+<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"表达式校验\"><a href=\"#表达式校验\" class=\"headerlink\" title=\"表达式校验\"></a>表达式校验</h2><ul>\n<li><a href=\"https://leetcode.com/problems/valid-parentheses/\" target=\"_blank\" rel=\"noopener\">Valid Parentheses</a><ul>\n<li>常规做法：用stack保存左括号，遇到右括号检验栈顶是不是对应的左括号，是的话就pop，不是就不正确</li>\n<li>非常规，思想相同：</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">isValid</span><span class=\"params\">(String s)</span> </span>&#123;</span><br><span class=\"line\">        Stack&lt;Character&gt; stack = <span class=\"keyword\">new</span> Stack&lt;Character&gt;();</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">char</span> c : s.toCharArray()) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (c == <span class=\"string\">'('</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">')'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (c == <span class=\"string\">'&#123;'</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">'&#125;'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (c == <span class=\"string\">'['</span>)</span><br><span class=\"line\">\t\t\tstack.push(<span class=\"string\">']'</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (stack.isEmpty() || stack.pop() != c)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> stack.isEmpty();</span><br></pre></td></tr></table></figure>\n<h2 id=\"三分查找\"><a href=\"#三分查找\" class=\"headerlink\" title=\"三分查找\"></a>三分查找</h2><ul>\n<li><a href=\"https://leetcode.com/problems/guess-number-higher-or-lower/solution/\" target=\"_blank\" rel=\"noopener\">guess number</a></li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">guessNumber</span><span class=\"params\">(<span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> low = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> high = n;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (low &lt;= high) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid1 = low + (high - low) / <span class=\"number\">3</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> mid2 = high - (high - low) / <span class=\"number\">3</span>;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> res1 = guess(mid1);</span><br><span class=\"line\">            <span class=\"keyword\">int</span> res2 = guess(mid2);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (res1 == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> mid1;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (res2 == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> mid2;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (res1 &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                high = mid1 - <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (res2 &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">                low = mid2 + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                low = mid1 + <span class=\"number\">1</span>;</span><br><span class=\"line\">                high = mid2 - <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>"},{"title":"【翻译】弹性分布式数据集：基于内存的集群计算的容错性抽象","date":"2019-04-12T16:00:00.000Z","_content":"## context\n大概用了16个小时完成了这篇关于RDD论文的翻译，这篇论文奠定了Spark的设计基础。\n\n原文:[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n\n推荐先看林子雨老师关于RDD的[解释](http://dblab.xmu.edu.cn/blog/985-2/)\n\n## 摘要\n本文提出了一个分布式内存抽象的概念--弹性分布式数据集（Resilient Distributed Datasets，以下称为RDDs），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDDs的提出是由于现有的两种计算框架并不高效：迭代式算法和交互式数据挖掘工具。在内存中操作数据可以将前两种计算方式的效率提高一个数量级。RDDs提供了一种受限的共享内存，是基于粗粒度的转换操作而不是细粒度的状态同步。尽管如此，RDDs依然能够表示多种类型的计算，包括专用的迭代编程模型（如Pregel）和一些新的应用模型。我们通过在Spark上评估各种应用和基准，实现了RDDs。\n\n## 1 引言\n集群计算框架，如MapReduce[10]和Dryad[19]，已经被广泛地运用于大规模数据分析。这些系统能够让用户在不用考虑任务调度和容错的前提下，使用一系列高级的操作进行并行计算。\n\n虽然这些框架为获取集权计算资源提供了大量的抽象，但是缺少对分布式内存的运用。这使他们对于一类新兴的应用十分低效：它们在不同的计算阶段*重用*中间结果。数据重用在*迭代式*机器学习和图算法中十分常见，包括PageRank、K-means聚类和逻辑回归。另一个明显的用例是*交互式*数据挖掘，用户在同样的数据子集上进行ad-hoc查询。然而不好的是，对于现在的框架，在不同计算阶段之间重用数据（如，在两个MapReduce的job之间）的唯一方式是将其写入外部稳定存储系统中，如，分布式文件系统。由于数据的复制、硬盘I/O和序列化，导致了大量的成本开销，并占据了应用运行的大部分时间。\n\n在意识到这个问题之后，研究人员针对需要数据重用的应用开发了专门的框架。比如，迭代式图计算系统Pregel[22]，其将中间数据存放在内存中，而HaLoop[7]提供了一种迭代式MapReduce接口。无论如何，这些框架仅支持特定的计算模式（如，循环一系列的MapReduce步骤），并隐式地提供这些模式的数据共享。它们没有提供更加通用的数据重用的抽象，如，让用户直接向内存中装载数据集，并对其进行ad-hoc查询。\n\n在这篇论文中，我们提出了一种能够广泛运用于各种应用中高效的数据重用抽象，_弹性分布式数据集_（RDDs）。RDDs是一个容错的、并行的数据结构，能够让用户明确地在内存中持久化中间结果，控制其分区以优化数据的放置和使用丰富的操作符对其进行处理。\n\n设计RDDs的主要挑战是，定义一个能够高效容错能力的编程接口。现有的基于集群的内存存储抽象，如分布式共享内存、键值对存储、数据库和Piccolo，提供了基于细粒度更新可变状态的接口（如，表中的cell）。运用这些接口时，达到数据容错的唯一方式是在不同的机器之间进行数据的冗余或者记录更新日志。这两种方法对于数据密集型工作来说代价高昂，因为他们需要在集群网络间复制大量数据，而网络带宽远小于RAM带宽，同时还会产生大量数据存储开销。\n\n相比于这些系统，RDDs提供基于粗粒度转换的，可用于大量数据项进行相同操作的接口（如，map,filter和join）。这使得RDDs能够通过记录产生数据集的一系列转换操作（称之为lineage），而不是记录真实的数据，来提高容错的效率【1当lineage链过长时，对一些RDDs进行检查点（checkpoint）设置可能更加有用，我们在5.4节对其进行讨论】。如果一个分区的RDD丢失了，它有足够的信息知道自己是如何从其他的RDD产生的，从而重新计算该分区。因此，丢失的数据可以很快地恢复，而不需要代价昂贵的复制。\n\n基于粗粒度转换的接口乍一看是有局限性的，但RDDs对于许多并行应用都非常适用，因为这些应用本质上就是会对多种数据项进行相同的操作。为此，我们证明了RDDs高效地运用于表达各种已经被现有的分布式系统所实现的集群编程模型，包括MapReduce，DryadLINQ,，SQL，Pregel 和 HaLoop，以及这些系统无法实现的新应用，如交互式数据挖掘。RDDs能够作为被用来解决前面提到的计算需求而引入的新框架的证据，是因为其强大的抽象能力。\n\n我们已经在一个被用于UC Berkeley的实验环境和许多公司生产环境下的应用--Spark上，实现了RDDs。Spark提供了一个运用Scala语言，类似DryadLINQ的易用的语言集成编程接口。另外，Spark还可以在Scala解释器中进行交互式大数据集的查询。我们相信Spark会是第一个运用通用编程语言完成交互式速度下集群内存数据挖掘的系统。\n\n我们通过微基准测试和用户应用对RDDs和Spark进行评估。我们得出，Spark在迭代式应用上比Hadoop快20倍，在真实数据报表分析上快40倍，并且能够在5-7秒的延迟内完成1TB数据集的交互式扫描。更加根本地，为了说明RDDs的通用性，我们在Spark上实现了Pregel和HaLoop的编程模型，包括用一些相对较小的库（每个库大概200行代码）实现它们所采用存储优化策略。\n\n这篇论文首先介绍RDDs的概览（第2部分）和Spark（第3部分），然后讨论RDDs的内部表示（第4部分），实现（第5部分），和一些实验结果（第6部分）。最后，我们讨论了用RDDs实现几个现有的集群编程模型（第7部分），相关研究工作（第8部分）和总结。\n\n## 2 弹性分布式数据集（RDDs）\n\n这一部分提供关于RDDs的概述。先定义RDDs（2.1节），介绍其在Spark中的编程接口（2.2节）。然后将RDDs与细粒度共享内存抽象进行比较（2.3节）。最后讨论RDD模型的限制（2.4节）。\n\n### 2.1 RDD抽象\n\n一个RDD是只读的，是将记录进行分区的集合。RDDs只能由（1）稳定物理存储中的数据集（2）其他RDDs通过明确的操作产生。我们称这些操作为转换（transformations）以区别其他对RDDs的操作。转换的例子包括map，filter和join。【2 虽然单个RDDs是不可变的，但是可以通过多个RDDs来表示不同版本的数据集以实现多状态。我们让RDDs不可变以使lineage图表示更简便，但这也相当于将我们的抽象变成版本化数据并在lineage图中追踪不同版本】\n\nRDDs不需要都实体化。一个RDD有足够的信息了解自己是如何从其他数据集产生的（lineage）并通过信息从稳定的物理存储中计算出自己的分区。这一强大特性的本质是，程序能够在RDD重建之后对其进行引用。\n\n最后一点，用户可以对RDDs进行2方面的控制：持久化和分区。用户可以表明将要重用的RDDs并为其选择存储策略（如，内存存储）。也可以将RDDs的元素通过特定键值进行分区。这些功能对于存储优化特别有用，比如保证两个将要进行join操作的数据集都进行了相同的哈希分区。\n\n### 2.2 Spark编程接口\n\nSpark暴露了类似DryadLINQ和FlumeJava的RDDs语言集成API，每个数据集都被表示成一个对象，并通过执行方法在这些对象上进行转换操作（transformations）。\n\n开发人员通过将物理存储上的数据集进行转换（如，map和filter）来定义一个或多个RDDs。然后可以用动作（actions）对这些RDDs进行操作，这些操作向应用返回值或者向存储系统产生外部数据。动作（actions）的例子包括，count（返回数据集中元素的数目），collect（返回元素本身）和save（将数据集输出到外部存储系统）。像DryadLINQ一样，Spark在遇到一个动作操作时才会真正计算出RDDs，所以其可以对转换（transformations）进行流水线操作。\n\n此外，开发人员还可以调用persist方法来声明他们想重用的RDDs。Spark默认会将RDDs留存在内存中，但是会在没有足够RAM的情况下将它们溢出到硬盘。用户可以采用其他的持久化策略，如通过persist标价，将特定RDD只存在硬盘上或在机器之间进行备份。最后，用户可以为每个RDD设置优先级来表明当需要时，将哪一个内存中的数据溢出到硬盘中。\n\n#### 2.2.1 例子：控制台日志挖掘\n\n假设一个网页服务出现错误，管理员想在HDFS中兆字节规模的日志中找到原因。应用Spark，管理员能够将错误信息从多个结点的日志中导入RAM，并交互式的进行查询。她会键入以下代码：\n\n```scala\nlines = spark.textFile(\"hdfs://...\")\nerrors = lines.filter(_.startsWith(\"ERROR\"))\nerrors.persist()\n```\n第一行从一个HDFS文件（文本行集合）定义了一个RDD，并在第二行产生过滤后的RDD。\n\n第三行将errors持久化在内存中，这样就能被查询。注意filter的参数是一个闭包的Scala语法。\n\n到目前为止，并没有在集群上运行作业。但是，用户可以对RDD进行动作（actions）操作，如计算消息的数目：\n\n`errors.count()`\n\n这位用户也可以对该RDD进行进一步的转换操作并运用他们的结果，如下：\n\n```scala\n// Count errors mentioning MySQL:\nerrors.filter(_.contains(\"MySQL\")).count()\n// Return the time fields of errors mentioning\n// HDFS as an array (assuming time is field\n// number 3 in a tab-separated format):\nerrors.filter(_.contains(\"HDFS\"))\n      .map(_.split(’\\t’)(3))\n      .collect()\n```\n在第一个对errors进行动作操作之后，Spark将在内存中村塾errors的各分区，这极大地加快了下游的计算操作。注意，最原始的RDD，lines，不会加载到RAM中。这是可取的，因为错误消息可能只是数据的一小部分（小到足够放进内存）。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060102174/0)\n\n最后为了说明模型是如何实现容错的，图1展示了该RDDs的lineage图。在这次查询中，我们首先对lines进行过滤得到errors，然后在运行collect操作前进行更进一步的filter和map操作。Spark调度器将会流水执行后两个转换操作并向缓存了errors分区的结点发送任务的集合来对其进行计算。除此之外，如果errors的一个分区丢失了，Spark只会在相关的lines分区上进行过滤操作来重建丢失的分区。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060116532/0)\n\n### 2.3 RDD模型的优势\n\n为了理解RDDs作为分布式内存抽象的优势，表1将其与分布式共享内存（DSM）进行了比较。在DSM系统中，应用对全局地址空间进行随意位置的读写。请注意，根据此定义，我们不仅包含了传统的共享内存系统[24]，还包括应用可以进行细粒度写共享状态的其他系统，如Piccolo[27]，其提供共享DHT和分布式数据库。DSM是非常通用的抽象，但是这样的通用性使其很难以一种高效且容错的方式运用在商业集群上。\n\nRDDs与DSM的主要不同是，RDDs只能通过粗粒度的转换操作产生（“写”），而DSM允许在内存中任意位置读写【注意对RDDs的读仍然可以是细粒度的。例如，应用系统可以将一个RDD视为一个大型只读查找表】。这会限制RDDs为执行批量写入的应用程序，但是这使得其具有高效的容错性。特别是，RDD不需要产生检查点（checkpoint）的开销，因为它们可以使用lineage来恢复【在一些应用中，在具有很长lineage链的RDDs中仍然可以使用checkpoint技术，我们将在5.4节讨论。但是，这些可以在后台完成，因为RDDs是不可变的，并且不需要像在DSM中那样保留整个应用程序的快照】。而且，只有丢失的RDD分区才需要在失败时重新计算，并且它们可以在不同的节点上并行计算，而不必回滚整个程序。\n\nRDDs的第二个优势是，由于它们不可变的特性，通过运行缓慢任务的副本来缓解慢结点对系统拖拽的压力，就和MapReduce一样[10]。使用DSM很难实现备份任务，因为任务的两个副本将访问相同的内存位置并干扰彼此的更新。\n\n最后，RDDs提供了DSM没有的两个其他好处。其一，在对RDDs的批量操作中，一个运行时可以基于数据的位置进行任务调度以提高性能。其二，当没有足够的内存来存储RDDs时，它就会优雅地降级，使它们仅用于扫描操作。不适合RAM的分区可以存储在磁盘上，并提供与当期数据并行系统类似的性能。\n\n### 2.4 不适合RDDs的应用\n\n正如引言中所讨论的，RDDs非常适用于对数据集中所有元素进行相同操作的批处理应用。在这些情况下，RDDs可以有效地将每个转换（transformations）记录为lineage图中的一个步骤，并且可以在不记录大量数据的情况下恢复丢失的分区。RDDs不太适合对共享状态进行异步细粒度共享状态更新的应用程序，例如Web应用程序的存储系统或增量Web爬网程序。对于这些应用程序，使用执行传统更新日志记录和checkpoint的系统更有效，例如数据库，RAMCloud [25]，Percolator [26]和Piccolo [27]。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。\n\n## 3 Spark编程接口\n\nSpark通过Scala [2]提供类似于DryadLINQ [31]语言集成API的RDDs抽象，Scala [2]是基于Java VM的静态类型函数编程语言。我们之所以选择Scala，是因为它结合了简洁（便于交互使用）和效率（静态类型）。但是，关于RDD抽象的任何内容都不需要函数式语言\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060128798/0)\n\n要使用Spark，开发人员编写一个连接到一组workers的driver程序，如图2所示。驱动程序定义一个或多个RDDs并在其上调用动作操作。驱动程序上的Spark代码也会追踪RDDs的lineage。这些workers是长期存在的过程，能够跨操作在RAM中存储RDDs分区。\n\n正如我们在2.2.1中的日志挖掘例子中所展示，用户想如map这样的RDD操作传递闭包（函数形式）来提供参数。Scala将每个闭包表示为Java对象，这些对象可以序列化并加载到另一个节点上，以通过网络传递闭包。Scala也将在闭包中绑定的任何变量作为Java对象的域。比如，可以编写代码：var x = 5; rdd.map(_ + x)，将在RDD中的每一个元素都加5【我们在闭包创建时对其进行保存，这样例子中的map操作永远都是加5，即使x发生变化】。\n\nRDD本身是由元素类型参数化的静态类型对象。例如，RDD [Int]是整数的RDD。但是，由于Scala支持类型推断，因此我们的大多数示例都省略了类型。\n\n虽然我们在Scala中暴露RDDs的方法在概念上很简单，但我们必须使用反射解决Scala的闭包对象的问题[33]。我们还需要更多的工作来使Spark可以使用Scala解释器，我们将在5.2节中讨论。但是，我们不必修改Scala编译器。\n\n### 3.1 Spark中的RDD操作\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060140749/0)\n\n表2列出了Spark中可用的主要RDD转换和动作操作。我们给出每个操作的签名，在方括号中显示类型参数。再次强调转换（transformations）是定义新RDD的延迟操作，而动作（actions）启动计算将向程序返回值或将数据写入外部存储。\n\n请注意，某些操作（如join）仅适用于键值对的RDDs。此外，我们的函数名称被选择为与Scala和其他函数式语言中的其他API匹配；例如，map是一对一映射，而flatMap将每个输入值映射到一个或多个输出（类似于MapReduce中的映射）。\n\n除了这些运算符，用户还可以持久化RDD。此外，用户可以获得RDD的分区顺序（由Partitioner类表示），并根据它对另一个数据集进行分区。诸如groupByKey，reduceByKey和sort操作自动地会产生哈希或范围分区的RDD。\n\n### 3.2 示例应用\n\n我们使用两个迭代应用程序补充了2.2.1节中的数据挖掘示例：逻辑回归和PageRank。后者还展示了如何控制RDD的分区以提高性能。\n\n#### 3.2.1 logistic回归\n\n许多机器学习算法本质上是迭代的，因为它们运行迭代优化过程，例如梯度下降，以最大化功能。因此，通过将数据保存在内存中，以更快地运行。\n\n例如，以下程序实现了逻辑回归[14]，这是用于搜索最能分开两类点（例如，垃圾邮件和非垃圾邮件）的超平面w的一个通用算法。该算法使用梯度下降：它以随机值开始w，并且在每次迭代时，它将w的函数与数据相加以沿着改善它的方向移动。\n\n```scala\nval points = spark.textFile(...)\n                  .map(parsePoint).persist()\nvar w = // random initial vector\nfor (i <- 1 to ITERATIONS) {\n  val gradient = points.map{ p =>\n    p.x * (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y\n  }.reduce((a,b) => a+b)\n  w -= gradient\n}\n```\n我们首先定义一个名为points的持久RDD作为文本文件上的map转换的结果，该文本文件将每行文本解析为Point对象。然后，我们通过对当前w的函数求和，对points重复运行map和reduce以计算每一步的梯度。在迭代的过程中将points保存在内存中可以获得20倍的加速，这将在6.1节展示。\n\n#### 3.2.2 PageRank\n\n更复杂的数据共享模式发生在PageRank [6]。算法通过累加在文件中对每个文件的应用次数迭代地更新每一文件的rank。在每次迭代时，每个文件都向其邻居发送r/n的贡献值，r是其排名，n是其邻居的数量。然后通过α/N + (1 − α)∑ci式子更新排名，其中求和是它所收到的贡献值，而N是文件的总数。我们可以通过如下代码Spark中实现PageRank：\n\n```scala\n// Load graph as an RDD of (URL, outlinks) pairs\nval links = spark.textFile(...).map(...).persist()\nvar ranks = // RDD of (URL, rank) pairs\nfor (i <- 1 to ITERATIONS) {\n// Build an RDD of (targetURL, float) pairs\n  // with the contributions sent by each page\n  val contribs = links.join(ranks).flatMap {\n    (url, (links, rank)) =>\n      links.map(dest => (dest, rank/links.size))\n  }\n  // Sum contributions by URL and get new ranks\n  ranks = contribs.reduceByKey((x,y) => x+y)\n             .mapValues(sum => a/N + (1-a)*sum)\n}\n```\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060154806/0)\n\n该程序产生图3中的RDD lineage图。在每次迭代中，我们基于来自先前的迭代的contribs和ranks以及静态links数据集来创建新的排名数据集【请注意，尽管RDD是不可变的，但程序中的变量rank和contribs指向每次迭代时的不同RDDs】。图3的一个有趣特征是随着迭代数目的增加而不断增长。因此，在具有多次迭代的作业中，可能需要可靠地复制某些版本的ranks以减少故障恢复时间[20]。用户可以使用RELIABLE标志调用persist来执行此操作。但请注意，不需要复制links数据集，因为可以通过在输入文件的块上重新运行map来有效地重建它的分区。此数据集通常比ranks大得多，因为每个文档都有许多链接，但只有一个数字作为其排名，因此使用lineage恢复它比使用checkpoint回复程序整个内存中状态的系统更节省时间。\n\n最后，我们可以通过控制RDDs的分区来优化PageRank中的通信。如果我们指定links的分区（例如，通过节点间的URL对link lists进行哈希分区），以相同的方式对ranks进行分区，这样就确保links和ranks之间的join操作不需要通信（因为每个URL的排名将与其link list在同一台机器上）。我们还可以编写自定义分区程序类来对相互链接的页面进行分组（例如，按域名对URL进行分区）。在定义links时，可以通过调用partitionBy来表示这两种优化：\n\n```scala\nlinks = spark.textFile(...).map(...)\n             .partitionBy(myPartFunc).persist()\n```\n\n在此初始化之后，links和ranks之间的join操作会自动地将每个URL的贡献值聚集到它link lists所在的机器上，在这台机器上计算新的排名并与它的links进行join操作。这种跨迭代的一致分区是Pregel等专用框架中的主要优化之一。 RDDs让用户直接表达这一目标。\n\n## 4 表示RDDs\n\n提供RDD作为抽象的挑战之一是为它们选择一种能够跟踪各种转换操作的lineage的表示。理想情况下，实现RDD的系统应该提供尽可能丰富的一组转换操作（例如，表2中的转换操作），并让用户以任意方式组合它们。我们为RDD提出了一个简单的基于图的表示，以达到这些目标。我们在Spark中使用这种表示来支持各种转换，而无需为调度器添加针对每个转换的特殊逻辑，这大大简化了系统设计。简而言之，我们建议通过一个公共接口来表示每个RDD，这个接口包含五条信息：一组分区，它们是数据集的原子部分；父RDDs（parent RDDs）的一组依赖关系；基于其父RDDs计算数据集的函数；有关其分区方案和数据放置的元数据。例如，表示HDFS文件的RDD具有文件的每个块的分区，并且知道每个块所在的机器。同时，对于这个RDD进行map操作的结果具有相同的分区，但是在计算其元素时将在父数据上应用map方法。我们在表3中总结了这个接口。\n\n![](https://puui.qpic.cn/fans_admin/0/3_234219102_1557060167174/0)\n\n设计此接口时最有趣的问题是如何表示RDDs之间的依赖关系。我们发现将依赖关系分为两类是足够且有用的：窄依赖关系（narrow dependencies），其中父RDD的每个分区最多由子RDD的一个分区使用；宽依赖关系（wide dependencies），其中多个子RDD分区可能依赖一个父DD分区。例如，map导致窄依赖关系，而join导致宽依赖关系（除非父RDD是哈希分区的）。图4显示了其他示例。\n\n![](https://puui.qpic.cn/fans_admin/0/3_256675758_1557060251325/0)\n\n这样区分有两个原因。首先，窄依赖关系允许在一个集群节点上进行流水线执行，这可以计算所有父分区。例如，可以逐个元素地应用map，然后应用filter操作。相比之下，宽依赖关系要求所有父分区的数据都已经计算完成，并使用类似MapReduce的操作在节点之间进行shuffle。其次，节点故障后的恢复在窄依赖时更有效，因为只需要重新计算丢失的对应的父分区，并且可以在不同节点上并行地重新计算它们。相反，在具有宽依赖的lineage图中，单个故障节点可能导致RDD的所有祖先丢失某些分区，从而需要完全重新执行计算。\n\nRDDs的这个通用接口使得可以在少于20行代码中实现Spark中的大多数转换操作。实际上，即使是新的Spark用户也已经实现了新的转换（例如，采样和各种类型的join），而不用知道调度器的细节。我们在下面阐述一些RDD实现。\n\n**HDFS文件：**\n我们样本中的输入RDDs是HDFS中的文件。对于这些RDDs，partitions为文件的每个块返回一个分区（块的偏移量存储在每个Partition对象中），preferredLocations给出块所在的节点，iterator读取块。\n\n**map：**\n在任何RDD上调用map都会返回MappedRDD对象。该操作传递一个函数参数给map，对父RDD上的记录按照iterator的方式执行这个函数，并返回一组符合条件的父RDD分区及其位置。\n\n**union：**\n在两个RDD上执行union操作，返回两个父RDD分区的并集。通过相应父RDD上的窄依赖关系计算每个子RDD分区【7注意union操作不会过滤重复值】。\n\n**join：**\n对两个RDD执行join操作可能产生窄依赖（如果这两个RDD拥有相同的哈希分区或范围分区），可能是宽依赖，也可能两种依赖都有（比如一个父RDD有分区，而另一父RDD没有）。在任何一种情况下，输出RDD都有一个分区程序（从父项继承的分区程序或默认的散列分区程序）。\n\n## 5 实现\n\n我们大约用14000行scala代码实现了Spark。该系统运行在Mesos集群管理器[17]上，允许它与Hadoop，MPI和其他应用程序共享资源。每个Spark程序作为单独的Mesos应用程序运行，具有自己的驱动程序（master）和工作程序（workers），这些应用程序之间的资源共享由Mesos管理的。Spark可以使用Hadoop现有的输入插件API从任何Hadoop输入源（例如，HDFS或HBase）读取数据，并在未经修改的Scala版本上运行。\n\n我们现在简要介绍系统中几个技术上有趣的部分：我们的作业调度程序（第5.1节），允许交互式使用的Spark解释器（第5.2节），内存管理（第5.3节）和支持检查点（第5.4节）。\n\n### 5.1 作业调度\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060276767/0)\n\nSpark的调度程序使用我们在第4节所述的RDD表示。\n\n总的来说，我们的调度程序类似于Dryad的[19]，但是另外考虑了RDDs在内存中持久化的分区。每当用户在RDD上运行动作（例如，count或save）时，该调度程序就会检查该RDD的lineage图以构建要执行的stage的DAG，如图5所示。每个阶段包含尽可能多的具有窄依赖的流水线转换。阶段的边界是宽依赖所需的shuffle操作，或任何已经计算过的分区，它们可以跳过父RDD的计算。然后，调度程序启动任务以计算每个阶段中丢失的分区，直到计算出目标RDD为止。\n\n我们的调度程序基于数据位置使用延迟调度将任务分配给机器[32]。如果任务需要处理节点内存中可用的分区，我们会将其发送到该节点。否则，如果任务处理包含RDD提供优选位置的分区（例如，HDFS文件），我们将其发送给那些分区。\n\n对于宽依赖关系（即，shuffle依赖关系），我们目前在包含父分区的节点上物化中间记录以简化故障恢复，就像MapReduce物化map输出一样。\n\n如果任务失败，只要其阶段的父项仍然可用，我们就会在另一个节点上重新运行它。如果某些阶段变得不可用（例如，因为来自shuffle的“map side”的输出丢失），我们重新提交任务以并行计算丢失的分区。我们还不能解决调度程序的失败，尽管复制RDD的lineage图会更简单。\n\n最后，尽管Spark中的所有计算当前都是为响应驱动程序中调用的操作而运行的，但我们也在尝试让集群上的任务（例如，映射）调用lookup操作，其提供根据键值对哈希分区的RDDs中的元素进行随机获取。在这种情况下，任务需要告诉调度程序在缺少时计算所需的分区。\n\n### 5.2 解释器整合\n\nScala包含一个类似于Ruby和Python的交互式shell。鉴于内存数据的延迟较低，我们希望让用户从解释器主动运行Spark来查询大数据集。\n\nScala解释器通常通过为用户键入的每一行编译一个类，将其加载到JVM中，并在其上调用函数来操作。该类包含一个单例对象，该对象包含该行上的变量或函数，并在初始化方法中运行行代码。例如，如果用户输入代码var x = 5，接着又输入println(x)，则解释器会定义一个包含x的Line1类，并将第2行编译为println(Line1.getInstance().x)。\n\n在Spark中我们对解释器做了两点改动：\n\n1.类传输：解释器能够支持基于HTTP传输类字节码，这样worker节点就能获取输入每行代码对应的类的字节码。\n\n2.改进的代码生成逻辑：通常每行上创建的单例对象通过对应类上的静态方法进行访问。也就是说，如果要序列化一个闭包，它引用了前面代码行中变量，比如上面的例子Line1.x，Java不会根据对象关系传输包含x的Line1实例。所以worker节点不会收到x。我们将这种代码生成逻辑改为直接引用各个行对象的实例。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060301153/0)\n\n图6显示了在我们更改之后，解释器如何将用户键入的一组行转换为Java对象。\n\nSpark解释器便于跟踪处理大量对象关系引用，并且便利了HDFS数据集的研究。我们计划以Spark解释器为基础，开发提供高级数据分析语言支持的交互式工具，比如SQL。\n\n### 5.3 内存管理\n\nSpark提供了三种持久化RDD的选项：反序列化Java对象的内存存储，序列化数据的内存存储和硬盘存储。第一个选项提供最快的性能，因为Java VM可以原生访问每个RDD元素。第二个选项允许用户在空间有限时选择比Java对象图更高效的内存表示，但代价是性能较低【成本取决于应用程序每个字节数据的计算量，但轻量级处理的最大值可提升2倍】。第三个选项对于太大而无法保留在RAM中但在每次使用时重新计算成本高昂的RDD非常有用\n\n为了管理可用的有限内存，我们在RDDs层面上使用LRU替换策略。当计算新的RDD分区但没有足够的空间来存储它时，我们从最近最少的RDD中替换出一个分区，除它这与具有新分区的RDD相同。在这种情况下，我们将旧分区保留在内存中，以防止来自同一RDD的分区循环进出。这很重要，因为大多数操作都会在整个RDD上运行任务，因此很可能将来需要已经在内存中的分区。到目前为止，我们发现此默认策略在所有应用程序中都能正常运行，但我们还通过每个RDD的“持久化优先级”为用户提供进一步控制。\n\n最后，群集上的每个Spark实例当前都有自己独立的内存空间。在未来的工作中，我们计划通过统一的内存管理器来研究跨Spark实例共享RDDs。\n\n### 5.4 对检查点的支持\n\n尽管在故障之后可以始终使用lineage来恢复RDDs，但对于具有长lineage的RDDs来说，这种恢复可能是耗时的。因此，将一些RDDs checkpoint到稳定存储可能会有所帮助。\n\n通常，检查点技术对于包含宽依赖关系的长lineage图的RDDs很有用，例如我们的PageRank示例（第3.2.2节）中的rank数据集。在这些情况下，集群中的节点故障可能导致每个父RDD丢失一些数据片段，从而需要完全重新计算[20]。相反，对于对稳定存储中的数据具有窄依赖的RDDs，例如我们的逻辑回归示例（第3.2.1节）中的points和PageRank中的link lists，检查点技术可能没多大用。如果节点发生故障，则可以在其他节点上并行重新计算从这些RDD中丢失的分区，而这只是复制整个RDD的成本的一小部分。\n\nSpark目前提供了一个用于检查点技术的API（一个在persist中的REPLICATE标志），但是由用户决定哪些数据使用检查点。但是，我们还在研究如何进行自动检查。因为我们的调度程序知道每个数据集的大小以及首次计算它所花费的时间，所以它应该能够选择一组最佳RDDs来检查点以最小化系统恢复时间[30]。\n\n最后，要强调的一点是，RDDs的只读特性使它们比通用共享存储更容易进行检查。由于一致性问题不需要考虑，因此可以在后台写出RDD，而无需程序暂停或采用分布式快照方案。\n\n## 6 评估\n\n我们通过Amazon EC2上的一系列实验以及用户应用程序的基准评估了Spark和RDDs。总的来说，我们的结果显示如下：\n\n* 在迭代机器学习和图形应用程序中，Spark的性能比Hadoop高出20倍。加速来自于通过将数据作为Java对象存储在内存中来避免I / O和反序列化成本。\n* 我们的用户编写的应用程序可以很好地执行和扩展。特别是，我们使用Spark分析报表比在Hadoop上运行快40倍。\n* 当节点发生故障时，Spark可以通过仅重建丢失的RDD分区来快速恢复。\n* Spark可用于交互查询1 TB数据集，延迟仅为5-7秒。\n\n我们首先与Hadoop进行迭代机器学习（第6.1节）和PageRank（第6.2节）的基准比较。然后，我们评估Spark中的故障恢复（第6.3节）以及数据集不适合存储时的行为（第6.4节）。最后，我们讨论了用户应用（第6.5节）和交互式数据挖掘（第6.6节）的结果。\n\n除非另有说明，否则我们的测试使用m1.xlarge EC2节点，其中包含4个内核和15 GB RAM。我们使用HDFS进行存储，具有256 MB块。在每次测试之前，我们清除了OS缓冲区高速缓存，以准确测量IO成本。\n\n### 6.1 迭代式机器学习应用\n\n我们实现了两个迭代机器学习应用程序，逻辑回归和k-means，以比较以下系统的性能：\n\n* Hadoop：The Hadoop 0.20.2 stable release。\n* HadoopBinMem：在首轮迭代中执行预处理，通过将输入数据转换成为开销较低的二进制格式来减少后续迭代过程中文本解析的开销，在HDFS中加载到内存。\n* Spark：基于RDDs的实现。\n\n我们使用25-100台机器在100 GB数据集上对这两个算法进行了10次迭代。两个应用程序之间的关键区别是它们每个数据字节执行的计算量。k-means的迭代时间由计算决定，而逻辑回归的计算密集度较低，但对反序列化和I / O花费的时间更敏感。\n\n由于典型的学习算法需要数十次迭代才能收敛，因此我们分别报告第一次迭代和后续迭代的时间。我们发现通过RDDs共享数据可以大大加快未来的迭代速度。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1280192908_1557060314454/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060328473/0)\n\n**首次迭代**\t\n所有三个系统在第一次迭代中从HDFS读取文本输入。如图7中的浅色柱状图所示，Spark在实验中比Hadoop要快一些。如图7中的灯条所示，Spark在实验中比Hadoop要快一些。HadoopBinMem是最慢的，因为它通过一个额外的MapReduce作业将数据转换成二进制格式，并必须通过网络在HDFS结点间复制数据。\n\n**后续迭代**\t\n图7还显示了后续迭代的平均运行时间，而图8显示了其随集群大小变化而产生的时间变化。对于逻辑回归，Spark在100台机器上分别比Hadoop和HadoopBinMem快25.3倍和20.7倍。对于更加计算密集型的k-means应用程序来说，Spark仍然实现了1.9倍到3.2倍的加速。\n\n**理解速度提升**\t\n我们惊讶地发现Spark甚至比内存存储二进制数据的Hadoop（HadoopBinMem）还要快20倍。在HadoopBinMem中，我们使用了Hadoop的标准二进制格式（SequenceFile）和256 MB大小的超大块，并且我们强制HDFS的数据目录位于内存文件系统中。但是，由于以下几个因素，Hadoop仍然运行缓慢：\n\n1.Hadoop软件堆栈的最小开销，\n\n2.提供数据时HDFS的开销，\n\n3.将二进制记录转换为可用的内存中Java对象的反序列化成本。\n\n我们依次研究了这些因素。为了估测1，我们运行空的Hadoop作业，仅仅执行作业的初始化、启动任务、清理工作就至少耗时25秒。对于2，我们发现为了服务每一个HDFS数据块，HDFS进行了多次复制以及计算校验和操作。\n\n最后，为了估测3，我们在一台机器上运行微基准测试，以256 MB输入的各种格式运行逻辑回归计算。特别是，我们比较了处理来自HDFS（HDFS堆栈中的开销将给出）和内存本地文件（内核可以非常有效地将数据传递给程序）的文本和二进制输入的时间。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060341567/0)\n\n结果如图9所示。内存中HDFS和本地文件之间的差异表明，通过HDFS读取产生了2秒的开销，即使数据存储在本地机器上也是如此。文本和二进制输入之间的差异表明解析开销为7秒。最后，即使从内存文件中读取，将预解析的二进制数据转换为Java对象也需要3秒钟，这仍然几乎与逻辑回归本身一样开销高昂。通过将RDD元素直接存储为内存中的Java对象，Spark可以避免所有这些开销。\n\n### 6.2 PageRank\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060351959/0)\n\n我们使用54 GB Wikipedia导出数据比较了Spark与Hadoop 进行PageRank的性能。PageRank算法通过10轮迭代处理了大约400万文章的链接图数据。图10展示了单独的内存存储使Spark在30个节点上的速度比Hadoop提高了2.4倍。此外，如第3.2.2节所述，控制RDD的分区以使其在迭代中保持一致，将提高到7.4倍。加速也能几乎线性地扩展到60个节点上。\n\n我们还评估了使用Spark实现Pregel版本的PageRank的性能，结果将在7.1节展示。迭代时间与图10中的相似，但是更长约4秒，因为Pregel在每次迭代时运行一个额外的操作，让顶点“投票”是否完成工作。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060363143/0)\n\n### 6.3 错误恢复\n\n我们评估了k-means应用程序中节点故障后使用lineage重建RDD分区的开销。图11比较了正常操作场景中75节点集群上k-means的10次运行的运行时间，其中一个节点在第6次迭代开始时失败。另一个没有任何故障，每次迭代包含400个任务，处理100 GB的数据。\n\n直到第5次迭代结束，迭代时间约为58秒。在第6次迭代中，其中一台机器被杀死，导致在该机器上运行的任务和存储在那里的RDD分区丢失。Spark在其他机器上并行重新执行这些任务，他们通过lineage重新读取相应的输入数据和重建的RDD，导致操作时间增加到80秒。一旦重建丢失的RDD分区，迭代时间就会回落到58秒。\n\n请注意，使用基于检查点的故障恢复机制，恢复可能需要重新运行至少几次迭代，具体取决于检查点的频率。此外，系统需要通过网络复制应用程序的100 GB工作集（文本输入数据转换为二进制），并且要么消耗两倍于Spark的内存以将其复制到RAM中，要么必须等待写入100 GB到磁盘。相比之下，我们示例中RDD的lineage图的大小都小于10 KB。\n\n### 6.4 内存不足时表现\n\n到现在为止，我们能保证集群中的每个节点都有足够的内存去缓存迭代过程中使用的RDDs。一个自然的问题是，如果没有足够的内存来存储作业的数据，Spark是如何运行的。在本实验中，我们将Spark配置为不使用超过一定百分比的内存来在每台机器上存储RDD。图12中，我们为的逻辑回归提供了各种存储空间的配置。我们发现性能在控件降低时缓慢降低。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060374238/0)\n\n### 6.5 用Spark构建的用户应用程序\n\n**内存分析**\t\nConviva Inc是一家视频发行公司，它使用Spark加速了之前在Hadoop上运行的大量数据分析报告。例如，一份报告作为一系列Hive [1]查询运行，这些查询计算出了客户的各种统计信息。这些查询都在相同的数据子集上工作（与客户提供的过滤器匹配的记录），但在不同的分组字段上执行了聚合（平均值，百分位数和COUNT DISTINCT），需要使用单独的MapReduce作业。通过在Spark中实现查询并将一次共享的数据子集加载到RDD中，该公司能够将报告速度提高40倍。在Hadoop集群上花费20小时的200 GB压缩数据的报告现在仅使用两台Spark计算机就能在30分钟内运行完成。此外，Spark程序只需要96 GB的RAM，因为它只存储与RDD中客户的过滤器匹配的行和列，而不是整个解压缩文件。\n\n**城市交通模型**\t\n在Berkeley的Mobile Millennium项目[18]中，基于一系列分散的汽车GPS监测数据，研究人员使用并行化机器学习算法来推算公路交通拥堵状况。数据来自市区10000个互联的公路线路网，还有600000个由汽车GPS装置采集到的样本数据，这些数据记录了汽车在两个地点之间行驶的时间（每一条路线的行驶时间可能跨多个公路线路网）。使用一个交通模型，通过推算跨多个公路网行驶耗时预期，系统能够估算拥堵状况。研究人员使用Spark实现了一个可迭代的EM算法，其中包括向Worker节点广播路线网络信息，在E和M阶段之间执行reduceByKey操作，应用从20个节点扩展到80个节点（每个节点4核），如图13（a）所示：\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060384098/0)\n\n**推特垃圾邮件分类**\nBerkeley的Monarch项目[29]使用Spark识别Twitter消息上的垃圾链接。他们在Spark上实现了一个类似6.1小节中示例的Logistic回归分类器，不同的是使用分布式的reduceByKey操作并行对梯度向量求和。图13（b）显示了基于50G数据子集训练训练分类器的结果，整个数据集是250000的URL、至少10^7个与网络相关的特征/维度，内容、词性与访问一个URL的页面相关。随着节点的增加，这并不像交通应用程序那样近似线性，主要是因为每轮迭代的固定通信代价较高。\n\n### 6.6 交互式数据挖掘\n\n为了演示Spark交互式查询大数据集的能力，我们用它来分析1TB的维基百科页面浏览日志（2年的数据）。在本次实验中，我们使用了100 m2.4xlarge EC2实例，每个实例有8个内核和68 GB内存。在整个输入数据集上简单地查询如下内容以获取页面浏览总数：（1）全部页面；（2）页面的标题能精确匹配给定的关键词；（3）页面的标题能部分匹配给定的关键词。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060393080/0)\n\n图14显示了分别在整个、1/2、1/10的数据上查询的响应时间，甚至1TB数据在Spark上查询仅耗时5-7秒，这比直接操作磁盘数据快几个数量级。例如，从磁盘上查询1TB数据耗时170秒，这表明了RDD缓存使得Spark成为一个交互式数据挖掘的强大工具。\n\n## 7 讨论\n\n虽然RDD由于其不可变性和粗粒度转换似乎提供有限的编程接口，但我们发现它们适用于广泛的应用。特别地，RDDs可以表达数量惊人的集群编程模型，这些模型迄今已被提议作为单独的框架，允许用户在一个程序中组合这些模型（例如，运行MapReduce操作来构建图形，然后在其上运行Pregel）并在它们之间共享数据。在本节中，我们将讨论RDDs可以表达哪些编程模型以及它们如此广泛适用的原因（第7.1节）。此外，我们讨论了我们正在探索的RDDs中lineage信息的另一个好处，能够促进这些模型的调试（第7.2节）。\n\n### 7.1 表达现有的编程模型\n\nRDDs可以有效地表达迄今为止独立提出的许多集群编程模型。“有效”，意味着RDDs不仅可以用于生成与这些模型中编写的程序相同的输出，而且RDDs还可以实现这些框架具有的优化，例如将特定数据保存在内存中，将其分区为最大限度地减少通信，并有效地从故障中恢复。使用RDD表达的模型包括：\n\n**MapReduce：** 可以使用Spark中的flatMap和groupByKey操作表示此模型，如果存在组合器，则可以使用reduceByKey表示.\n\n**DryadLINQ：** \n与更普遍的Dryad运行时相比，DryadLINQ系统提供了比MapReduce更广泛的运算符，但这些都是直接对应于Spark（map，groupByKey，join等）中可用的RDD转换的批量运算符。\n\n**SQL：** \n与DryadLINQ表达式一样，SQL查询对记录集执行数据并行操作。\n\n**Pregel：**\nGoogle的Pregel [22]是迭代图形应用程序的专用模型，起初与其他系统中的面向集合的编程模型完全不同。在Pregel中，程序作为一系列协调的“supersteps”运行。在每个supersteps中，图中的每个顶点都运行一个用户函数，可以更新与顶点相关的状态，更改图形拓扑，并将消息发送到其他顶点用于下一个superstep。该模型可以表达许多图算法，包括最短路径，二分匹配和PageRank。\n\n让我们用RDDs实现这个模型的关键点是Pregel将相同的用户函数应用于每次迭代的所有顶点。因此，我们可以将每次迭代的顶点状态存储在RDD中，并执行批量转换（flatMap）以应用此函数并生成消息的RDD。之后可以将该RDD与顶点状态连接操作以表示消息的转换。同样重要的是，RDDs允许我们像Pregel那样将顶点状态保存在内存中，通过控制其分区来最小化通信，并支持故障时的部分恢复。我们在Spark上实现了Pregel，其作为200行库，并向读者推荐[33]以获取更多详细信息。\n\n迭代式MapReduce： 最近提出的几个系统，包括HaLoop [7]和Twister [11]，提供了迭代的MapReduce模型，用户可以为系统提供一系列MapReduce作业循环执行。系统使数据在迭代中保持一致，Twister也可以将其保存在内存中。两种优化都很容易用RDDs表达，我们能够使用Spark将HaLoop实现为200行库。\n\n流式批处理： 研究人员最近提出了几种增量处理系统，用于定期用新数据更新结果的应用[21,15,4]。例如，每15分钟更新一次广告点击统计数据的应用程序应该能够将前一个15分钟窗口的中间状态与新日志中的数据相结合。这些系统执行类似于Dryad的批量操作，但将应用程序状态存储在分布式文件系统中。将中间状态置于RDDs中将加速其处理。\n\n**_解释RDDs的表达性能_**\t\n为什么RDD能够表达这些不同的编程模型？原因是对RDD的限制对许多并行应用程序几乎没有影响。特别是，尽管RDDs只能通过批量转换创建，但许多并行程序本质上就是将相同的操作应用于许多记录，所以使其易于用RDDs表达。类似地，RDD的不变性不是一个障碍，因为可以创建多个RDD来表示同一数据集的不同版本。实际上，今天的许多MapReduce应用程序都运行在不允许更新文件的文件系统上，例如HDFS。\n\n最后一个问题是为什么以前的框架没有提供相同的一般性。我们认为这是因为这些系统探索了MapReduce和Dryad无法很好处理的特定问题，例如迭代，但是没有观察到这些问题的常见原因是缺乏数据共享抽象。\n\n### 7.2 利用RDDs进行调试\n\n虽然我们最初设计的RDD在确定性方面可以重新计算以实现容错，但这个属性也可以简化调试。特别是，通过记录在作业期间创建的RDD的lineage，可以（1）稍后重建这些RDD并让用户以交互方式查询它们，以及（2）在单个过程调试中重新执行作业的任何任务，通过重新计算它所依赖的RDD分区。与通用分布式系统[13]的传统重放调试器不同，不必捕获或推断跨多节点的时间顺序，这种方法几乎不增加记录开销，因为只需要记录RDD lineage图【9 与这些系统不同，基于RDD的调试器不会重放用户函数中的非确定性行为（例如，非确定性映射），但它至少可以通过校验和数据来报告它】。我们目前正在开发基于这些想法的Spark调试器[33]。\n\n## 8 相关工作\n\n**集群编程模型：** \n集群编程模型的相关工作分为几类。首先，MapReduce [10]，Dryad [19]和Ciel [23]等数据流模型支持丰富的运算符集，用于处理数据，但通过稳定的存储系统共享数据。RDDs代表比稳定存储更有效的数据共享抽象，因为它们避免了数据复制，I / O和序列化的成本【10 请注意，在像RAMCloud [25]这样的内存数据存储中运行MapReduce / Dryad仍然需要数据复制和序列化，这对于某些应用程序来说可能代价很高，如6.1节所示】。\n\n其次，数据流系统的几个高级编程接口，包括DryadLINQ [31]和FlumeJava [8]，提供了语言集成的API，用户通过map和join等操作符操作“并行集合”。但是，在这些系统中，并行集合表示磁盘上的文件或用于表示查询计划的临时数据集。虽然系统会在相同的操作符查询间流水式地处理数据（如，一个map操作接着一个map操作），但是它们并不能在各个查询之间有效地共享数据。我们在并行收集模型上基于Spark的API，因为它的方便性，并没有增加语言集成接口的新颖性，但通过提供RDD作为此接口背后的存储抽象，我们允许它支持更广泛的应用程序。\n\n第三类系统为需要数据共享的特定类别的应用程序提供高级接口。例如，Pregel [22]支持迭代图应用，而Twister [11]和HaLoop [7]是迭代MapReduce运行时。但是，这些框架隐式地为他们支持的计算模式提供数据共享，并且不提供一般抽象来供用户选择。例如，用户不能使用Pregel或Twister将数据集加载到内存中，然后决定在其上运行哪个查询，RDD明确地提供分布式存储抽象，因此可以支持这些专用系统不支持的应用，例如交互式数据挖掘。\n\n最后，一些系统暴露共享可变状态以允许用户执行内存计算。例如，Piccolo [27]允许用户运行并行功能以读取和更新分布式哈希表中的单元格。分布式共享存储（DSM）系统[24]和键值存储如RAMCloud [25]提供一个相似的模型。RDD在两个方面与这些系统不同。首先，RDD基于运算符（如map，sort和join）提供更高级别的编程接口，而Piccolo和DSM中的接口只是对表格单元格的读取和更新。其次，Piccolo和DSM系统通过检查点和回滚实现恢复，这比许多应用程序中基于lineage策略的RDDs更昂贵。最后，正如第2.3节所讨论的那样，RDD还提供了其他优于DSM的优势，例如straggler缓解。\n\n**缓存系统：** \nNectar [12]可以通过程序分析识别常见的子表达式，在DryadLINQ作业中重用中间结果[16]。这种能力对于添加到基于RDD的系统非常有吸引力。但是，Nectar不提供内存中缓存（它将数据放在分布式文件系统中），也不允许用户明确控制要持久化的数据集以及如何对它们进行分区。Ciel [23]和FlumeJava [8]同样可以缓存任务结果，但不提供内存缓存或显式控制缓存哪些数据。Ananthanarayanan等。建议在分布式文件系统中添加内存缓存，以利用数据访问的时间和空间局部性[3]。虽然此解决方案可以更快地访问文件系统中已有的数据，但它不像在RDDs中那样有效地在一个应用中共享中间结果，因为它仍然需要应用程序在不同阶段间将这些结果写入文件系统。\n\n**Lineage：**\n记载数据的lineage或起源信息长期以来一直是科学计算和数据库中的研究课题，应用于解释结果，允许数据可以被别的数据重建，同时如果在工作流中出现了bug或者数据及丢失了，能够重新计算得到数据。我们推荐读者阅读[5]和[9]来了解这些工作。RDDs提供并行编程模型，其中获取细粒度的lineage成本低廉，因此可用于故障恢复。\n\n我们基于lineage的恢复机制也类似于MapReduce和Dryad中计算（作业）中使用的恢复机制，它跟踪任务的DAG之间的依赖关系。但是，在这些系统中，谱系（lineage）信息在作业结束后丢失，需要使用复制的存储系统来跨计算共享数据。相比之下，RDDs应用lineage来有效地跨计算保留内存数据，而无需复制和磁盘I / O的成本。\n\n**关系型数据库：**\nRDDs在概念上类似于数据库中的视图，而持久化RDDs类似于物化视图[28]。但是，与DSM系统一样，数据库通常允许对所有记录进行细粒度的读写访问，需要记录操作和数据以实现容错，并且需要额外的开销来维护一致性。RDD的粗粒度转换模型不需要这些开销。\n\n## 9 结论\n\n我们提供了弹性分布式数据集（RDDs），这是一种高效，通用和容错的用于在集群应用程序中共享数据的抽象。RDDs可以表达各种并行应用程序，包括已经提出用于迭代计算的许多专用编程模型，以及这些模型还未实现的新应用程序。与现有的集群存储抽象（需要数据复制以实现容错）不同，RDDs提供基于粗粒度转换的API，使其能够使用lineage来有效地恢复数据。我们在Spark中实现了RDDs，它在迭代应用程序中的性能比Hadoop高出20倍，并且可以交互式查询数百GB的数据。\n\n我们在spark-project.org上提供了开源Spark作为可扩展数据分析和系统研究的工具。\n\n## 致谢\n\n我们感谢第一批Spark用户，包括Tim Hunter，Lester Mackey，Dilip Joseph和Jibin Zhan，他们在实际应用中尝试我们的系统，提供了许多好的建议，并指出了一些研究中的挑战。我们还要感谢我们的指导者Ed Nightingale以及审核的反馈。这项研究部分由Berkeley AMP Lab \n支持，由 Google, SAP, Amazon Web Services, Cloudera, **_Huawei_**, IBM, Intel, Microsoft, NEC, NetApp 和 VMWare，DARPA，the Natural Sci- ences 和 Engineering Research Council of Canada赞助。\n\n## 引用\n\n[1] Apache Hive. http://hadoop.apache.org/hive.\n\n[2] Scala. http://www.scala-lang.org.\n\n[3] G.Ananthanarayanan,A.Ghodsi,S.Shenker,andI.Stoica.\nDisk-locality in datacenter computing considered irrelevant. In\nHotOS ’11, 2011.\n\n[4] P.Bhatotia,A.Wieder,R.Rodrigues,U.A.Acar,and\nR. Pasquin. Incoop: MapReduce for incremental computations.\nIn ACM SOCC ’11, 2011.\n\n[5] R.BoseandJ.Frew.Lineageretrievalforscientificdata\nprocessing: a survey. ACM Computing Surveys, 37:1–28, 2005.\n\n[6] S.BrinandL.Page.Theanatomyofalarge-scalehypertextual\nweb search engine. In WWW, 1998.\n\n[7] Y.Bu,B.Howe,M.Balazinska,andM.D.Ernst.HaLoop:\nefficient iterative data processing on large clusters. Proc. VLDB\nEndow., 3:285–296, September 2010.\n\n[8] C.Chambers,A.Raniwala,F.Perry,S.Adams,R.R.Henry,\nR. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient\ndata-parallel pipelines. In PLDI ’10. ACM, 2010.\n\n[9] J.Cheney,L.Chiticariu,andW.-C.Tan.Provenancein\ndatabases: Why, how, and where. Foundations and Trends in\nDatabases, 1(4):379–474, 2009.\n\n[10] J.DeanandS.Ghemawat.MapReduce:Simplifieddata\nprocessing on large clusters. In OSDI, 2004.\n\n[11] J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu, and G. Fox. Twister: a runtime for iterative mapreduce. In HPDC ’10, 2010.\n\n[12] P.K.Gunda,L.Ravindranath,C.A.Thekkath,Y.Yu,and L. Zhuang. Nectar: automatic management of data and computation in datacenters. In OSDI ’10, 2010.\n\n[13] Z.Guo,X.Wang,J.Tang,X.Liu,Z.Xu,M.Wu,M.F. Kaashoek, and Z. Zhang. R2: an application-level kernel for record and replay. OSDI’08, 2008.\n\n[14] T.Hastie,R.Tibshirani,andJ.Friedman.TheElementsof Statistical Learning: Data Mining, Inference, and Prediction. Springer Publishing Company, New York, NY, 2009.\n\n[15] B.He,M.Yang,Z.Guo,R.Chen,B.Su,W.Lin,andL.Zhou. Comet: batched stream processing for data intensive distributed computing. In SoCC ’10.\n\n[16] A.Heydon,R.Levin,andY.Yu.Cachingfunctioncallsusing precise dependencies. In ACM SIGPLAN Notices, pages 311–320, 2000.\n\n[17] B.Hindman,A.Konwinski,M.Zaharia,A.Ghodsi,A.D. Joseph, R. H. Katz, S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI ’11.\n\n[18] T.Hunter,T.Moldovan,M.Zaharia,S.Merzgui,J.Ma,M.J. Franklin, P. Abbeel, and A. M. Bayen. Scaling the Mobile Millennium system in the cloud. In SOCC ’11, 2011.\n\n[19] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys ’07, 2007.\n\n[20] S.Y.Ko,I.Hoque,B.Cho,andI.Gupta.Onavailabilityof intermediate data in cloud computations. In HotOS ’09, 2009.\n\n[21] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum. Stateful bulk processing for incremental analytics. SoCC ’10. [22] G.Malewicz,M.H.Austern,A.J.Bik,J.C.Dehnert,I.Horn,\nN. Leiser, and G. Czajkowski. Pregel: a system for large-scale\ngraph processing. In SIGMOD, 2010.\n\n[23] D.G.Murray,M.Schwarzkopf,C.Smowton,S.Smith,\nA. Madhavapeddy, and S. Hand. Ciel: a universal execution\nengine for distributed data-flow computing. In NSDI, 2011. \n\n\n[24] B.NitzbergandV.Lo.Distributedsharedmemory:asurveyof\nissues and algorithms. Computer, 24(8):52 –60, Aug 1991. \n\n[25] J.Ousterhout,P.Agrawal,D.Erickson,C.Kozyrakis,\nJ. Leverich, D. Mazie`res, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman. The case for RAMClouds: scalable high-performance storage entirely in DRAM. SIGOPS Op. Sys. Rev., 43:92–105, Jan 2010.\n\n[26] D.PengandF.Dabek.Large-scaleincrementalprocessingusing distributed transactions and notifications. In OSDI 2010.\n\n[27] R.PowerandJ.Li.Piccolo:Buildingfast,distributedprograms with partitioned tables. In Proc. OSDI 2010, 2010.\n\n[28] R.RamakrishnanandJ.Gehrke.DatabaseManagement Systems. McGraw-Hill, Inc., 3 edition, 2003.\n\n[29] K.Thomas,C.Grier,J.Ma,V.Paxson,andD.Song.Designand evaluation of a real-time URL spam filtering service. In IEEE Symposium on Security and Privacy, 2011.\n\n[30] J.W.Young.Afirstorderapproximationtotheoptimum checkpoint interval. Commun. ACM, 17:530–531, Sept 1974.\n\n[31] Y.Yu,M.Isard,D.Fetterly,M.Budiu,U ́.Erlingsson,P.K. Gunda, and J. Currey. DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language. In OSDI ’08, 2008.\n\n[32] M.Zaharia,D.Borthakur,J.SenSarma,K.Elmeleegy,\nS. Shenker, and I. Stoica. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys ’10, 2010.\n\n[33] M.Zaharia,M.Chowdhury,T.Das,A.Dave,J.Ma,\nM. McCauley, M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. Technical Report UCB/EECS-2011-82, EECS Department, UC Berkeley, 2011.\n\n\n\n\n\n\n\n\n    \n\n    ","source":"_posts/tanslate-Resilient-Distributed-Datasets-A-Fault-Tolerant-Abstraction–for-In-Memory-Cluster-Computing.md","raw":"---\ntitle: 【翻译】弹性分布式数据集：基于内存的集群计算的容错性抽象\ndate: 2019/04/13\ncategories:\n    - 毕业设计\ntags:\n    - Spark\n    - RDD\n---\n## context\n大概用了16个小时完成了这篇关于RDD论文的翻译，这篇论文奠定了Spark的设计基础。\n\n原文:[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n\n推荐先看林子雨老师关于RDD的[解释](http://dblab.xmu.edu.cn/blog/985-2/)\n\n## 摘要\n本文提出了一个分布式内存抽象的概念--弹性分布式数据集（Resilient Distributed Datasets，以下称为RDDs），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDDs的提出是由于现有的两种计算框架并不高效：迭代式算法和交互式数据挖掘工具。在内存中操作数据可以将前两种计算方式的效率提高一个数量级。RDDs提供了一种受限的共享内存，是基于粗粒度的转换操作而不是细粒度的状态同步。尽管如此，RDDs依然能够表示多种类型的计算，包括专用的迭代编程模型（如Pregel）和一些新的应用模型。我们通过在Spark上评估各种应用和基准，实现了RDDs。\n\n## 1 引言\n集群计算框架，如MapReduce[10]和Dryad[19]，已经被广泛地运用于大规模数据分析。这些系统能够让用户在不用考虑任务调度和容错的前提下，使用一系列高级的操作进行并行计算。\n\n虽然这些框架为获取集权计算资源提供了大量的抽象，但是缺少对分布式内存的运用。这使他们对于一类新兴的应用十分低效：它们在不同的计算阶段*重用*中间结果。数据重用在*迭代式*机器学习和图算法中十分常见，包括PageRank、K-means聚类和逻辑回归。另一个明显的用例是*交互式*数据挖掘，用户在同样的数据子集上进行ad-hoc查询。然而不好的是，对于现在的框架，在不同计算阶段之间重用数据（如，在两个MapReduce的job之间）的唯一方式是将其写入外部稳定存储系统中，如，分布式文件系统。由于数据的复制、硬盘I/O和序列化，导致了大量的成本开销，并占据了应用运行的大部分时间。\n\n在意识到这个问题之后，研究人员针对需要数据重用的应用开发了专门的框架。比如，迭代式图计算系统Pregel[22]，其将中间数据存放在内存中，而HaLoop[7]提供了一种迭代式MapReduce接口。无论如何，这些框架仅支持特定的计算模式（如，循环一系列的MapReduce步骤），并隐式地提供这些模式的数据共享。它们没有提供更加通用的数据重用的抽象，如，让用户直接向内存中装载数据集，并对其进行ad-hoc查询。\n\n在这篇论文中，我们提出了一种能够广泛运用于各种应用中高效的数据重用抽象，_弹性分布式数据集_（RDDs）。RDDs是一个容错的、并行的数据结构，能够让用户明确地在内存中持久化中间结果，控制其分区以优化数据的放置和使用丰富的操作符对其进行处理。\n\n设计RDDs的主要挑战是，定义一个能够高效容错能力的编程接口。现有的基于集群的内存存储抽象，如分布式共享内存、键值对存储、数据库和Piccolo，提供了基于细粒度更新可变状态的接口（如，表中的cell）。运用这些接口时，达到数据容错的唯一方式是在不同的机器之间进行数据的冗余或者记录更新日志。这两种方法对于数据密集型工作来说代价高昂，因为他们需要在集群网络间复制大量数据，而网络带宽远小于RAM带宽，同时还会产生大量数据存储开销。\n\n相比于这些系统，RDDs提供基于粗粒度转换的，可用于大量数据项进行相同操作的接口（如，map,filter和join）。这使得RDDs能够通过记录产生数据集的一系列转换操作（称之为lineage），而不是记录真实的数据，来提高容错的效率【1当lineage链过长时，对一些RDDs进行检查点（checkpoint）设置可能更加有用，我们在5.4节对其进行讨论】。如果一个分区的RDD丢失了，它有足够的信息知道自己是如何从其他的RDD产生的，从而重新计算该分区。因此，丢失的数据可以很快地恢复，而不需要代价昂贵的复制。\n\n基于粗粒度转换的接口乍一看是有局限性的，但RDDs对于许多并行应用都非常适用，因为这些应用本质上就是会对多种数据项进行相同的操作。为此，我们证明了RDDs高效地运用于表达各种已经被现有的分布式系统所实现的集群编程模型，包括MapReduce，DryadLINQ,，SQL，Pregel 和 HaLoop，以及这些系统无法实现的新应用，如交互式数据挖掘。RDDs能够作为被用来解决前面提到的计算需求而引入的新框架的证据，是因为其强大的抽象能力。\n\n我们已经在一个被用于UC Berkeley的实验环境和许多公司生产环境下的应用--Spark上，实现了RDDs。Spark提供了一个运用Scala语言，类似DryadLINQ的易用的语言集成编程接口。另外，Spark还可以在Scala解释器中进行交互式大数据集的查询。我们相信Spark会是第一个运用通用编程语言完成交互式速度下集群内存数据挖掘的系统。\n\n我们通过微基准测试和用户应用对RDDs和Spark进行评估。我们得出，Spark在迭代式应用上比Hadoop快20倍，在真实数据报表分析上快40倍，并且能够在5-7秒的延迟内完成1TB数据集的交互式扫描。更加根本地，为了说明RDDs的通用性，我们在Spark上实现了Pregel和HaLoop的编程模型，包括用一些相对较小的库（每个库大概200行代码）实现它们所采用存储优化策略。\n\n这篇论文首先介绍RDDs的概览（第2部分）和Spark（第3部分），然后讨论RDDs的内部表示（第4部分），实现（第5部分），和一些实验结果（第6部分）。最后，我们讨论了用RDDs实现几个现有的集群编程模型（第7部分），相关研究工作（第8部分）和总结。\n\n## 2 弹性分布式数据集（RDDs）\n\n这一部分提供关于RDDs的概述。先定义RDDs（2.1节），介绍其在Spark中的编程接口（2.2节）。然后将RDDs与细粒度共享内存抽象进行比较（2.3节）。最后讨论RDD模型的限制（2.4节）。\n\n### 2.1 RDD抽象\n\n一个RDD是只读的，是将记录进行分区的集合。RDDs只能由（1）稳定物理存储中的数据集（2）其他RDDs通过明确的操作产生。我们称这些操作为转换（transformations）以区别其他对RDDs的操作。转换的例子包括map，filter和join。【2 虽然单个RDDs是不可变的，但是可以通过多个RDDs来表示不同版本的数据集以实现多状态。我们让RDDs不可变以使lineage图表示更简便，但这也相当于将我们的抽象变成版本化数据并在lineage图中追踪不同版本】\n\nRDDs不需要都实体化。一个RDD有足够的信息了解自己是如何从其他数据集产生的（lineage）并通过信息从稳定的物理存储中计算出自己的分区。这一强大特性的本质是，程序能够在RDD重建之后对其进行引用。\n\n最后一点，用户可以对RDDs进行2方面的控制：持久化和分区。用户可以表明将要重用的RDDs并为其选择存储策略（如，内存存储）。也可以将RDDs的元素通过特定键值进行分区。这些功能对于存储优化特别有用，比如保证两个将要进行join操作的数据集都进行了相同的哈希分区。\n\n### 2.2 Spark编程接口\n\nSpark暴露了类似DryadLINQ和FlumeJava的RDDs语言集成API，每个数据集都被表示成一个对象，并通过执行方法在这些对象上进行转换操作（transformations）。\n\n开发人员通过将物理存储上的数据集进行转换（如，map和filter）来定义一个或多个RDDs。然后可以用动作（actions）对这些RDDs进行操作，这些操作向应用返回值或者向存储系统产生外部数据。动作（actions）的例子包括，count（返回数据集中元素的数目），collect（返回元素本身）和save（将数据集输出到外部存储系统）。像DryadLINQ一样，Spark在遇到一个动作操作时才会真正计算出RDDs，所以其可以对转换（transformations）进行流水线操作。\n\n此外，开发人员还可以调用persist方法来声明他们想重用的RDDs。Spark默认会将RDDs留存在内存中，但是会在没有足够RAM的情况下将它们溢出到硬盘。用户可以采用其他的持久化策略，如通过persist标价，将特定RDD只存在硬盘上或在机器之间进行备份。最后，用户可以为每个RDD设置优先级来表明当需要时，将哪一个内存中的数据溢出到硬盘中。\n\n#### 2.2.1 例子：控制台日志挖掘\n\n假设一个网页服务出现错误，管理员想在HDFS中兆字节规模的日志中找到原因。应用Spark，管理员能够将错误信息从多个结点的日志中导入RAM，并交互式的进行查询。她会键入以下代码：\n\n```scala\nlines = spark.textFile(\"hdfs://...\")\nerrors = lines.filter(_.startsWith(\"ERROR\"))\nerrors.persist()\n```\n第一行从一个HDFS文件（文本行集合）定义了一个RDD，并在第二行产生过滤后的RDD。\n\n第三行将errors持久化在内存中，这样就能被查询。注意filter的参数是一个闭包的Scala语法。\n\n到目前为止，并没有在集群上运行作业。但是，用户可以对RDD进行动作（actions）操作，如计算消息的数目：\n\n`errors.count()`\n\n这位用户也可以对该RDD进行进一步的转换操作并运用他们的结果，如下：\n\n```scala\n// Count errors mentioning MySQL:\nerrors.filter(_.contains(\"MySQL\")).count()\n// Return the time fields of errors mentioning\n// HDFS as an array (assuming time is field\n// number 3 in a tab-separated format):\nerrors.filter(_.contains(\"HDFS\"))\n      .map(_.split(’\\t’)(3))\n      .collect()\n```\n在第一个对errors进行动作操作之后，Spark将在内存中村塾errors的各分区，这极大地加快了下游的计算操作。注意，最原始的RDD，lines，不会加载到RAM中。这是可取的，因为错误消息可能只是数据的一小部分（小到足够放进内存）。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060102174/0)\n\n最后为了说明模型是如何实现容错的，图1展示了该RDDs的lineage图。在这次查询中，我们首先对lines进行过滤得到errors，然后在运行collect操作前进行更进一步的filter和map操作。Spark调度器将会流水执行后两个转换操作并向缓存了errors分区的结点发送任务的集合来对其进行计算。除此之外，如果errors的一个分区丢失了，Spark只会在相关的lines分区上进行过滤操作来重建丢失的分区。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060116532/0)\n\n### 2.3 RDD模型的优势\n\n为了理解RDDs作为分布式内存抽象的优势，表1将其与分布式共享内存（DSM）进行了比较。在DSM系统中，应用对全局地址空间进行随意位置的读写。请注意，根据此定义，我们不仅包含了传统的共享内存系统[24]，还包括应用可以进行细粒度写共享状态的其他系统，如Piccolo[27]，其提供共享DHT和分布式数据库。DSM是非常通用的抽象，但是这样的通用性使其很难以一种高效且容错的方式运用在商业集群上。\n\nRDDs与DSM的主要不同是，RDDs只能通过粗粒度的转换操作产生（“写”），而DSM允许在内存中任意位置读写【注意对RDDs的读仍然可以是细粒度的。例如，应用系统可以将一个RDD视为一个大型只读查找表】。这会限制RDDs为执行批量写入的应用程序，但是这使得其具有高效的容错性。特别是，RDD不需要产生检查点（checkpoint）的开销，因为它们可以使用lineage来恢复【在一些应用中，在具有很长lineage链的RDDs中仍然可以使用checkpoint技术，我们将在5.4节讨论。但是，这些可以在后台完成，因为RDDs是不可变的，并且不需要像在DSM中那样保留整个应用程序的快照】。而且，只有丢失的RDD分区才需要在失败时重新计算，并且它们可以在不同的节点上并行计算，而不必回滚整个程序。\n\nRDDs的第二个优势是，由于它们不可变的特性，通过运行缓慢任务的副本来缓解慢结点对系统拖拽的压力，就和MapReduce一样[10]。使用DSM很难实现备份任务，因为任务的两个副本将访问相同的内存位置并干扰彼此的更新。\n\n最后，RDDs提供了DSM没有的两个其他好处。其一，在对RDDs的批量操作中，一个运行时可以基于数据的位置进行任务调度以提高性能。其二，当没有足够的内存来存储RDDs时，它就会优雅地降级，使它们仅用于扫描操作。不适合RAM的分区可以存储在磁盘上，并提供与当期数据并行系统类似的性能。\n\n### 2.4 不适合RDDs的应用\n\n正如引言中所讨论的，RDDs非常适用于对数据集中所有元素进行相同操作的批处理应用。在这些情况下，RDDs可以有效地将每个转换（transformations）记录为lineage图中的一个步骤，并且可以在不记录大量数据的情况下恢复丢失的分区。RDDs不太适合对共享状态进行异步细粒度共享状态更新的应用程序，例如Web应用程序的存储系统或增量Web爬网程序。对于这些应用程序，使用执行传统更新日志记录和checkpoint的系统更有效，例如数据库，RAMCloud [25]，Percolator [26]和Piccolo [27]。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。\n\n## 3 Spark编程接口\n\nSpark通过Scala [2]提供类似于DryadLINQ [31]语言集成API的RDDs抽象，Scala [2]是基于Java VM的静态类型函数编程语言。我们之所以选择Scala，是因为它结合了简洁（便于交互使用）和效率（静态类型）。但是，关于RDD抽象的任何内容都不需要函数式语言\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060128798/0)\n\n要使用Spark，开发人员编写一个连接到一组workers的driver程序，如图2所示。驱动程序定义一个或多个RDDs并在其上调用动作操作。驱动程序上的Spark代码也会追踪RDDs的lineage。这些workers是长期存在的过程，能够跨操作在RAM中存储RDDs分区。\n\n正如我们在2.2.1中的日志挖掘例子中所展示，用户想如map这样的RDD操作传递闭包（函数形式）来提供参数。Scala将每个闭包表示为Java对象，这些对象可以序列化并加载到另一个节点上，以通过网络传递闭包。Scala也将在闭包中绑定的任何变量作为Java对象的域。比如，可以编写代码：var x = 5; rdd.map(_ + x)，将在RDD中的每一个元素都加5【我们在闭包创建时对其进行保存，这样例子中的map操作永远都是加5，即使x发生变化】。\n\nRDD本身是由元素类型参数化的静态类型对象。例如，RDD [Int]是整数的RDD。但是，由于Scala支持类型推断，因此我们的大多数示例都省略了类型。\n\n虽然我们在Scala中暴露RDDs的方法在概念上很简单，但我们必须使用反射解决Scala的闭包对象的问题[33]。我们还需要更多的工作来使Spark可以使用Scala解释器，我们将在5.2节中讨论。但是，我们不必修改Scala编译器。\n\n### 3.1 Spark中的RDD操作\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060140749/0)\n\n表2列出了Spark中可用的主要RDD转换和动作操作。我们给出每个操作的签名，在方括号中显示类型参数。再次强调转换（transformations）是定义新RDD的延迟操作，而动作（actions）启动计算将向程序返回值或将数据写入外部存储。\n\n请注意，某些操作（如join）仅适用于键值对的RDDs。此外，我们的函数名称被选择为与Scala和其他函数式语言中的其他API匹配；例如，map是一对一映射，而flatMap将每个输入值映射到一个或多个输出（类似于MapReduce中的映射）。\n\n除了这些运算符，用户还可以持久化RDD。此外，用户可以获得RDD的分区顺序（由Partitioner类表示），并根据它对另一个数据集进行分区。诸如groupByKey，reduceByKey和sort操作自动地会产生哈希或范围分区的RDD。\n\n### 3.2 示例应用\n\n我们使用两个迭代应用程序补充了2.2.1节中的数据挖掘示例：逻辑回归和PageRank。后者还展示了如何控制RDD的分区以提高性能。\n\n#### 3.2.1 logistic回归\n\n许多机器学习算法本质上是迭代的，因为它们运行迭代优化过程，例如梯度下降，以最大化功能。因此，通过将数据保存在内存中，以更快地运行。\n\n例如，以下程序实现了逻辑回归[14]，这是用于搜索最能分开两类点（例如，垃圾邮件和非垃圾邮件）的超平面w的一个通用算法。该算法使用梯度下降：它以随机值开始w，并且在每次迭代时，它将w的函数与数据相加以沿着改善它的方向移动。\n\n```scala\nval points = spark.textFile(...)\n                  .map(parsePoint).persist()\nvar w = // random initial vector\nfor (i <- 1 to ITERATIONS) {\n  val gradient = points.map{ p =>\n    p.x * (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y\n  }.reduce((a,b) => a+b)\n  w -= gradient\n}\n```\n我们首先定义一个名为points的持久RDD作为文本文件上的map转换的结果，该文本文件将每行文本解析为Point对象。然后，我们通过对当前w的函数求和，对points重复运行map和reduce以计算每一步的梯度。在迭代的过程中将points保存在内存中可以获得20倍的加速，这将在6.1节展示。\n\n#### 3.2.2 PageRank\n\n更复杂的数据共享模式发生在PageRank [6]。算法通过累加在文件中对每个文件的应用次数迭代地更新每一文件的rank。在每次迭代时，每个文件都向其邻居发送r/n的贡献值，r是其排名，n是其邻居的数量。然后通过α/N + (1 − α)∑ci式子更新排名，其中求和是它所收到的贡献值，而N是文件的总数。我们可以通过如下代码Spark中实现PageRank：\n\n```scala\n// Load graph as an RDD of (URL, outlinks) pairs\nval links = spark.textFile(...).map(...).persist()\nvar ranks = // RDD of (URL, rank) pairs\nfor (i <- 1 to ITERATIONS) {\n// Build an RDD of (targetURL, float) pairs\n  // with the contributions sent by each page\n  val contribs = links.join(ranks).flatMap {\n    (url, (links, rank)) =>\n      links.map(dest => (dest, rank/links.size))\n  }\n  // Sum contributions by URL and get new ranks\n  ranks = contribs.reduceByKey((x,y) => x+y)\n             .mapValues(sum => a/N + (1-a)*sum)\n}\n```\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060154806/0)\n\n该程序产生图3中的RDD lineage图。在每次迭代中，我们基于来自先前的迭代的contribs和ranks以及静态links数据集来创建新的排名数据集【请注意，尽管RDD是不可变的，但程序中的变量rank和contribs指向每次迭代时的不同RDDs】。图3的一个有趣特征是随着迭代数目的增加而不断增长。因此，在具有多次迭代的作业中，可能需要可靠地复制某些版本的ranks以减少故障恢复时间[20]。用户可以使用RELIABLE标志调用persist来执行此操作。但请注意，不需要复制links数据集，因为可以通过在输入文件的块上重新运行map来有效地重建它的分区。此数据集通常比ranks大得多，因为每个文档都有许多链接，但只有一个数字作为其排名，因此使用lineage恢复它比使用checkpoint回复程序整个内存中状态的系统更节省时间。\n\n最后，我们可以通过控制RDDs的分区来优化PageRank中的通信。如果我们指定links的分区（例如，通过节点间的URL对link lists进行哈希分区），以相同的方式对ranks进行分区，这样就确保links和ranks之间的join操作不需要通信（因为每个URL的排名将与其link list在同一台机器上）。我们还可以编写自定义分区程序类来对相互链接的页面进行分组（例如，按域名对URL进行分区）。在定义links时，可以通过调用partitionBy来表示这两种优化：\n\n```scala\nlinks = spark.textFile(...).map(...)\n             .partitionBy(myPartFunc).persist()\n```\n\n在此初始化之后，links和ranks之间的join操作会自动地将每个URL的贡献值聚集到它link lists所在的机器上，在这台机器上计算新的排名并与它的links进行join操作。这种跨迭代的一致分区是Pregel等专用框架中的主要优化之一。 RDDs让用户直接表达这一目标。\n\n## 4 表示RDDs\n\n提供RDD作为抽象的挑战之一是为它们选择一种能够跟踪各种转换操作的lineage的表示。理想情况下，实现RDD的系统应该提供尽可能丰富的一组转换操作（例如，表2中的转换操作），并让用户以任意方式组合它们。我们为RDD提出了一个简单的基于图的表示，以达到这些目标。我们在Spark中使用这种表示来支持各种转换，而无需为调度器添加针对每个转换的特殊逻辑，这大大简化了系统设计。简而言之，我们建议通过一个公共接口来表示每个RDD，这个接口包含五条信息：一组分区，它们是数据集的原子部分；父RDDs（parent RDDs）的一组依赖关系；基于其父RDDs计算数据集的函数；有关其分区方案和数据放置的元数据。例如，表示HDFS文件的RDD具有文件的每个块的分区，并且知道每个块所在的机器。同时，对于这个RDD进行map操作的结果具有相同的分区，但是在计算其元素时将在父数据上应用map方法。我们在表3中总结了这个接口。\n\n![](https://puui.qpic.cn/fans_admin/0/3_234219102_1557060167174/0)\n\n设计此接口时最有趣的问题是如何表示RDDs之间的依赖关系。我们发现将依赖关系分为两类是足够且有用的：窄依赖关系（narrow dependencies），其中父RDD的每个分区最多由子RDD的一个分区使用；宽依赖关系（wide dependencies），其中多个子RDD分区可能依赖一个父DD分区。例如，map导致窄依赖关系，而join导致宽依赖关系（除非父RDD是哈希分区的）。图4显示了其他示例。\n\n![](https://puui.qpic.cn/fans_admin/0/3_256675758_1557060251325/0)\n\n这样区分有两个原因。首先，窄依赖关系允许在一个集群节点上进行流水线执行，这可以计算所有父分区。例如，可以逐个元素地应用map，然后应用filter操作。相比之下，宽依赖关系要求所有父分区的数据都已经计算完成，并使用类似MapReduce的操作在节点之间进行shuffle。其次，节点故障后的恢复在窄依赖时更有效，因为只需要重新计算丢失的对应的父分区，并且可以在不同节点上并行地重新计算它们。相反，在具有宽依赖的lineage图中，单个故障节点可能导致RDD的所有祖先丢失某些分区，从而需要完全重新执行计算。\n\nRDDs的这个通用接口使得可以在少于20行代码中实现Spark中的大多数转换操作。实际上，即使是新的Spark用户也已经实现了新的转换（例如，采样和各种类型的join），而不用知道调度器的细节。我们在下面阐述一些RDD实现。\n\n**HDFS文件：**\n我们样本中的输入RDDs是HDFS中的文件。对于这些RDDs，partitions为文件的每个块返回一个分区（块的偏移量存储在每个Partition对象中），preferredLocations给出块所在的节点，iterator读取块。\n\n**map：**\n在任何RDD上调用map都会返回MappedRDD对象。该操作传递一个函数参数给map，对父RDD上的记录按照iterator的方式执行这个函数，并返回一组符合条件的父RDD分区及其位置。\n\n**union：**\n在两个RDD上执行union操作，返回两个父RDD分区的并集。通过相应父RDD上的窄依赖关系计算每个子RDD分区【7注意union操作不会过滤重复值】。\n\n**join：**\n对两个RDD执行join操作可能产生窄依赖（如果这两个RDD拥有相同的哈希分区或范围分区），可能是宽依赖，也可能两种依赖都有（比如一个父RDD有分区，而另一父RDD没有）。在任何一种情况下，输出RDD都有一个分区程序（从父项继承的分区程序或默认的散列分区程序）。\n\n## 5 实现\n\n我们大约用14000行scala代码实现了Spark。该系统运行在Mesos集群管理器[17]上，允许它与Hadoop，MPI和其他应用程序共享资源。每个Spark程序作为单独的Mesos应用程序运行，具有自己的驱动程序（master）和工作程序（workers），这些应用程序之间的资源共享由Mesos管理的。Spark可以使用Hadoop现有的输入插件API从任何Hadoop输入源（例如，HDFS或HBase）读取数据，并在未经修改的Scala版本上运行。\n\n我们现在简要介绍系统中几个技术上有趣的部分：我们的作业调度程序（第5.1节），允许交互式使用的Spark解释器（第5.2节），内存管理（第5.3节）和支持检查点（第5.4节）。\n\n### 5.1 作业调度\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060276767/0)\n\nSpark的调度程序使用我们在第4节所述的RDD表示。\n\n总的来说，我们的调度程序类似于Dryad的[19]，但是另外考虑了RDDs在内存中持久化的分区。每当用户在RDD上运行动作（例如，count或save）时，该调度程序就会检查该RDD的lineage图以构建要执行的stage的DAG，如图5所示。每个阶段包含尽可能多的具有窄依赖的流水线转换。阶段的边界是宽依赖所需的shuffle操作，或任何已经计算过的分区，它们可以跳过父RDD的计算。然后，调度程序启动任务以计算每个阶段中丢失的分区，直到计算出目标RDD为止。\n\n我们的调度程序基于数据位置使用延迟调度将任务分配给机器[32]。如果任务需要处理节点内存中可用的分区，我们会将其发送到该节点。否则，如果任务处理包含RDD提供优选位置的分区（例如，HDFS文件），我们将其发送给那些分区。\n\n对于宽依赖关系（即，shuffle依赖关系），我们目前在包含父分区的节点上物化中间记录以简化故障恢复，就像MapReduce物化map输出一样。\n\n如果任务失败，只要其阶段的父项仍然可用，我们就会在另一个节点上重新运行它。如果某些阶段变得不可用（例如，因为来自shuffle的“map side”的输出丢失），我们重新提交任务以并行计算丢失的分区。我们还不能解决调度程序的失败，尽管复制RDD的lineage图会更简单。\n\n最后，尽管Spark中的所有计算当前都是为响应驱动程序中调用的操作而运行的，但我们也在尝试让集群上的任务（例如，映射）调用lookup操作，其提供根据键值对哈希分区的RDDs中的元素进行随机获取。在这种情况下，任务需要告诉调度程序在缺少时计算所需的分区。\n\n### 5.2 解释器整合\n\nScala包含一个类似于Ruby和Python的交互式shell。鉴于内存数据的延迟较低，我们希望让用户从解释器主动运行Spark来查询大数据集。\n\nScala解释器通常通过为用户键入的每一行编译一个类，将其加载到JVM中，并在其上调用函数来操作。该类包含一个单例对象，该对象包含该行上的变量或函数，并在初始化方法中运行行代码。例如，如果用户输入代码var x = 5，接着又输入println(x)，则解释器会定义一个包含x的Line1类，并将第2行编译为println(Line1.getInstance().x)。\n\n在Spark中我们对解释器做了两点改动：\n\n1.类传输：解释器能够支持基于HTTP传输类字节码，这样worker节点就能获取输入每行代码对应的类的字节码。\n\n2.改进的代码生成逻辑：通常每行上创建的单例对象通过对应类上的静态方法进行访问。也就是说，如果要序列化一个闭包，它引用了前面代码行中变量，比如上面的例子Line1.x，Java不会根据对象关系传输包含x的Line1实例。所以worker节点不会收到x。我们将这种代码生成逻辑改为直接引用各个行对象的实例。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060301153/0)\n\n图6显示了在我们更改之后，解释器如何将用户键入的一组行转换为Java对象。\n\nSpark解释器便于跟踪处理大量对象关系引用，并且便利了HDFS数据集的研究。我们计划以Spark解释器为基础，开发提供高级数据分析语言支持的交互式工具，比如SQL。\n\n### 5.3 内存管理\n\nSpark提供了三种持久化RDD的选项：反序列化Java对象的内存存储，序列化数据的内存存储和硬盘存储。第一个选项提供最快的性能，因为Java VM可以原生访问每个RDD元素。第二个选项允许用户在空间有限时选择比Java对象图更高效的内存表示，但代价是性能较低【成本取决于应用程序每个字节数据的计算量，但轻量级处理的最大值可提升2倍】。第三个选项对于太大而无法保留在RAM中但在每次使用时重新计算成本高昂的RDD非常有用\n\n为了管理可用的有限内存，我们在RDDs层面上使用LRU替换策略。当计算新的RDD分区但没有足够的空间来存储它时，我们从最近最少的RDD中替换出一个分区，除它这与具有新分区的RDD相同。在这种情况下，我们将旧分区保留在内存中，以防止来自同一RDD的分区循环进出。这很重要，因为大多数操作都会在整个RDD上运行任务，因此很可能将来需要已经在内存中的分区。到目前为止，我们发现此默认策略在所有应用程序中都能正常运行，但我们还通过每个RDD的“持久化优先级”为用户提供进一步控制。\n\n最后，群集上的每个Spark实例当前都有自己独立的内存空间。在未来的工作中，我们计划通过统一的内存管理器来研究跨Spark实例共享RDDs。\n\n### 5.4 对检查点的支持\n\n尽管在故障之后可以始终使用lineage来恢复RDDs，但对于具有长lineage的RDDs来说，这种恢复可能是耗时的。因此，将一些RDDs checkpoint到稳定存储可能会有所帮助。\n\n通常，检查点技术对于包含宽依赖关系的长lineage图的RDDs很有用，例如我们的PageRank示例（第3.2.2节）中的rank数据集。在这些情况下，集群中的节点故障可能导致每个父RDD丢失一些数据片段，从而需要完全重新计算[20]。相反，对于对稳定存储中的数据具有窄依赖的RDDs，例如我们的逻辑回归示例（第3.2.1节）中的points和PageRank中的link lists，检查点技术可能没多大用。如果节点发生故障，则可以在其他节点上并行重新计算从这些RDD中丢失的分区，而这只是复制整个RDD的成本的一小部分。\n\nSpark目前提供了一个用于检查点技术的API（一个在persist中的REPLICATE标志），但是由用户决定哪些数据使用检查点。但是，我们还在研究如何进行自动检查。因为我们的调度程序知道每个数据集的大小以及首次计算它所花费的时间，所以它应该能够选择一组最佳RDDs来检查点以最小化系统恢复时间[30]。\n\n最后，要强调的一点是，RDDs的只读特性使它们比通用共享存储更容易进行检查。由于一致性问题不需要考虑，因此可以在后台写出RDD，而无需程序暂停或采用分布式快照方案。\n\n## 6 评估\n\n我们通过Amazon EC2上的一系列实验以及用户应用程序的基准评估了Spark和RDDs。总的来说，我们的结果显示如下：\n\n* 在迭代机器学习和图形应用程序中，Spark的性能比Hadoop高出20倍。加速来自于通过将数据作为Java对象存储在内存中来避免I / O和反序列化成本。\n* 我们的用户编写的应用程序可以很好地执行和扩展。特别是，我们使用Spark分析报表比在Hadoop上运行快40倍。\n* 当节点发生故障时，Spark可以通过仅重建丢失的RDD分区来快速恢复。\n* Spark可用于交互查询1 TB数据集，延迟仅为5-7秒。\n\n我们首先与Hadoop进行迭代机器学习（第6.1节）和PageRank（第6.2节）的基准比较。然后，我们评估Spark中的故障恢复（第6.3节）以及数据集不适合存储时的行为（第6.4节）。最后，我们讨论了用户应用（第6.5节）和交互式数据挖掘（第6.6节）的结果。\n\n除非另有说明，否则我们的测试使用m1.xlarge EC2节点，其中包含4个内核和15 GB RAM。我们使用HDFS进行存储，具有256 MB块。在每次测试之前，我们清除了OS缓冲区高速缓存，以准确测量IO成本。\n\n### 6.1 迭代式机器学习应用\n\n我们实现了两个迭代机器学习应用程序，逻辑回归和k-means，以比较以下系统的性能：\n\n* Hadoop：The Hadoop 0.20.2 stable release。\n* HadoopBinMem：在首轮迭代中执行预处理，通过将输入数据转换成为开销较低的二进制格式来减少后续迭代过程中文本解析的开销，在HDFS中加载到内存。\n* Spark：基于RDDs的实现。\n\n我们使用25-100台机器在100 GB数据集上对这两个算法进行了10次迭代。两个应用程序之间的关键区别是它们每个数据字节执行的计算量。k-means的迭代时间由计算决定，而逻辑回归的计算密集度较低，但对反序列化和I / O花费的时间更敏感。\n\n由于典型的学习算法需要数十次迭代才能收敛，因此我们分别报告第一次迭代和后续迭代的时间。我们发现通过RDDs共享数据可以大大加快未来的迭代速度。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1280192908_1557060314454/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060328473/0)\n\n**首次迭代**\t\n所有三个系统在第一次迭代中从HDFS读取文本输入。如图7中的浅色柱状图所示，Spark在实验中比Hadoop要快一些。如图7中的灯条所示，Spark在实验中比Hadoop要快一些。HadoopBinMem是最慢的，因为它通过一个额外的MapReduce作业将数据转换成二进制格式，并必须通过网络在HDFS结点间复制数据。\n\n**后续迭代**\t\n图7还显示了后续迭代的平均运行时间，而图8显示了其随集群大小变化而产生的时间变化。对于逻辑回归，Spark在100台机器上分别比Hadoop和HadoopBinMem快25.3倍和20.7倍。对于更加计算密集型的k-means应用程序来说，Spark仍然实现了1.9倍到3.2倍的加速。\n\n**理解速度提升**\t\n我们惊讶地发现Spark甚至比内存存储二进制数据的Hadoop（HadoopBinMem）还要快20倍。在HadoopBinMem中，我们使用了Hadoop的标准二进制格式（SequenceFile）和256 MB大小的超大块，并且我们强制HDFS的数据目录位于内存文件系统中。但是，由于以下几个因素，Hadoop仍然运行缓慢：\n\n1.Hadoop软件堆栈的最小开销，\n\n2.提供数据时HDFS的开销，\n\n3.将二进制记录转换为可用的内存中Java对象的反序列化成本。\n\n我们依次研究了这些因素。为了估测1，我们运行空的Hadoop作业，仅仅执行作业的初始化、启动任务、清理工作就至少耗时25秒。对于2，我们发现为了服务每一个HDFS数据块，HDFS进行了多次复制以及计算校验和操作。\n\n最后，为了估测3，我们在一台机器上运行微基准测试，以256 MB输入的各种格式运行逻辑回归计算。特别是，我们比较了处理来自HDFS（HDFS堆栈中的开销将给出）和内存本地文件（内核可以非常有效地将数据传递给程序）的文本和二进制输入的时间。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060341567/0)\n\n结果如图9所示。内存中HDFS和本地文件之间的差异表明，通过HDFS读取产生了2秒的开销，即使数据存储在本地机器上也是如此。文本和二进制输入之间的差异表明解析开销为7秒。最后，即使从内存文件中读取，将预解析的二进制数据转换为Java对象也需要3秒钟，这仍然几乎与逻辑回归本身一样开销高昂。通过将RDD元素直接存储为内存中的Java对象，Spark可以避免所有这些开销。\n\n### 6.2 PageRank\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060351959/0)\n\n我们使用54 GB Wikipedia导出数据比较了Spark与Hadoop 进行PageRank的性能。PageRank算法通过10轮迭代处理了大约400万文章的链接图数据。图10展示了单独的内存存储使Spark在30个节点上的速度比Hadoop提高了2.4倍。此外，如第3.2.2节所述，控制RDD的分区以使其在迭代中保持一致，将提高到7.4倍。加速也能几乎线性地扩展到60个节点上。\n\n我们还评估了使用Spark实现Pregel版本的PageRank的性能，结果将在7.1节展示。迭代时间与图10中的相似，但是更长约4秒，因为Pregel在每次迭代时运行一个额外的操作，让顶点“投票”是否完成工作。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060363143/0)\n\n### 6.3 错误恢复\n\n我们评估了k-means应用程序中节点故障后使用lineage重建RDD分区的开销。图11比较了正常操作场景中75节点集群上k-means的10次运行的运行时间，其中一个节点在第6次迭代开始时失败。另一个没有任何故障，每次迭代包含400个任务，处理100 GB的数据。\n\n直到第5次迭代结束，迭代时间约为58秒。在第6次迭代中，其中一台机器被杀死，导致在该机器上运行的任务和存储在那里的RDD分区丢失。Spark在其他机器上并行重新执行这些任务，他们通过lineage重新读取相应的输入数据和重建的RDD，导致操作时间增加到80秒。一旦重建丢失的RDD分区，迭代时间就会回落到58秒。\n\n请注意，使用基于检查点的故障恢复机制，恢复可能需要重新运行至少几次迭代，具体取决于检查点的频率。此外，系统需要通过网络复制应用程序的100 GB工作集（文本输入数据转换为二进制），并且要么消耗两倍于Spark的内存以将其复制到RAM中，要么必须等待写入100 GB到磁盘。相比之下，我们示例中RDD的lineage图的大小都小于10 KB。\n\n### 6.4 内存不足时表现\n\n到现在为止，我们能保证集群中的每个节点都有足够的内存去缓存迭代过程中使用的RDDs。一个自然的问题是，如果没有足够的内存来存储作业的数据，Spark是如何运行的。在本实验中，我们将Spark配置为不使用超过一定百分比的内存来在每台机器上存储RDD。图12中，我们为的逻辑回归提供了各种存储空间的配置。我们发现性能在控件降低时缓慢降低。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060374238/0)\n\n### 6.5 用Spark构建的用户应用程序\n\n**内存分析**\t\nConviva Inc是一家视频发行公司，它使用Spark加速了之前在Hadoop上运行的大量数据分析报告。例如，一份报告作为一系列Hive [1]查询运行，这些查询计算出了客户的各种统计信息。这些查询都在相同的数据子集上工作（与客户提供的过滤器匹配的记录），但在不同的分组字段上执行了聚合（平均值，百分位数和COUNT DISTINCT），需要使用单独的MapReduce作业。通过在Spark中实现查询并将一次共享的数据子集加载到RDD中，该公司能够将报告速度提高40倍。在Hadoop集群上花费20小时的200 GB压缩数据的报告现在仅使用两台Spark计算机就能在30分钟内运行完成。此外，Spark程序只需要96 GB的RAM，因为它只存储与RDD中客户的过滤器匹配的行和列，而不是整个解压缩文件。\n\n**城市交通模型**\t\n在Berkeley的Mobile Millennium项目[18]中，基于一系列分散的汽车GPS监测数据，研究人员使用并行化机器学习算法来推算公路交通拥堵状况。数据来自市区10000个互联的公路线路网，还有600000个由汽车GPS装置采集到的样本数据，这些数据记录了汽车在两个地点之间行驶的时间（每一条路线的行驶时间可能跨多个公路线路网）。使用一个交通模型，通过推算跨多个公路网行驶耗时预期，系统能够估算拥堵状况。研究人员使用Spark实现了一个可迭代的EM算法，其中包括向Worker节点广播路线网络信息，在E和M阶段之间执行reduceByKey操作，应用从20个节点扩展到80个节点（每个节点4核），如图13（a）所示：\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060384098/0)\n\n**推特垃圾邮件分类**\nBerkeley的Monarch项目[29]使用Spark识别Twitter消息上的垃圾链接。他们在Spark上实现了一个类似6.1小节中示例的Logistic回归分类器，不同的是使用分布式的reduceByKey操作并行对梯度向量求和。图13（b）显示了基于50G数据子集训练训练分类器的结果，整个数据集是250000的URL、至少10^7个与网络相关的特征/维度，内容、词性与访问一个URL的页面相关。随着节点的增加，这并不像交通应用程序那样近似线性，主要是因为每轮迭代的固定通信代价较高。\n\n### 6.6 交互式数据挖掘\n\n为了演示Spark交互式查询大数据集的能力，我们用它来分析1TB的维基百科页面浏览日志（2年的数据）。在本次实验中，我们使用了100 m2.4xlarge EC2实例，每个实例有8个内核和68 GB内存。在整个输入数据集上简单地查询如下内容以获取页面浏览总数：（1）全部页面；（2）页面的标题能精确匹配给定的关键词；（3）页面的标题能部分匹配给定的关键词。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060393080/0)\n\n图14显示了分别在整个、1/2、1/10的数据上查询的响应时间，甚至1TB数据在Spark上查询仅耗时5-7秒，这比直接操作磁盘数据快几个数量级。例如，从磁盘上查询1TB数据耗时170秒，这表明了RDD缓存使得Spark成为一个交互式数据挖掘的强大工具。\n\n## 7 讨论\n\n虽然RDD由于其不可变性和粗粒度转换似乎提供有限的编程接口，但我们发现它们适用于广泛的应用。特别地，RDDs可以表达数量惊人的集群编程模型，这些模型迄今已被提议作为单独的框架，允许用户在一个程序中组合这些模型（例如，运行MapReduce操作来构建图形，然后在其上运行Pregel）并在它们之间共享数据。在本节中，我们将讨论RDDs可以表达哪些编程模型以及它们如此广泛适用的原因（第7.1节）。此外，我们讨论了我们正在探索的RDDs中lineage信息的另一个好处，能够促进这些模型的调试（第7.2节）。\n\n### 7.1 表达现有的编程模型\n\nRDDs可以有效地表达迄今为止独立提出的许多集群编程模型。“有效”，意味着RDDs不仅可以用于生成与这些模型中编写的程序相同的输出，而且RDDs还可以实现这些框架具有的优化，例如将特定数据保存在内存中，将其分区为最大限度地减少通信，并有效地从故障中恢复。使用RDD表达的模型包括：\n\n**MapReduce：** 可以使用Spark中的flatMap和groupByKey操作表示此模型，如果存在组合器，则可以使用reduceByKey表示.\n\n**DryadLINQ：** \n与更普遍的Dryad运行时相比，DryadLINQ系统提供了比MapReduce更广泛的运算符，但这些都是直接对应于Spark（map，groupByKey，join等）中可用的RDD转换的批量运算符。\n\n**SQL：** \n与DryadLINQ表达式一样，SQL查询对记录集执行数据并行操作。\n\n**Pregel：**\nGoogle的Pregel [22]是迭代图形应用程序的专用模型，起初与其他系统中的面向集合的编程模型完全不同。在Pregel中，程序作为一系列协调的“supersteps”运行。在每个supersteps中，图中的每个顶点都运行一个用户函数，可以更新与顶点相关的状态，更改图形拓扑，并将消息发送到其他顶点用于下一个superstep。该模型可以表达许多图算法，包括最短路径，二分匹配和PageRank。\n\n让我们用RDDs实现这个模型的关键点是Pregel将相同的用户函数应用于每次迭代的所有顶点。因此，我们可以将每次迭代的顶点状态存储在RDD中，并执行批量转换（flatMap）以应用此函数并生成消息的RDD。之后可以将该RDD与顶点状态连接操作以表示消息的转换。同样重要的是，RDDs允许我们像Pregel那样将顶点状态保存在内存中，通过控制其分区来最小化通信，并支持故障时的部分恢复。我们在Spark上实现了Pregel，其作为200行库，并向读者推荐[33]以获取更多详细信息。\n\n迭代式MapReduce： 最近提出的几个系统，包括HaLoop [7]和Twister [11]，提供了迭代的MapReduce模型，用户可以为系统提供一系列MapReduce作业循环执行。系统使数据在迭代中保持一致，Twister也可以将其保存在内存中。两种优化都很容易用RDDs表达，我们能够使用Spark将HaLoop实现为200行库。\n\n流式批处理： 研究人员最近提出了几种增量处理系统，用于定期用新数据更新结果的应用[21,15,4]。例如，每15分钟更新一次广告点击统计数据的应用程序应该能够将前一个15分钟窗口的中间状态与新日志中的数据相结合。这些系统执行类似于Dryad的批量操作，但将应用程序状态存储在分布式文件系统中。将中间状态置于RDDs中将加速其处理。\n\n**_解释RDDs的表达性能_**\t\n为什么RDD能够表达这些不同的编程模型？原因是对RDD的限制对许多并行应用程序几乎没有影响。特别是，尽管RDDs只能通过批量转换创建，但许多并行程序本质上就是将相同的操作应用于许多记录，所以使其易于用RDDs表达。类似地，RDD的不变性不是一个障碍，因为可以创建多个RDD来表示同一数据集的不同版本。实际上，今天的许多MapReduce应用程序都运行在不允许更新文件的文件系统上，例如HDFS。\n\n最后一个问题是为什么以前的框架没有提供相同的一般性。我们认为这是因为这些系统探索了MapReduce和Dryad无法很好处理的特定问题，例如迭代，但是没有观察到这些问题的常见原因是缺乏数据共享抽象。\n\n### 7.2 利用RDDs进行调试\n\n虽然我们最初设计的RDD在确定性方面可以重新计算以实现容错，但这个属性也可以简化调试。特别是，通过记录在作业期间创建的RDD的lineage，可以（1）稍后重建这些RDD并让用户以交互方式查询它们，以及（2）在单个过程调试中重新执行作业的任何任务，通过重新计算它所依赖的RDD分区。与通用分布式系统[13]的传统重放调试器不同，不必捕获或推断跨多节点的时间顺序，这种方法几乎不增加记录开销，因为只需要记录RDD lineage图【9 与这些系统不同，基于RDD的调试器不会重放用户函数中的非确定性行为（例如，非确定性映射），但它至少可以通过校验和数据来报告它】。我们目前正在开发基于这些想法的Spark调试器[33]。\n\n## 8 相关工作\n\n**集群编程模型：** \n集群编程模型的相关工作分为几类。首先，MapReduce [10]，Dryad [19]和Ciel [23]等数据流模型支持丰富的运算符集，用于处理数据，但通过稳定的存储系统共享数据。RDDs代表比稳定存储更有效的数据共享抽象，因为它们避免了数据复制，I / O和序列化的成本【10 请注意，在像RAMCloud [25]这样的内存数据存储中运行MapReduce / Dryad仍然需要数据复制和序列化，这对于某些应用程序来说可能代价很高，如6.1节所示】。\n\n其次，数据流系统的几个高级编程接口，包括DryadLINQ [31]和FlumeJava [8]，提供了语言集成的API，用户通过map和join等操作符操作“并行集合”。但是，在这些系统中，并行集合表示磁盘上的文件或用于表示查询计划的临时数据集。虽然系统会在相同的操作符查询间流水式地处理数据（如，一个map操作接着一个map操作），但是它们并不能在各个查询之间有效地共享数据。我们在并行收集模型上基于Spark的API，因为它的方便性，并没有增加语言集成接口的新颖性，但通过提供RDD作为此接口背后的存储抽象，我们允许它支持更广泛的应用程序。\n\n第三类系统为需要数据共享的特定类别的应用程序提供高级接口。例如，Pregel [22]支持迭代图应用，而Twister [11]和HaLoop [7]是迭代MapReduce运行时。但是，这些框架隐式地为他们支持的计算模式提供数据共享，并且不提供一般抽象来供用户选择。例如，用户不能使用Pregel或Twister将数据集加载到内存中，然后决定在其上运行哪个查询，RDD明确地提供分布式存储抽象，因此可以支持这些专用系统不支持的应用，例如交互式数据挖掘。\n\n最后，一些系统暴露共享可变状态以允许用户执行内存计算。例如，Piccolo [27]允许用户运行并行功能以读取和更新分布式哈希表中的单元格。分布式共享存储（DSM）系统[24]和键值存储如RAMCloud [25]提供一个相似的模型。RDD在两个方面与这些系统不同。首先，RDD基于运算符（如map，sort和join）提供更高级别的编程接口，而Piccolo和DSM中的接口只是对表格单元格的读取和更新。其次，Piccolo和DSM系统通过检查点和回滚实现恢复，这比许多应用程序中基于lineage策略的RDDs更昂贵。最后，正如第2.3节所讨论的那样，RDD还提供了其他优于DSM的优势，例如straggler缓解。\n\n**缓存系统：** \nNectar [12]可以通过程序分析识别常见的子表达式，在DryadLINQ作业中重用中间结果[16]。这种能力对于添加到基于RDD的系统非常有吸引力。但是，Nectar不提供内存中缓存（它将数据放在分布式文件系统中），也不允许用户明确控制要持久化的数据集以及如何对它们进行分区。Ciel [23]和FlumeJava [8]同样可以缓存任务结果，但不提供内存缓存或显式控制缓存哪些数据。Ananthanarayanan等。建议在分布式文件系统中添加内存缓存，以利用数据访问的时间和空间局部性[3]。虽然此解决方案可以更快地访问文件系统中已有的数据，但它不像在RDDs中那样有效地在一个应用中共享中间结果，因为它仍然需要应用程序在不同阶段间将这些结果写入文件系统。\n\n**Lineage：**\n记载数据的lineage或起源信息长期以来一直是科学计算和数据库中的研究课题，应用于解释结果，允许数据可以被别的数据重建，同时如果在工作流中出现了bug或者数据及丢失了，能够重新计算得到数据。我们推荐读者阅读[5]和[9]来了解这些工作。RDDs提供并行编程模型，其中获取细粒度的lineage成本低廉，因此可用于故障恢复。\n\n我们基于lineage的恢复机制也类似于MapReduce和Dryad中计算（作业）中使用的恢复机制，它跟踪任务的DAG之间的依赖关系。但是，在这些系统中，谱系（lineage）信息在作业结束后丢失，需要使用复制的存储系统来跨计算共享数据。相比之下，RDDs应用lineage来有效地跨计算保留内存数据，而无需复制和磁盘I / O的成本。\n\n**关系型数据库：**\nRDDs在概念上类似于数据库中的视图，而持久化RDDs类似于物化视图[28]。但是，与DSM系统一样，数据库通常允许对所有记录进行细粒度的读写访问，需要记录操作和数据以实现容错，并且需要额外的开销来维护一致性。RDD的粗粒度转换模型不需要这些开销。\n\n## 9 结论\n\n我们提供了弹性分布式数据集（RDDs），这是一种高效，通用和容错的用于在集群应用程序中共享数据的抽象。RDDs可以表达各种并行应用程序，包括已经提出用于迭代计算的许多专用编程模型，以及这些模型还未实现的新应用程序。与现有的集群存储抽象（需要数据复制以实现容错）不同，RDDs提供基于粗粒度转换的API，使其能够使用lineage来有效地恢复数据。我们在Spark中实现了RDDs，它在迭代应用程序中的性能比Hadoop高出20倍，并且可以交互式查询数百GB的数据。\n\n我们在spark-project.org上提供了开源Spark作为可扩展数据分析和系统研究的工具。\n\n## 致谢\n\n我们感谢第一批Spark用户，包括Tim Hunter，Lester Mackey，Dilip Joseph和Jibin Zhan，他们在实际应用中尝试我们的系统，提供了许多好的建议，并指出了一些研究中的挑战。我们还要感谢我们的指导者Ed Nightingale以及审核的反馈。这项研究部分由Berkeley AMP Lab \n支持，由 Google, SAP, Amazon Web Services, Cloudera, **_Huawei_**, IBM, Intel, Microsoft, NEC, NetApp 和 VMWare，DARPA，the Natural Sci- ences 和 Engineering Research Council of Canada赞助。\n\n## 引用\n\n[1] Apache Hive. http://hadoop.apache.org/hive.\n\n[2] Scala. http://www.scala-lang.org.\n\n[3] G.Ananthanarayanan,A.Ghodsi,S.Shenker,andI.Stoica.\nDisk-locality in datacenter computing considered irrelevant. In\nHotOS ’11, 2011.\n\n[4] P.Bhatotia,A.Wieder,R.Rodrigues,U.A.Acar,and\nR. Pasquin. Incoop: MapReduce for incremental computations.\nIn ACM SOCC ’11, 2011.\n\n[5] R.BoseandJ.Frew.Lineageretrievalforscientificdata\nprocessing: a survey. ACM Computing Surveys, 37:1–28, 2005.\n\n[6] S.BrinandL.Page.Theanatomyofalarge-scalehypertextual\nweb search engine. In WWW, 1998.\n\n[7] Y.Bu,B.Howe,M.Balazinska,andM.D.Ernst.HaLoop:\nefficient iterative data processing on large clusters. Proc. VLDB\nEndow., 3:285–296, September 2010.\n\n[8] C.Chambers,A.Raniwala,F.Perry,S.Adams,R.R.Henry,\nR. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient\ndata-parallel pipelines. In PLDI ’10. ACM, 2010.\n\n[9] J.Cheney,L.Chiticariu,andW.-C.Tan.Provenancein\ndatabases: Why, how, and where. Foundations and Trends in\nDatabases, 1(4):379–474, 2009.\n\n[10] J.DeanandS.Ghemawat.MapReduce:Simplifieddata\nprocessing on large clusters. In OSDI, 2004.\n\n[11] J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu, and G. Fox. Twister: a runtime for iterative mapreduce. In HPDC ’10, 2010.\n\n[12] P.K.Gunda,L.Ravindranath,C.A.Thekkath,Y.Yu,and L. Zhuang. Nectar: automatic management of data and computation in datacenters. In OSDI ’10, 2010.\n\n[13] Z.Guo,X.Wang,J.Tang,X.Liu,Z.Xu,M.Wu,M.F. Kaashoek, and Z. Zhang. R2: an application-level kernel for record and replay. OSDI’08, 2008.\n\n[14] T.Hastie,R.Tibshirani,andJ.Friedman.TheElementsof Statistical Learning: Data Mining, Inference, and Prediction. Springer Publishing Company, New York, NY, 2009.\n\n[15] B.He,M.Yang,Z.Guo,R.Chen,B.Su,W.Lin,andL.Zhou. Comet: batched stream processing for data intensive distributed computing. In SoCC ’10.\n\n[16] A.Heydon,R.Levin,andY.Yu.Cachingfunctioncallsusing precise dependencies. In ACM SIGPLAN Notices, pages 311–320, 2000.\n\n[17] B.Hindman,A.Konwinski,M.Zaharia,A.Ghodsi,A.D. Joseph, R. H. Katz, S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI ’11.\n\n[18] T.Hunter,T.Moldovan,M.Zaharia,S.Merzgui,J.Ma,M.J. Franklin, P. Abbeel, and A. M. Bayen. Scaling the Mobile Millennium system in the cloud. In SOCC ’11, 2011.\n\n[19] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys ’07, 2007.\n\n[20] S.Y.Ko,I.Hoque,B.Cho,andI.Gupta.Onavailabilityof intermediate data in cloud computations. In HotOS ’09, 2009.\n\n[21] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum. Stateful bulk processing for incremental analytics. SoCC ’10. [22] G.Malewicz,M.H.Austern,A.J.Bik,J.C.Dehnert,I.Horn,\nN. Leiser, and G. Czajkowski. Pregel: a system for large-scale\ngraph processing. In SIGMOD, 2010.\n\n[23] D.G.Murray,M.Schwarzkopf,C.Smowton,S.Smith,\nA. Madhavapeddy, and S. Hand. Ciel: a universal execution\nengine for distributed data-flow computing. In NSDI, 2011. \n\n\n[24] B.NitzbergandV.Lo.Distributedsharedmemory:asurveyof\nissues and algorithms. Computer, 24(8):52 –60, Aug 1991. \n\n[25] J.Ousterhout,P.Agrawal,D.Erickson,C.Kozyrakis,\nJ. Leverich, D. Mazie`res, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman. The case for RAMClouds: scalable high-performance storage entirely in DRAM. SIGOPS Op. Sys. Rev., 43:92–105, Jan 2010.\n\n[26] D.PengandF.Dabek.Large-scaleincrementalprocessingusing distributed transactions and notifications. In OSDI 2010.\n\n[27] R.PowerandJ.Li.Piccolo:Buildingfast,distributedprograms with partitioned tables. In Proc. OSDI 2010, 2010.\n\n[28] R.RamakrishnanandJ.Gehrke.DatabaseManagement Systems. McGraw-Hill, Inc., 3 edition, 2003.\n\n[29] K.Thomas,C.Grier,J.Ma,V.Paxson,andD.Song.Designand evaluation of a real-time URL spam filtering service. In IEEE Symposium on Security and Privacy, 2011.\n\n[30] J.W.Young.Afirstorderapproximationtotheoptimum checkpoint interval. Commun. ACM, 17:530–531, Sept 1974.\n\n[31] Y.Yu,M.Isard,D.Fetterly,M.Budiu,U ́.Erlingsson,P.K. Gunda, and J. Currey. DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language. In OSDI ’08, 2008.\n\n[32] M.Zaharia,D.Borthakur,J.SenSarma,K.Elmeleegy,\nS. Shenker, and I. Stoica. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys ’10, 2010.\n\n[33] M.Zaharia,M.Chowdhury,T.Das,A.Dave,J.Ma,\nM. McCauley, M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. Technical Report UCB/EECS-2011-82, EECS Department, UC Berkeley, 2011.\n\n\n\n\n\n\n\n\n    \n\n    ","slug":"tanslate-Resilient-Distributed-Datasets-A-Fault-Tolerant-Abstraction–for-In-Memory-Cluster-Computing","published":1,"updated":"2019-06-04T01:06:18.981Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwk8002c48upz0se2o00","content":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p>大概用了16个小时完成了这篇关于RDD论文的翻译，这篇论文奠定了Spark的设计基础。</p>\n<p>原文:<a href=\"https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\" target=\"_blank\" rel=\"noopener\">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a></p>\n<p>推荐先看林子雨老师关于RDD的<a href=\"http://dblab.xmu.edu.cn/blog/985-2/\" target=\"_blank\" rel=\"noopener\">解释</a></p>\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>本文提出了一个分布式内存抽象的概念–弹性分布式数据集（Resilient Distributed Datasets，以下称为RDDs），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDDs的提出是由于现有的两种计算框架并不高效：迭代式算法和交互式数据挖掘工具。在内存中操作数据可以将前两种计算方式的效率提高一个数量级。RDDs提供了一种受限的共享内存，是基于粗粒度的转换操作而不是细粒度的状态同步。尽管如此，RDDs依然能够表示多种类型的计算，包括专用的迭代编程模型（如Pregel）和一些新的应用模型。我们通过在Spark上评估各种应用和基准，实现了RDDs。</p>\n<h2 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h2><p>集群计算框架，如MapReduce[10]和Dryad[19]，已经被广泛地运用于大规模数据分析。这些系统能够让用户在不用考虑任务调度和容错的前提下，使用一系列高级的操作进行并行计算。</p>\n<p>虽然这些框架为获取集权计算资源提供了大量的抽象，但是缺少对分布式内存的运用。这使他们对于一类新兴的应用十分低效：它们在不同的计算阶段<em>重用</em>中间结果。数据重用在<em>迭代式</em>机器学习和图算法中十分常见，包括PageRank、K-means聚类和逻辑回归。另一个明显的用例是<em>交互式</em>数据挖掘，用户在同样的数据子集上进行ad-hoc查询。然而不好的是，对于现在的框架，在不同计算阶段之间重用数据（如，在两个MapReduce的job之间）的唯一方式是将其写入外部稳定存储系统中，如，分布式文件系统。由于数据的复制、硬盘I/O和序列化，导致了大量的成本开销，并占据了应用运行的大部分时间。</p>\n<p>在意识到这个问题之后，研究人员针对需要数据重用的应用开发了专门的框架。比如，迭代式图计算系统Pregel[22]，其将中间数据存放在内存中，而HaLoop[7]提供了一种迭代式MapReduce接口。无论如何，这些框架仅支持特定的计算模式（如，循环一系列的MapReduce步骤），并隐式地提供这些模式的数据共享。它们没有提供更加通用的数据重用的抽象，如，让用户直接向内存中装载数据集，并对其进行ad-hoc查询。</p>\n<p>在这篇论文中，我们提出了一种能够广泛运用于各种应用中高效的数据重用抽象，<em>弹性分布式数据集</em>（RDDs）。RDDs是一个容错的、并行的数据结构，能够让用户明确地在内存中持久化中间结果，控制其分区以优化数据的放置和使用丰富的操作符对其进行处理。</p>\n<p>设计RDDs的主要挑战是，定义一个能够高效容错能力的编程接口。现有的基于集群的内存存储抽象，如分布式共享内存、键值对存储、数据库和Piccolo，提供了基于细粒度更新可变状态的接口（如，表中的cell）。运用这些接口时，达到数据容错的唯一方式是在不同的机器之间进行数据的冗余或者记录更新日志。这两种方法对于数据密集型工作来说代价高昂，因为他们需要在集群网络间复制大量数据，而网络带宽远小于RAM带宽，同时还会产生大量数据存储开销。</p>\n<p>相比于这些系统，RDDs提供基于粗粒度转换的，可用于大量数据项进行相同操作的接口（如，map,filter和join）。这使得RDDs能够通过记录产生数据集的一系列转换操作（称之为lineage），而不是记录真实的数据，来提高容错的效率【1当lineage链过长时，对一些RDDs进行检查点（checkpoint）设置可能更加有用，我们在5.4节对其进行讨论】。如果一个分区的RDD丢失了，它有足够的信息知道自己是如何从其他的RDD产生的，从而重新计算该分区。因此，丢失的数据可以很快地恢复，而不需要代价昂贵的复制。</p>\n<p>基于粗粒度转换的接口乍一看是有局限性的，但RDDs对于许多并行应用都非常适用，因为这些应用本质上就是会对多种数据项进行相同的操作。为此，我们证明了RDDs高效地运用于表达各种已经被现有的分布式系统所实现的集群编程模型，包括MapReduce，DryadLINQ,，SQL，Pregel 和 HaLoop，以及这些系统无法实现的新应用，如交互式数据挖掘。RDDs能够作为被用来解决前面提到的计算需求而引入的新框架的证据，是因为其强大的抽象能力。</p>\n<p>我们已经在一个被用于UC Berkeley的实验环境和许多公司生产环境下的应用–Spark上，实现了RDDs。Spark提供了一个运用Scala语言，类似DryadLINQ的易用的语言集成编程接口。另外，Spark还可以在Scala解释器中进行交互式大数据集的查询。我们相信Spark会是第一个运用通用编程语言完成交互式速度下集群内存数据挖掘的系统。</p>\n<p>我们通过微基准测试和用户应用对RDDs和Spark进行评估。我们得出，Spark在迭代式应用上比Hadoop快20倍，在真实数据报表分析上快40倍，并且能够在5-7秒的延迟内完成1TB数据集的交互式扫描。更加根本地，为了说明RDDs的通用性，我们在Spark上实现了Pregel和HaLoop的编程模型，包括用一些相对较小的库（每个库大概200行代码）实现它们所采用存储优化策略。</p>\n<p>这篇论文首先介绍RDDs的概览（第2部分）和Spark（第3部分），然后讨论RDDs的内部表示（第4部分），实现（第5部分），和一些实验结果（第6部分）。最后，我们讨论了用RDDs实现几个现有的集群编程模型（第7部分），相关研究工作（第8部分）和总结。</p>\n<h2 id=\"2-弹性分布式数据集（RDDs）\"><a href=\"#2-弹性分布式数据集（RDDs）\" class=\"headerlink\" title=\"2 弹性分布式数据集（RDDs）\"></a>2 弹性分布式数据集（RDDs）</h2><p>这一部分提供关于RDDs的概述。先定义RDDs（2.1节），介绍其在Spark中的编程接口（2.2节）。然后将RDDs与细粒度共享内存抽象进行比较（2.3节）。最后讨论RDD模型的限制（2.4节）。</p>\n<h3 id=\"2-1-RDD抽象\"><a href=\"#2-1-RDD抽象\" class=\"headerlink\" title=\"2.1 RDD抽象\"></a>2.1 RDD抽象</h3><p>一个RDD是只读的，是将记录进行分区的集合。RDDs只能由（1）稳定物理存储中的数据集（2）其他RDDs通过明确的操作产生。我们称这些操作为转换（transformations）以区别其他对RDDs的操作。转换的例子包括map，filter和join。【2 虽然单个RDDs是不可变的，但是可以通过多个RDDs来表示不同版本的数据集以实现多状态。我们让RDDs不可变以使lineage图表示更简便，但这也相当于将我们的抽象变成版本化数据并在lineage图中追踪不同版本】</p>\n<p>RDDs不需要都实体化。一个RDD有足够的信息了解自己是如何从其他数据集产生的（lineage）并通过信息从稳定的物理存储中计算出自己的分区。这一强大特性的本质是，程序能够在RDD重建之后对其进行引用。</p>\n<p>最后一点，用户可以对RDDs进行2方面的控制：持久化和分区。用户可以表明将要重用的RDDs并为其选择存储策略（如，内存存储）。也可以将RDDs的元素通过特定键值进行分区。这些功能对于存储优化特别有用，比如保证两个将要进行join操作的数据集都进行了相同的哈希分区。</p>\n<h3 id=\"2-2-Spark编程接口\"><a href=\"#2-2-Spark编程接口\" class=\"headerlink\" title=\"2.2 Spark编程接口\"></a>2.2 Spark编程接口</h3><p>Spark暴露了类似DryadLINQ和FlumeJava的RDDs语言集成API，每个数据集都被表示成一个对象，并通过执行方法在这些对象上进行转换操作（transformations）。</p>\n<p>开发人员通过将物理存储上的数据集进行转换（如，map和filter）来定义一个或多个RDDs。然后可以用动作（actions）对这些RDDs进行操作，这些操作向应用返回值或者向存储系统产生外部数据。动作（actions）的例子包括，count（返回数据集中元素的数目），collect（返回元素本身）和save（将数据集输出到外部存储系统）。像DryadLINQ一样，Spark在遇到一个动作操作时才会真正计算出RDDs，所以其可以对转换（transformations）进行流水线操作。</p>\n<p>此外，开发人员还可以调用persist方法来声明他们想重用的RDDs。Spark默认会将RDDs留存在内存中，但是会在没有足够RAM的情况下将它们溢出到硬盘。用户可以采用其他的持久化策略，如通过persist标价，将特定RDD只存在硬盘上或在机器之间进行备份。最后，用户可以为每个RDD设置优先级来表明当需要时，将哪一个内存中的数据溢出到硬盘中。</p>\n<h4 id=\"2-2-1-例子：控制台日志挖掘\"><a href=\"#2-2-1-例子：控制台日志挖掘\" class=\"headerlink\" title=\"2.2.1 例子：控制台日志挖掘\"></a>2.2.1 例子：控制台日志挖掘</h4><p>假设一个网页服务出现错误，管理员想在HDFS中兆字节规模的日志中找到原因。应用Spark，管理员能够将错误信息从多个结点的日志中导入RAM，并交互式的进行查询。她会键入以下代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lines = spark.textFile(<span class=\"string\">\"hdfs://...\"</span>)</span><br><span class=\"line\">errors = lines.filter(_.startsWith(<span class=\"string\">\"ERROR\"</span>))</span><br><span class=\"line\">errors.persist()</span><br></pre></td></tr></table></figure>\n<p>第一行从一个HDFS文件（文本行集合）定义了一个RDD，并在第二行产生过滤后的RDD。</p>\n<p>第三行将errors持久化在内存中，这样就能被查询。注意filter的参数是一个闭包的Scala语法。</p>\n<p>到目前为止，并没有在集群上运行作业。但是，用户可以对RDD进行动作（actions）操作，如计算消息的数目：</p>\n<p><code>errors.count()</code></p>\n<p>这位用户也可以对该RDD进行进一步的转换操作并运用他们的结果，如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Count errors mentioning MySQL:</span></span><br><span class=\"line\">errors.filter(_.contains(<span class=\"string\">\"MySQL\"</span>)).count()</span><br><span class=\"line\"><span class=\"comment\">// Return the time fields of errors mentioning</span></span><br><span class=\"line\"><span class=\"comment\">// HDFS as an array (assuming time is field</span></span><br><span class=\"line\"><span class=\"comment\">// number 3 in a tab-separated format):</span></span><br><span class=\"line\">errors.filter(_.contains(<span class=\"string\">\"HDFS\"</span>))</span><br><span class=\"line\">      .map(_.split(’\\t’)(<span class=\"number\">3</span>))</span><br><span class=\"line\">      .collect()</span><br></pre></td></tr></table></figure>\n<p>在第一个对errors进行动作操作之后，Spark将在内存中村塾errors的各分区，这极大地加快了下游的计算操作。注意，最原始的RDD，lines，不会加载到RAM中。这是可取的，因为错误消息可能只是数据的一小部分（小到足够放进内存）。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060102174/0\" alt></p>\n<p>最后为了说明模型是如何实现容错的，图1展示了该RDDs的lineage图。在这次查询中，我们首先对lines进行过滤得到errors，然后在运行collect操作前进行更进一步的filter和map操作。Spark调度器将会流水执行后两个转换操作并向缓存了errors分区的结点发送任务的集合来对其进行计算。除此之外，如果errors的一个分区丢失了，Spark只会在相关的lines分区上进行过滤操作来重建丢失的分区。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060116532/0\" alt></p>\n<h3 id=\"2-3-RDD模型的优势\"><a href=\"#2-3-RDD模型的优势\" class=\"headerlink\" title=\"2.3 RDD模型的优势\"></a>2.3 RDD模型的优势</h3><p>为了理解RDDs作为分布式内存抽象的优势，表1将其与分布式共享内存（DSM）进行了比较。在DSM系统中，应用对全局地址空间进行随意位置的读写。请注意，根据此定义，我们不仅包含了传统的共享内存系统[24]，还包括应用可以进行细粒度写共享状态的其他系统，如Piccolo[27]，其提供共享DHT和分布式数据库。DSM是非常通用的抽象，但是这样的通用性使其很难以一种高效且容错的方式运用在商业集群上。</p>\n<p>RDDs与DSM的主要不同是，RDDs只能通过粗粒度的转换操作产生（“写”），而DSM允许在内存中任意位置读写【注意对RDDs的读仍然可以是细粒度的。例如，应用系统可以将一个RDD视为一个大型只读查找表】。这会限制RDDs为执行批量写入的应用程序，但是这使得其具有高效的容错性。特别是，RDD不需要产生检查点（checkpoint）的开销，因为它们可以使用lineage来恢复【在一些应用中，在具有很长lineage链的RDDs中仍然可以使用checkpoint技术，我们将在5.4节讨论。但是，这些可以在后台完成，因为RDDs是不可变的，并且不需要像在DSM中那样保留整个应用程序的快照】。而且，只有丢失的RDD分区才需要在失败时重新计算，并且它们可以在不同的节点上并行计算，而不必回滚整个程序。</p>\n<p>RDDs的第二个优势是，由于它们不可变的特性，通过运行缓慢任务的副本来缓解慢结点对系统拖拽的压力，就和MapReduce一样[10]。使用DSM很难实现备份任务，因为任务的两个副本将访问相同的内存位置并干扰彼此的更新。</p>\n<p>最后，RDDs提供了DSM没有的两个其他好处。其一，在对RDDs的批量操作中，一个运行时可以基于数据的位置进行任务调度以提高性能。其二，当没有足够的内存来存储RDDs时，它就会优雅地降级，使它们仅用于扫描操作。不适合RAM的分区可以存储在磁盘上，并提供与当期数据并行系统类似的性能。</p>\n<h3 id=\"2-4-不适合RDDs的应用\"><a href=\"#2-4-不适合RDDs的应用\" class=\"headerlink\" title=\"2.4 不适合RDDs的应用\"></a>2.4 不适合RDDs的应用</h3><p>正如引言中所讨论的，RDDs非常适用于对数据集中所有元素进行相同操作的批处理应用。在这些情况下，RDDs可以有效地将每个转换（transformations）记录为lineage图中的一个步骤，并且可以在不记录大量数据的情况下恢复丢失的分区。RDDs不太适合对共享状态进行异步细粒度共享状态更新的应用程序，例如Web应用程序的存储系统或增量Web爬网程序。对于这些应用程序，使用执行传统更新日志记录和checkpoint的系统更有效，例如数据库，RAMCloud [25]，Percolator [26]和Piccolo [27]。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。</p>\n<h2 id=\"3-Spark编程接口\"><a href=\"#3-Spark编程接口\" class=\"headerlink\" title=\"3 Spark编程接口\"></a>3 Spark编程接口</h2><p>Spark通过Scala [2]提供类似于DryadLINQ [31]语言集成API的RDDs抽象，Scala [2]是基于Java VM的静态类型函数编程语言。我们之所以选择Scala，是因为它结合了简洁（便于交互使用）和效率（静态类型）。但是，关于RDD抽象的任何内容都不需要函数式语言</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060128798/0\" alt></p>\n<p>要使用Spark，开发人员编写一个连接到一组workers的driver程序，如图2所示。驱动程序定义一个或多个RDDs并在其上调用动作操作。驱动程序上的Spark代码也会追踪RDDs的lineage。这些workers是长期存在的过程，能够跨操作在RAM中存储RDDs分区。</p>\n<p>正如我们在2.2.1中的日志挖掘例子中所展示，用户想如map这样的RDD操作传递闭包（函数形式）来提供参数。Scala将每个闭包表示为Java对象，这些对象可以序列化并加载到另一个节点上，以通过网络传递闭包。Scala也将在闭包中绑定的任何变量作为Java对象的域。比如，可以编写代码：var x = 5; rdd.map(_ + x)，将在RDD中的每一个元素都加5【我们在闭包创建时对其进行保存，这样例子中的map操作永远都是加5，即使x发生变化】。</p>\n<p>RDD本身是由元素类型参数化的静态类型对象。例如，RDD [Int]是整数的RDD。但是，由于Scala支持类型推断，因此我们的大多数示例都省略了类型。</p>\n<p>虽然我们在Scala中暴露RDDs的方法在概念上很简单，但我们必须使用反射解决Scala的闭包对象的问题[33]。我们还需要更多的工作来使Spark可以使用Scala解释器，我们将在5.2节中讨论。但是，我们不必修改Scala编译器。</p>\n<h3 id=\"3-1-Spark中的RDD操作\"><a href=\"#3-1-Spark中的RDD操作\" class=\"headerlink\" title=\"3.1 Spark中的RDD操作\"></a>3.1 Spark中的RDD操作</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060140749/0\" alt></p>\n<p>表2列出了Spark中可用的主要RDD转换和动作操作。我们给出每个操作的签名，在方括号中显示类型参数。再次强调转换（transformations）是定义新RDD的延迟操作，而动作（actions）启动计算将向程序返回值或将数据写入外部存储。</p>\n<p>请注意，某些操作（如join）仅适用于键值对的RDDs。此外，我们的函数名称被选择为与Scala和其他函数式语言中的其他API匹配；例如，map是一对一映射，而flatMap将每个输入值映射到一个或多个输出（类似于MapReduce中的映射）。</p>\n<p>除了这些运算符，用户还可以持久化RDD。此外，用户可以获得RDD的分区顺序（由Partitioner类表示），并根据它对另一个数据集进行分区。诸如groupByKey，reduceByKey和sort操作自动地会产生哈希或范围分区的RDD。</p>\n<h3 id=\"3-2-示例应用\"><a href=\"#3-2-示例应用\" class=\"headerlink\" title=\"3.2 示例应用\"></a>3.2 示例应用</h3><p>我们使用两个迭代应用程序补充了2.2.1节中的数据挖掘示例：逻辑回归和PageRank。后者还展示了如何控制RDD的分区以提高性能。</p>\n<h4 id=\"3-2-1-logistic回归\"><a href=\"#3-2-1-logistic回归\" class=\"headerlink\" title=\"3.2.1 logistic回归\"></a>3.2.1 logistic回归</h4><p>许多机器学习算法本质上是迭代的，因为它们运行迭代优化过程，例如梯度下降，以最大化功能。因此，通过将数据保存在内存中，以更快地运行。</p>\n<p>例如，以下程序实现了逻辑回归[14]，这是用于搜索最能分开两类点（例如，垃圾邮件和非垃圾邮件）的超平面w的一个通用算法。该算法使用梯度下降：它以随机值开始w，并且在每次迭代时，它将w的函数与数据相加以沿着改善它的方向移动。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> points = spark.textFile(...)</span><br><span class=\"line\">                  .map(parsePoint).persist()</span><br><span class=\"line\"><span class=\"keyword\">var</span> w = <span class=\"comment\">// random initial vector</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"type\">ITERATIONS</span>) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> gradient = points.map&#123; p =&gt;</span><br><span class=\"line\">    p.x * (<span class=\"number\">1</span>/(<span class=\"number\">1</span>+exp(-p.y*(w dot p.x)))<span class=\"number\">-1</span>)*p.y</span><br><span class=\"line\">  &#125;.reduce((a,b) =&gt; a+b)</span><br><span class=\"line\">  w -= gradient</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们首先定义一个名为points的持久RDD作为文本文件上的map转换的结果，该文本文件将每行文本解析为Point对象。然后，我们通过对当前w的函数求和，对points重复运行map和reduce以计算每一步的梯度。在迭代的过程中将points保存在内存中可以获得20倍的加速，这将在6.1节展示。</p>\n<h4 id=\"3-2-2-PageRank\"><a href=\"#3-2-2-PageRank\" class=\"headerlink\" title=\"3.2.2 PageRank\"></a>3.2.2 PageRank</h4><p>更复杂的数据共享模式发生在PageRank [6]。算法通过累加在文件中对每个文件的应用次数迭代地更新每一文件的rank。在每次迭代时，每个文件都向其邻居发送r/n的贡献值，r是其排名，n是其邻居的数量。然后通过α/N + (1 − α)∑ci式子更新排名，其中求和是它所收到的贡献值，而N是文件的总数。我们可以通过如下代码Spark中实现PageRank：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Load graph as an RDD of (URL, outlinks) pairs</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> links = spark.textFile(...).map(...).persist()</span><br><span class=\"line\"><span class=\"keyword\">var</span> ranks = <span class=\"comment\">// RDD of (URL, rank) pairs</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"type\">ITERATIONS</span>) &#123;</span><br><span class=\"line\"><span class=\"comment\">// Build an RDD of (targetURL, float) pairs</span></span><br><span class=\"line\">  <span class=\"comment\">// with the contributions sent by each page</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> contribs = links.join(ranks).flatMap &#123;</span><br><span class=\"line\">    (url, (links, rank)) =&gt;</span><br><span class=\"line\">      links.map(dest =&gt; (dest, rank/links.size))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Sum contributions by URL and get new ranks</span></span><br><span class=\"line\">  ranks = contribs.reduceByKey((x,y) =&gt; x+y)</span><br><span class=\"line\">             .mapValues(sum =&gt; a/<span class=\"type\">N</span> + (<span class=\"number\">1</span>-a)*sum)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060154806/0\" alt></p>\n<p>该程序产生图3中的RDD lineage图。在每次迭代中，我们基于来自先前的迭代的contribs和ranks以及静态links数据集来创建新的排名数据集【请注意，尽管RDD是不可变的，但程序中的变量rank和contribs指向每次迭代时的不同RDDs】。图3的一个有趣特征是随着迭代数目的增加而不断增长。因此，在具有多次迭代的作业中，可能需要可靠地复制某些版本的ranks以减少故障恢复时间[20]。用户可以使用RELIABLE标志调用persist来执行此操作。但请注意，不需要复制links数据集，因为可以通过在输入文件的块上重新运行map来有效地重建它的分区。此数据集通常比ranks大得多，因为每个文档都有许多链接，但只有一个数字作为其排名，因此使用lineage恢复它比使用checkpoint回复程序整个内存中状态的系统更节省时间。</p>\n<p>最后，我们可以通过控制RDDs的分区来优化PageRank中的通信。如果我们指定links的分区（例如，通过节点间的URL对link lists进行哈希分区），以相同的方式对ranks进行分区，这样就确保links和ranks之间的join操作不需要通信（因为每个URL的排名将与其link list在同一台机器上）。我们还可以编写自定义分区程序类来对相互链接的页面进行分组（例如，按域名对URL进行分区）。在定义links时，可以通过调用partitionBy来表示这两种优化：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">links = spark.textFile(...).map(...)</span><br><span class=\"line\">             .partitionBy(myPartFunc).persist()</span><br></pre></td></tr></table></figure>\n<p>在此初始化之后，links和ranks之间的join操作会自动地将每个URL的贡献值聚集到它link lists所在的机器上，在这台机器上计算新的排名并与它的links进行join操作。这种跨迭代的一致分区是Pregel等专用框架中的主要优化之一。 RDDs让用户直接表达这一目标。</p>\n<h2 id=\"4-表示RDDs\"><a href=\"#4-表示RDDs\" class=\"headerlink\" title=\"4 表示RDDs\"></a>4 表示RDDs</h2><p>提供RDD作为抽象的挑战之一是为它们选择一种能够跟踪各种转换操作的lineage的表示。理想情况下，实现RDD的系统应该提供尽可能丰富的一组转换操作（例如，表2中的转换操作），并让用户以任意方式组合它们。我们为RDD提出了一个简单的基于图的表示，以达到这些目标。我们在Spark中使用这种表示来支持各种转换，而无需为调度器添加针对每个转换的特殊逻辑，这大大简化了系统设计。简而言之，我们建议通过一个公共接口来表示每个RDD，这个接口包含五条信息：一组分区，它们是数据集的原子部分；父RDDs（parent RDDs）的一组依赖关系；基于其父RDDs计算数据集的函数；有关其分区方案和数据放置的元数据。例如，表示HDFS文件的RDD具有文件的每个块的分区，并且知道每个块所在的机器。同时，对于这个RDD进行map操作的结果具有相同的分区，但是在计算其元素时将在父数据上应用map方法。我们在表3中总结了这个接口。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_234219102_1557060167174/0\" alt></p>\n<p>设计此接口时最有趣的问题是如何表示RDDs之间的依赖关系。我们发现将依赖关系分为两类是足够且有用的：窄依赖关系（narrow dependencies），其中父RDD的每个分区最多由子RDD的一个分区使用；宽依赖关系（wide dependencies），其中多个子RDD分区可能依赖一个父DD分区。例如，map导致窄依赖关系，而join导致宽依赖关系（除非父RDD是哈希分区的）。图4显示了其他示例。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_256675758_1557060251325/0\" alt></p>\n<p>这样区分有两个原因。首先，窄依赖关系允许在一个集群节点上进行流水线执行，这可以计算所有父分区。例如，可以逐个元素地应用map，然后应用filter操作。相比之下，宽依赖关系要求所有父分区的数据都已经计算完成，并使用类似MapReduce的操作在节点之间进行shuffle。其次，节点故障后的恢复在窄依赖时更有效，因为只需要重新计算丢失的对应的父分区，并且可以在不同节点上并行地重新计算它们。相反，在具有宽依赖的lineage图中，单个故障节点可能导致RDD的所有祖先丢失某些分区，从而需要完全重新执行计算。</p>\n<p>RDDs的这个通用接口使得可以在少于20行代码中实现Spark中的大多数转换操作。实际上，即使是新的Spark用户也已经实现了新的转换（例如，采样和各种类型的join），而不用知道调度器的细节。我们在下面阐述一些RDD实现。</p>\n<p><strong>HDFS文件：</strong><br>我们样本中的输入RDDs是HDFS中的文件。对于这些RDDs，partitions为文件的每个块返回一个分区（块的偏移量存储在每个Partition对象中），preferredLocations给出块所在的节点，iterator读取块。</p>\n<p><strong>map：</strong><br>在任何RDD上调用map都会返回MappedRDD对象。该操作传递一个函数参数给map，对父RDD上的记录按照iterator的方式执行这个函数，并返回一组符合条件的父RDD分区及其位置。</p>\n<p><strong>union：</strong><br>在两个RDD上执行union操作，返回两个父RDD分区的并集。通过相应父RDD上的窄依赖关系计算每个子RDD分区【7注意union操作不会过滤重复值】。</p>\n<p><strong>join：</strong><br>对两个RDD执行join操作可能产生窄依赖（如果这两个RDD拥有相同的哈希分区或范围分区），可能是宽依赖，也可能两种依赖都有（比如一个父RDD有分区，而另一父RDD没有）。在任何一种情况下，输出RDD都有一个分区程序（从父项继承的分区程序或默认的散列分区程序）。</p>\n<h2 id=\"5-实现\"><a href=\"#5-实现\" class=\"headerlink\" title=\"5 实现\"></a>5 实现</h2><p>我们大约用14000行scala代码实现了Spark。该系统运行在Mesos集群管理器[17]上，允许它与Hadoop，MPI和其他应用程序共享资源。每个Spark程序作为单独的Mesos应用程序运行，具有自己的驱动程序（master）和工作程序（workers），这些应用程序之间的资源共享由Mesos管理的。Spark可以使用Hadoop现有的输入插件API从任何Hadoop输入源（例如，HDFS或HBase）读取数据，并在未经修改的Scala版本上运行。</p>\n<p>我们现在简要介绍系统中几个技术上有趣的部分：我们的作业调度程序（第5.1节），允许交互式使用的Spark解释器（第5.2节），内存管理（第5.3节）和支持检查点（第5.4节）。</p>\n<h3 id=\"5-1-作业调度\"><a href=\"#5-1-作业调度\" class=\"headerlink\" title=\"5.1 作业调度\"></a>5.1 作业调度</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060276767/0\" alt></p>\n<p>Spark的调度程序使用我们在第4节所述的RDD表示。</p>\n<p>总的来说，我们的调度程序类似于Dryad的[19]，但是另外考虑了RDDs在内存中持久化的分区。每当用户在RDD上运行动作（例如，count或save）时，该调度程序就会检查该RDD的lineage图以构建要执行的stage的DAG，如图5所示。每个阶段包含尽可能多的具有窄依赖的流水线转换。阶段的边界是宽依赖所需的shuffle操作，或任何已经计算过的分区，它们可以跳过父RDD的计算。然后，调度程序启动任务以计算每个阶段中丢失的分区，直到计算出目标RDD为止。</p>\n<p>我们的调度程序基于数据位置使用延迟调度将任务分配给机器[32]。如果任务需要处理节点内存中可用的分区，我们会将其发送到该节点。否则，如果任务处理包含RDD提供优选位置的分区（例如，HDFS文件），我们将其发送给那些分区。</p>\n<p>对于宽依赖关系（即，shuffle依赖关系），我们目前在包含父分区的节点上物化中间记录以简化故障恢复，就像MapReduce物化map输出一样。</p>\n<p>如果任务失败，只要其阶段的父项仍然可用，我们就会在另一个节点上重新运行它。如果某些阶段变得不可用（例如，因为来自shuffle的“map side”的输出丢失），我们重新提交任务以并行计算丢失的分区。我们还不能解决调度程序的失败，尽管复制RDD的lineage图会更简单。</p>\n<p>最后，尽管Spark中的所有计算当前都是为响应驱动程序中调用的操作而运行的，但我们也在尝试让集群上的任务（例如，映射）调用lookup操作，其提供根据键值对哈希分区的RDDs中的元素进行随机获取。在这种情况下，任务需要告诉调度程序在缺少时计算所需的分区。</p>\n<h3 id=\"5-2-解释器整合\"><a href=\"#5-2-解释器整合\" class=\"headerlink\" title=\"5.2 解释器整合\"></a>5.2 解释器整合</h3><p>Scala包含一个类似于Ruby和Python的交互式shell。鉴于内存数据的延迟较低，我们希望让用户从解释器主动运行Spark来查询大数据集。</p>\n<p>Scala解释器通常通过为用户键入的每一行编译一个类，将其加载到JVM中，并在其上调用函数来操作。该类包含一个单例对象，该对象包含该行上的变量或函数，并在初始化方法中运行行代码。例如，如果用户输入代码var x = 5，接着又输入println(x)，则解释器会定义一个包含x的Line1类，并将第2行编译为println(Line1.getInstance().x)。</p>\n<p>在Spark中我们对解释器做了两点改动：</p>\n<p>1.类传输：解释器能够支持基于HTTP传输类字节码，这样worker节点就能获取输入每行代码对应的类的字节码。</p>\n<p>2.改进的代码生成逻辑：通常每行上创建的单例对象通过对应类上的静态方法进行访问。也就是说，如果要序列化一个闭包，它引用了前面代码行中变量，比如上面的例子Line1.x，Java不会根据对象关系传输包含x的Line1实例。所以worker节点不会收到x。我们将这种代码生成逻辑改为直接引用各个行对象的实例。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060301153/0\" alt></p>\n<p>图6显示了在我们更改之后，解释器如何将用户键入的一组行转换为Java对象。</p>\n<p>Spark解释器便于跟踪处理大量对象关系引用，并且便利了HDFS数据集的研究。我们计划以Spark解释器为基础，开发提供高级数据分析语言支持的交互式工具，比如SQL。</p>\n<h3 id=\"5-3-内存管理\"><a href=\"#5-3-内存管理\" class=\"headerlink\" title=\"5.3 内存管理\"></a>5.3 内存管理</h3><p>Spark提供了三种持久化RDD的选项：反序列化Java对象的内存存储，序列化数据的内存存储和硬盘存储。第一个选项提供最快的性能，因为Java VM可以原生访问每个RDD元素。第二个选项允许用户在空间有限时选择比Java对象图更高效的内存表示，但代价是性能较低【成本取决于应用程序每个字节数据的计算量，但轻量级处理的最大值可提升2倍】。第三个选项对于太大而无法保留在RAM中但在每次使用时重新计算成本高昂的RDD非常有用</p>\n<p>为了管理可用的有限内存，我们在RDDs层面上使用LRU替换策略。当计算新的RDD分区但没有足够的空间来存储它时，我们从最近最少的RDD中替换出一个分区，除它这与具有新分区的RDD相同。在这种情况下，我们将旧分区保留在内存中，以防止来自同一RDD的分区循环进出。这很重要，因为大多数操作都会在整个RDD上运行任务，因此很可能将来需要已经在内存中的分区。到目前为止，我们发现此默认策略在所有应用程序中都能正常运行，但我们还通过每个RDD的“持久化优先级”为用户提供进一步控制。</p>\n<p>最后，群集上的每个Spark实例当前都有自己独立的内存空间。在未来的工作中，我们计划通过统一的内存管理器来研究跨Spark实例共享RDDs。</p>\n<h3 id=\"5-4-对检查点的支持\"><a href=\"#5-4-对检查点的支持\" class=\"headerlink\" title=\"5.4 对检查点的支持\"></a>5.4 对检查点的支持</h3><p>尽管在故障之后可以始终使用lineage来恢复RDDs，但对于具有长lineage的RDDs来说，这种恢复可能是耗时的。因此，将一些RDDs checkpoint到稳定存储可能会有所帮助。</p>\n<p>通常，检查点技术对于包含宽依赖关系的长lineage图的RDDs很有用，例如我们的PageRank示例（第3.2.2节）中的rank数据集。在这些情况下，集群中的节点故障可能导致每个父RDD丢失一些数据片段，从而需要完全重新计算[20]。相反，对于对稳定存储中的数据具有窄依赖的RDDs，例如我们的逻辑回归示例（第3.2.1节）中的points和PageRank中的link lists，检查点技术可能没多大用。如果节点发生故障，则可以在其他节点上并行重新计算从这些RDD中丢失的分区，而这只是复制整个RDD的成本的一小部分。</p>\n<p>Spark目前提供了一个用于检查点技术的API（一个在persist中的REPLICATE标志），但是由用户决定哪些数据使用检查点。但是，我们还在研究如何进行自动检查。因为我们的调度程序知道每个数据集的大小以及首次计算它所花费的时间，所以它应该能够选择一组最佳RDDs来检查点以最小化系统恢复时间[30]。</p>\n<p>最后，要强调的一点是，RDDs的只读特性使它们比通用共享存储更容易进行检查。由于一致性问题不需要考虑，因此可以在后台写出RDD，而无需程序暂停或采用分布式快照方案。</p>\n<h2 id=\"6-评估\"><a href=\"#6-评估\" class=\"headerlink\" title=\"6 评估\"></a>6 评估</h2><p>我们通过Amazon EC2上的一系列实验以及用户应用程序的基准评估了Spark和RDDs。总的来说，我们的结果显示如下：</p>\n<ul>\n<li>在迭代机器学习和图形应用程序中，Spark的性能比Hadoop高出20倍。加速来自于通过将数据作为Java对象存储在内存中来避免I / O和反序列化成本。</li>\n<li>我们的用户编写的应用程序可以很好地执行和扩展。特别是，我们使用Spark分析报表比在Hadoop上运行快40倍。</li>\n<li>当节点发生故障时，Spark可以通过仅重建丢失的RDD分区来快速恢复。</li>\n<li>Spark可用于交互查询1 TB数据集，延迟仅为5-7秒。</li>\n</ul>\n<p>我们首先与Hadoop进行迭代机器学习（第6.1节）和PageRank（第6.2节）的基准比较。然后，我们评估Spark中的故障恢复（第6.3节）以及数据集不适合存储时的行为（第6.4节）。最后，我们讨论了用户应用（第6.5节）和交互式数据挖掘（第6.6节）的结果。</p>\n<p>除非另有说明，否则我们的测试使用m1.xlarge EC2节点，其中包含4个内核和15 GB RAM。我们使用HDFS进行存储，具有256 MB块。在每次测试之前，我们清除了OS缓冲区高速缓存，以准确测量IO成本。</p>\n<h3 id=\"6-1-迭代式机器学习应用\"><a href=\"#6-1-迭代式机器学习应用\" class=\"headerlink\" title=\"6.1 迭代式机器学习应用\"></a>6.1 迭代式机器学习应用</h3><p>我们实现了两个迭代机器学习应用程序，逻辑回归和k-means，以比较以下系统的性能：</p>\n<ul>\n<li>Hadoop：The Hadoop 0.20.2 stable release。</li>\n<li>HadoopBinMem：在首轮迭代中执行预处理，通过将输入数据转换成为开销较低的二进制格式来减少后续迭代过程中文本解析的开销，在HDFS中加载到内存。</li>\n<li>Spark：基于RDDs的实现。</li>\n</ul>\n<p>我们使用25-100台机器在100 GB数据集上对这两个算法进行了10次迭代。两个应用程序之间的关键区别是它们每个数据字节执行的计算量。k-means的迭代时间由计算决定，而逻辑回归的计算密集度较低，但对反序列化和I / O花费的时间更敏感。</p>\n<p>由于典型的学习算法需要数十次迭代才能收敛，因此我们分别报告第一次迭代和后续迭代的时间。我们发现通过RDDs共享数据可以大大加快未来的迭代速度。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1280192908_1557060314454/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060328473/0\" alt></p>\n<p><strong>首次迭代</strong><br>所有三个系统在第一次迭代中从HDFS读取文本输入。如图7中的浅色柱状图所示，Spark在实验中比Hadoop要快一些。如图7中的灯条所示，Spark在实验中比Hadoop要快一些。HadoopBinMem是最慢的，因为它通过一个额外的MapReduce作业将数据转换成二进制格式，并必须通过网络在HDFS结点间复制数据。</p>\n<p><strong>后续迭代</strong><br>图7还显示了后续迭代的平均运行时间，而图8显示了其随集群大小变化而产生的时间变化。对于逻辑回归，Spark在100台机器上分别比Hadoop和HadoopBinMem快25.3倍和20.7倍。对于更加计算密集型的k-means应用程序来说，Spark仍然实现了1.9倍到3.2倍的加速。</p>\n<p><strong>理解速度提升</strong><br>我们惊讶地发现Spark甚至比内存存储二进制数据的Hadoop（HadoopBinMem）还要快20倍。在HadoopBinMem中，我们使用了Hadoop的标准二进制格式（SequenceFile）和256 MB大小的超大块，并且我们强制HDFS的数据目录位于内存文件系统中。但是，由于以下几个因素，Hadoop仍然运行缓慢：</p>\n<p>1.Hadoop软件堆栈的最小开销，</p>\n<p>2.提供数据时HDFS的开销，</p>\n<p>3.将二进制记录转换为可用的内存中Java对象的反序列化成本。</p>\n<p>我们依次研究了这些因素。为了估测1，我们运行空的Hadoop作业，仅仅执行作业的初始化、启动任务、清理工作就至少耗时25秒。对于2，我们发现为了服务每一个HDFS数据块，HDFS进行了多次复制以及计算校验和操作。</p>\n<p>最后，为了估测3，我们在一台机器上运行微基准测试，以256 MB输入的各种格式运行逻辑回归计算。特别是，我们比较了处理来自HDFS（HDFS堆栈中的开销将给出）和内存本地文件（内核可以非常有效地将数据传递给程序）的文本和二进制输入的时间。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060341567/0\" alt></p>\n<p>结果如图9所示。内存中HDFS和本地文件之间的差异表明，通过HDFS读取产生了2秒的开销，即使数据存储在本地机器上也是如此。文本和二进制输入之间的差异表明解析开销为7秒。最后，即使从内存文件中读取，将预解析的二进制数据转换为Java对象也需要3秒钟，这仍然几乎与逻辑回归本身一样开销高昂。通过将RDD元素直接存储为内存中的Java对象，Spark可以避免所有这些开销。</p>\n<h3 id=\"6-2-PageRank\"><a href=\"#6-2-PageRank\" class=\"headerlink\" title=\"6.2 PageRank\"></a>6.2 PageRank</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060351959/0\" alt></p>\n<p>我们使用54 GB Wikipedia导出数据比较了Spark与Hadoop 进行PageRank的性能。PageRank算法通过10轮迭代处理了大约400万文章的链接图数据。图10展示了单独的内存存储使Spark在30个节点上的速度比Hadoop提高了2.4倍。此外，如第3.2.2节所述，控制RDD的分区以使其在迭代中保持一致，将提高到7.4倍。加速也能几乎线性地扩展到60个节点上。</p>\n<p>我们还评估了使用Spark实现Pregel版本的PageRank的性能，结果将在7.1节展示。迭代时间与图10中的相似，但是更长约4秒，因为Pregel在每次迭代时运行一个额外的操作，让顶点“投票”是否完成工作。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060363143/0\" alt></p>\n<h3 id=\"6-3-错误恢复\"><a href=\"#6-3-错误恢复\" class=\"headerlink\" title=\"6.3 错误恢复\"></a>6.3 错误恢复</h3><p>我们评估了k-means应用程序中节点故障后使用lineage重建RDD分区的开销。图11比较了正常操作场景中75节点集群上k-means的10次运行的运行时间，其中一个节点在第6次迭代开始时失败。另一个没有任何故障，每次迭代包含400个任务，处理100 GB的数据。</p>\n<p>直到第5次迭代结束，迭代时间约为58秒。在第6次迭代中，其中一台机器被杀死，导致在该机器上运行的任务和存储在那里的RDD分区丢失。Spark在其他机器上并行重新执行这些任务，他们通过lineage重新读取相应的输入数据和重建的RDD，导致操作时间增加到80秒。一旦重建丢失的RDD分区，迭代时间就会回落到58秒。</p>\n<p>请注意，使用基于检查点的故障恢复机制，恢复可能需要重新运行至少几次迭代，具体取决于检查点的频率。此外，系统需要通过网络复制应用程序的100 GB工作集（文本输入数据转换为二进制），并且要么消耗两倍于Spark的内存以将其复制到RAM中，要么必须等待写入100 GB到磁盘。相比之下，我们示例中RDD的lineage图的大小都小于10 KB。</p>\n<h3 id=\"6-4-内存不足时表现\"><a href=\"#6-4-内存不足时表现\" class=\"headerlink\" title=\"6.4 内存不足时表现\"></a>6.4 内存不足时表现</h3><p>到现在为止，我们能保证集群中的每个节点都有足够的内存去缓存迭代过程中使用的RDDs。一个自然的问题是，如果没有足够的内存来存储作业的数据，Spark是如何运行的。在本实验中，我们将Spark配置为不使用超过一定百分比的内存来在每台机器上存储RDD。图12中，我们为的逻辑回归提供了各种存储空间的配置。我们发现性能在控件降低时缓慢降低。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060374238/0\" alt></p>\n<h3 id=\"6-5-用Spark构建的用户应用程序\"><a href=\"#6-5-用Spark构建的用户应用程序\" class=\"headerlink\" title=\"6.5 用Spark构建的用户应用程序\"></a>6.5 用Spark构建的用户应用程序</h3><p><strong>内存分析</strong><br>Conviva Inc是一家视频发行公司，它使用Spark加速了之前在Hadoop上运行的大量数据分析报告。例如，一份报告作为一系列Hive [1]查询运行，这些查询计算出了客户的各种统计信息。这些查询都在相同的数据子集上工作（与客户提供的过滤器匹配的记录），但在不同的分组字段上执行了聚合（平均值，百分位数和COUNT DISTINCT），需要使用单独的MapReduce作业。通过在Spark中实现查询并将一次共享的数据子集加载到RDD中，该公司能够将报告速度提高40倍。在Hadoop集群上花费20小时的200 GB压缩数据的报告现在仅使用两台Spark计算机就能在30分钟内运行完成。此外，Spark程序只需要96 GB的RAM，因为它只存储与RDD中客户的过滤器匹配的行和列，而不是整个解压缩文件。</p>\n<p><strong>城市交通模型</strong><br>在Berkeley的Mobile Millennium项目[18]中，基于一系列分散的汽车GPS监测数据，研究人员使用并行化机器学习算法来推算公路交通拥堵状况。数据来自市区10000个互联的公路线路网，还有600000个由汽车GPS装置采集到的样本数据，这些数据记录了汽车在两个地点之间行驶的时间（每一条路线的行驶时间可能跨多个公路线路网）。使用一个交通模型，通过推算跨多个公路网行驶耗时预期，系统能够估算拥堵状况。研究人员使用Spark实现了一个可迭代的EM算法，其中包括向Worker节点广播路线网络信息，在E和M阶段之间执行reduceByKey操作，应用从20个节点扩展到80个节点（每个节点4核），如图13（a）所示：</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060384098/0\" alt></p>\n<p><strong>推特垃圾邮件分类</strong><br>Berkeley的Monarch项目[29]使用Spark识别Twitter消息上的垃圾链接。他们在Spark上实现了一个类似6.1小节中示例的Logistic回归分类器，不同的是使用分布式的reduceByKey操作并行对梯度向量求和。图13（b）显示了基于50G数据子集训练训练分类器的结果，整个数据集是250000的URL、至少10^7个与网络相关的特征/维度，内容、词性与访问一个URL的页面相关。随着节点的增加，这并不像交通应用程序那样近似线性，主要是因为每轮迭代的固定通信代价较高。</p>\n<h3 id=\"6-6-交互式数据挖掘\"><a href=\"#6-6-交互式数据挖掘\" class=\"headerlink\" title=\"6.6 交互式数据挖掘\"></a>6.6 交互式数据挖掘</h3><p>为了演示Spark交互式查询大数据集的能力，我们用它来分析1TB的维基百科页面浏览日志（2年的数据）。在本次实验中，我们使用了100 m2.4xlarge EC2实例，每个实例有8个内核和68 GB内存。在整个输入数据集上简单地查询如下内容以获取页面浏览总数：（1）全部页面；（2）页面的标题能精确匹配给定的关键词；（3）页面的标题能部分匹配给定的关键词。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060393080/0\" alt></p>\n<p>图14显示了分别在整个、1/2、1/10的数据上查询的响应时间，甚至1TB数据在Spark上查询仅耗时5-7秒，这比直接操作磁盘数据快几个数量级。例如，从磁盘上查询1TB数据耗时170秒，这表明了RDD缓存使得Spark成为一个交互式数据挖掘的强大工具。</p>\n<h2 id=\"7-讨论\"><a href=\"#7-讨论\" class=\"headerlink\" title=\"7 讨论\"></a>7 讨论</h2><p>虽然RDD由于其不可变性和粗粒度转换似乎提供有限的编程接口，但我们发现它们适用于广泛的应用。特别地，RDDs可以表达数量惊人的集群编程模型，这些模型迄今已被提议作为单独的框架，允许用户在一个程序中组合这些模型（例如，运行MapReduce操作来构建图形，然后在其上运行Pregel）并在它们之间共享数据。在本节中，我们将讨论RDDs可以表达哪些编程模型以及它们如此广泛适用的原因（第7.1节）。此外，我们讨论了我们正在探索的RDDs中lineage信息的另一个好处，能够促进这些模型的调试（第7.2节）。</p>\n<h3 id=\"7-1-表达现有的编程模型\"><a href=\"#7-1-表达现有的编程模型\" class=\"headerlink\" title=\"7.1 表达现有的编程模型\"></a>7.1 表达现有的编程模型</h3><p>RDDs可以有效地表达迄今为止独立提出的许多集群编程模型。“有效”，意味着RDDs不仅可以用于生成与这些模型中编写的程序相同的输出，而且RDDs还可以实现这些框架具有的优化，例如将特定数据保存在内存中，将其分区为最大限度地减少通信，并有效地从故障中恢复。使用RDD表达的模型包括：</p>\n<p><strong>MapReduce：</strong> 可以使用Spark中的flatMap和groupByKey操作表示此模型，如果存在组合器，则可以使用reduceByKey表示.</p>\n<p><strong>DryadLINQ：</strong><br>与更普遍的Dryad运行时相比，DryadLINQ系统提供了比MapReduce更广泛的运算符，但这些都是直接对应于Spark（map，groupByKey，join等）中可用的RDD转换的批量运算符。</p>\n<p><strong>SQL：</strong><br>与DryadLINQ表达式一样，SQL查询对记录集执行数据并行操作。</p>\n<p><strong>Pregel：</strong><br>Google的Pregel [22]是迭代图形应用程序的专用模型，起初与其他系统中的面向集合的编程模型完全不同。在Pregel中，程序作为一系列协调的“supersteps”运行。在每个supersteps中，图中的每个顶点都运行一个用户函数，可以更新与顶点相关的状态，更改图形拓扑，并将消息发送到其他顶点用于下一个superstep。该模型可以表达许多图算法，包括最短路径，二分匹配和PageRank。</p>\n<p>让我们用RDDs实现这个模型的关键点是Pregel将相同的用户函数应用于每次迭代的所有顶点。因此，我们可以将每次迭代的顶点状态存储在RDD中，并执行批量转换（flatMap）以应用此函数并生成消息的RDD。之后可以将该RDD与顶点状态连接操作以表示消息的转换。同样重要的是，RDDs允许我们像Pregel那样将顶点状态保存在内存中，通过控制其分区来最小化通信，并支持故障时的部分恢复。我们在Spark上实现了Pregel，其作为200行库，并向读者推荐[33]以获取更多详细信息。</p>\n<p>迭代式MapReduce： 最近提出的几个系统，包括HaLoop [7]和Twister [11]，提供了迭代的MapReduce模型，用户可以为系统提供一系列MapReduce作业循环执行。系统使数据在迭代中保持一致，Twister也可以将其保存在内存中。两种优化都很容易用RDDs表达，我们能够使用Spark将HaLoop实现为200行库。</p>\n<p>流式批处理： 研究人员最近提出了几种增量处理系统，用于定期用新数据更新结果的应用[21,15,4]。例如，每15分钟更新一次广告点击统计数据的应用程序应该能够将前一个15分钟窗口的中间状态与新日志中的数据相结合。这些系统执行类似于Dryad的批量操作，但将应用程序状态存储在分布式文件系统中。将中间状态置于RDDs中将加速其处理。</p>\n<p><strong><em>解释RDDs的表达性能</em></strong><br>为什么RDD能够表达这些不同的编程模型？原因是对RDD的限制对许多并行应用程序几乎没有影响。特别是，尽管RDDs只能通过批量转换创建，但许多并行程序本质上就是将相同的操作应用于许多记录，所以使其易于用RDDs表达。类似地，RDD的不变性不是一个障碍，因为可以创建多个RDD来表示同一数据集的不同版本。实际上，今天的许多MapReduce应用程序都运行在不允许更新文件的文件系统上，例如HDFS。</p>\n<p>最后一个问题是为什么以前的框架没有提供相同的一般性。我们认为这是因为这些系统探索了MapReduce和Dryad无法很好处理的特定问题，例如迭代，但是没有观察到这些问题的常见原因是缺乏数据共享抽象。</p>\n<h3 id=\"7-2-利用RDDs进行调试\"><a href=\"#7-2-利用RDDs进行调试\" class=\"headerlink\" title=\"7.2 利用RDDs进行调试\"></a>7.2 利用RDDs进行调试</h3><p>虽然我们最初设计的RDD在确定性方面可以重新计算以实现容错，但这个属性也可以简化调试。特别是，通过记录在作业期间创建的RDD的lineage，可以（1）稍后重建这些RDD并让用户以交互方式查询它们，以及（2）在单个过程调试中重新执行作业的任何任务，通过重新计算它所依赖的RDD分区。与通用分布式系统[13]的传统重放调试器不同，不必捕获或推断跨多节点的时间顺序，这种方法几乎不增加记录开销，因为只需要记录RDD lineage图【9 与这些系统不同，基于RDD的调试器不会重放用户函数中的非确定性行为（例如，非确定性映射），但它至少可以通过校验和数据来报告它】。我们目前正在开发基于这些想法的Spark调试器[33]。</p>\n<h2 id=\"8-相关工作\"><a href=\"#8-相关工作\" class=\"headerlink\" title=\"8 相关工作\"></a>8 相关工作</h2><p><strong>集群编程模型：</strong><br>集群编程模型的相关工作分为几类。首先，MapReduce [10]，Dryad [19]和Ciel [23]等数据流模型支持丰富的运算符集，用于处理数据，但通过稳定的存储系统共享数据。RDDs代表比稳定存储更有效的数据共享抽象，因为它们避免了数据复制，I / O和序列化的成本【10 请注意，在像RAMCloud [25]这样的内存数据存储中运行MapReduce / Dryad仍然需要数据复制和序列化，这对于某些应用程序来说可能代价很高，如6.1节所示】。</p>\n<p>其次，数据流系统的几个高级编程接口，包括DryadLINQ [31]和FlumeJava [8]，提供了语言集成的API，用户通过map和join等操作符操作“并行集合”。但是，在这些系统中，并行集合表示磁盘上的文件或用于表示查询计划的临时数据集。虽然系统会在相同的操作符查询间流水式地处理数据（如，一个map操作接着一个map操作），但是它们并不能在各个查询之间有效地共享数据。我们在并行收集模型上基于Spark的API，因为它的方便性，并没有增加语言集成接口的新颖性，但通过提供RDD作为此接口背后的存储抽象，我们允许它支持更广泛的应用程序。</p>\n<p>第三类系统为需要数据共享的特定类别的应用程序提供高级接口。例如，Pregel [22]支持迭代图应用，而Twister [11]和HaLoop [7]是迭代MapReduce运行时。但是，这些框架隐式地为他们支持的计算模式提供数据共享，并且不提供一般抽象来供用户选择。例如，用户不能使用Pregel或Twister将数据集加载到内存中，然后决定在其上运行哪个查询，RDD明确地提供分布式存储抽象，因此可以支持这些专用系统不支持的应用，例如交互式数据挖掘。</p>\n<p>最后，一些系统暴露共享可变状态以允许用户执行内存计算。例如，Piccolo [27]允许用户运行并行功能以读取和更新分布式哈希表中的单元格。分布式共享存储（DSM）系统[24]和键值存储如RAMCloud [25]提供一个相似的模型。RDD在两个方面与这些系统不同。首先，RDD基于运算符（如map，sort和join）提供更高级别的编程接口，而Piccolo和DSM中的接口只是对表格单元格的读取和更新。其次，Piccolo和DSM系统通过检查点和回滚实现恢复，这比许多应用程序中基于lineage策略的RDDs更昂贵。最后，正如第2.3节所讨论的那样，RDD还提供了其他优于DSM的优势，例如straggler缓解。</p>\n<p><strong>缓存系统：</strong><br>Nectar [12]可以通过程序分析识别常见的子表达式，在DryadLINQ作业中重用中间结果[16]。这种能力对于添加到基于RDD的系统非常有吸引力。但是，Nectar不提供内存中缓存（它将数据放在分布式文件系统中），也不允许用户明确控制要持久化的数据集以及如何对它们进行分区。Ciel [23]和FlumeJava [8]同样可以缓存任务结果，但不提供内存缓存或显式控制缓存哪些数据。Ananthanarayanan等。建议在分布式文件系统中添加内存缓存，以利用数据访问的时间和空间局部性[3]。虽然此解决方案可以更快地访问文件系统中已有的数据，但它不像在RDDs中那样有效地在一个应用中共享中间结果，因为它仍然需要应用程序在不同阶段间将这些结果写入文件系统。</p>\n<p><strong>Lineage：</strong><br>记载数据的lineage或起源信息长期以来一直是科学计算和数据库中的研究课题，应用于解释结果，允许数据可以被别的数据重建，同时如果在工作流中出现了bug或者数据及丢失了，能够重新计算得到数据。我们推荐读者阅读[5]和[9]来了解这些工作。RDDs提供并行编程模型，其中获取细粒度的lineage成本低廉，因此可用于故障恢复。</p>\n<p>我们基于lineage的恢复机制也类似于MapReduce和Dryad中计算（作业）中使用的恢复机制，它跟踪任务的DAG之间的依赖关系。但是，在这些系统中，谱系（lineage）信息在作业结束后丢失，需要使用复制的存储系统来跨计算共享数据。相比之下，RDDs应用lineage来有效地跨计算保留内存数据，而无需复制和磁盘I / O的成本。</p>\n<p><strong>关系型数据库：</strong><br>RDDs在概念上类似于数据库中的视图，而持久化RDDs类似于物化视图[28]。但是，与DSM系统一样，数据库通常允许对所有记录进行细粒度的读写访问，需要记录操作和数据以实现容错，并且需要额外的开销来维护一致性。RDD的粗粒度转换模型不需要这些开销。</p>\n<h2 id=\"9-结论\"><a href=\"#9-结论\" class=\"headerlink\" title=\"9 结论\"></a>9 结论</h2><p>我们提供了弹性分布式数据集（RDDs），这是一种高效，通用和容错的用于在集群应用程序中共享数据的抽象。RDDs可以表达各种并行应用程序，包括已经提出用于迭代计算的许多专用编程模型，以及这些模型还未实现的新应用程序。与现有的集群存储抽象（需要数据复制以实现容错）不同，RDDs提供基于粗粒度转换的API，使其能够使用lineage来有效地恢复数据。我们在Spark中实现了RDDs，它在迭代应用程序中的性能比Hadoop高出20倍，并且可以交互式查询数百GB的数据。</p>\n<p>我们在spark-project.org上提供了开源Spark作为可扩展数据分析和系统研究的工具。</p>\n<h2 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h2><p>我们感谢第一批Spark用户，包括Tim Hunter，Lester Mackey，Dilip Joseph和Jibin Zhan，他们在实际应用中尝试我们的系统，提供了许多好的建议，并指出了一些研究中的挑战。我们还要感谢我们的指导者Ed Nightingale以及审核的反馈。这项研究部分由Berkeley AMP Lab<br>支持，由 Google, SAP, Amazon Web Services, Cloudera, <strong><em>Huawei</em></strong>, IBM, Intel, Microsoft, NEC, NetApp 和 VMWare，DARPA，the Natural Sci- ences 和 Engineering Research Council of Canada赞助。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p>[1] Apache Hive. <a href=\"http://hadoop.apache.org/hive\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/hive</a>.</p>\n<p>[2] Scala. <a href=\"http://www.scala-lang.org\" target=\"_blank\" rel=\"noopener\">http://www.scala-lang.org</a>.</p>\n<p>[3] G.Ananthanarayanan,A.Ghodsi,S.Shenker,andI.Stoica.<br>Disk-locality in datacenter computing considered irrelevant. In<br>HotOS ’11, 2011.</p>\n<p>[4] P.Bhatotia,A.Wieder,R.Rodrigues,U.A.Acar,and<br>R. Pasquin. Incoop: MapReduce for incremental computations.<br>In ACM SOCC ’11, 2011.</p>\n<p>[5] R.BoseandJ.Frew.Lineageretrievalforscientificdata<br>processing: a survey. ACM Computing Surveys, 37:1–28, 2005.</p>\n<p>[6] S.BrinandL.Page.Theanatomyofalarge-scalehypertextual<br>web search engine. In WWW, 1998.</p>\n<p>[7] Y.Bu,B.Howe,M.Balazinska,andM.D.Ernst.HaLoop:<br>efficient iterative data processing on large clusters. Proc. VLDB<br>Endow., 3:285–296, September 2010.</p>\n<p>[8] C.Chambers,A.Raniwala,F.Perry,S.Adams,R.R.Henry,<br>R. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient<br>data-parallel pipelines. In PLDI ’10. ACM, 2010.</p>\n<p>[9] J.Cheney,L.Chiticariu,andW.-C.Tan.Provenancein<br>databases: Why, how, and where. Foundations and Trends in<br>Databases, 1(4):379–474, 2009.</p>\n<p>[10] J.DeanandS.Ghemawat.MapReduce:Simplifieddata<br>processing on large clusters. In OSDI, 2004.</p>\n<p>[11] J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu, and G. Fox. Twister: a runtime for iterative mapreduce. In HPDC ’10, 2010.</p>\n<p>[12] P.K.Gunda,L.Ravindranath,C.A.Thekkath,Y.Yu,and L. Zhuang. Nectar: automatic management of data and computation in datacenters. In OSDI ’10, 2010.</p>\n<p>[13] Z.Guo,X.Wang,J.Tang,X.Liu,Z.Xu,M.Wu,M.F. Kaashoek, and Z. Zhang. R2: an application-level kernel for record and replay. OSDI’08, 2008.</p>\n<p>[14] T.Hastie,R.Tibshirani,andJ.Friedman.TheElementsof Statistical Learning: Data Mining, Inference, and Prediction. Springer Publishing Company, New York, NY, 2009.</p>\n<p>[15] B.He,M.Yang,Z.Guo,R.Chen,B.Su,W.Lin,andL.Zhou. Comet: batched stream processing for data intensive distributed computing. In SoCC ’10.</p>\n<p>[16] A.Heydon,R.Levin,andY.Yu.Cachingfunctioncallsusing precise dependencies. In ACM SIGPLAN Notices, pages 311–320, 2000.</p>\n<p>[17] B.Hindman,A.Konwinski,M.Zaharia,A.Ghodsi,A.D. Joseph, R. H. Katz, S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI ’11.</p>\n<p>[18] T.Hunter,T.Moldovan,M.Zaharia,S.Merzgui,J.Ma,M.J. Franklin, P. Abbeel, and A. M. Bayen. Scaling the Mobile Millennium system in the cloud. In SOCC ’11, 2011.</p>\n<p>[19] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys ’07, 2007.</p>\n<p>[20] S.Y.Ko,I.Hoque,B.Cho,andI.Gupta.Onavailabilityof intermediate data in cloud computations. In HotOS ’09, 2009.</p>\n<p>[21] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum. Stateful bulk processing for incremental analytics. SoCC ’10. [22] G.Malewicz,M.H.Austern,A.J.Bik,J.C.Dehnert,I.Horn,<br>N. Leiser, and G. Czajkowski. Pregel: a system for large-scale<br>graph processing. In SIGMOD, 2010.</p>\n<p>[23] D.G.Murray,M.Schwarzkopf,C.Smowton,S.Smith,<br>A. Madhavapeddy, and S. Hand. Ciel: a universal execution<br>engine for distributed data-flow computing. In NSDI, 2011. </p>\n<p>[24] B.NitzbergandV.Lo.Distributedsharedmemory:asurveyof<br>issues and algorithms. Computer, 24(8):52 –60, Aug 1991. </p>\n<p>[25] J.Ousterhout,P.Agrawal,D.Erickson,C.Kozyrakis,<br>J. Leverich, D. Mazie`res, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman. The case for RAMClouds: scalable high-performance storage entirely in DRAM. SIGOPS Op. Sys. Rev., 43:92–105, Jan 2010.</p>\n<p>[26] D.PengandF.Dabek.Large-scaleincrementalprocessingusing distributed transactions and notifications. In OSDI 2010.</p>\n<p>[27] R.PowerandJ.Li.Piccolo:Buildingfast,distributedprograms with partitioned tables. In Proc. OSDI 2010, 2010.</p>\n<p>[28] R.RamakrishnanandJ.Gehrke.DatabaseManagement Systems. McGraw-Hill, Inc., 3 edition, 2003.</p>\n<p>[29] K.Thomas,C.Grier,J.Ma,V.Paxson,andD.Song.Designand evaluation of a real-time URL spam filtering service. In IEEE Symposium on Security and Privacy, 2011.</p>\n<p>[30] J.W.Young.Afirstorderapproximationtotheoptimum checkpoint interval. Commun. ACM, 17:530–531, Sept 1974.</p>\n<p>[31] Y.Yu,M.Isard,D.Fetterly,M.Budiu,U ́.Erlingsson,P.K. Gunda, and J. Currey. DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language. In OSDI ’08, 2008.</p>\n<p>[32] M.Zaharia,D.Borthakur,J.SenSarma,K.Elmeleegy,<br>S. Shenker, and I. Stoica. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys ’10, 2010.</p>\n<p>[33] M.Zaharia,M.Chowdhury,T.Das,A.Dave,J.Ma,<br>M. McCauley, M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. Technical Report UCB/EECS-2011-82, EECS Department, UC Berkeley, 2011.</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><p>大概用了16个小时完成了这篇关于RDD论文的翻译，这篇论文奠定了Spark的设计基础。</p>\n<p>原文:<a href=\"https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\" target=\"_blank\" rel=\"noopener\">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a></p>\n<p>推荐先看林子雨老师关于RDD的<a href=\"http://dblab.xmu.edu.cn/blog/985-2/\" target=\"_blank\" rel=\"noopener\">解释</a></p>\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>本文提出了一个分布式内存抽象的概念–弹性分布式数据集（Resilient Distributed Datasets，以下称为RDDs），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDDs的提出是由于现有的两种计算框架并不高效：迭代式算法和交互式数据挖掘工具。在内存中操作数据可以将前两种计算方式的效率提高一个数量级。RDDs提供了一种受限的共享内存，是基于粗粒度的转换操作而不是细粒度的状态同步。尽管如此，RDDs依然能够表示多种类型的计算，包括专用的迭代编程模型（如Pregel）和一些新的应用模型。我们通过在Spark上评估各种应用和基准，实现了RDDs。</p>\n<h2 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h2><p>集群计算框架，如MapReduce[10]和Dryad[19]，已经被广泛地运用于大规模数据分析。这些系统能够让用户在不用考虑任务调度和容错的前提下，使用一系列高级的操作进行并行计算。</p>\n<p>虽然这些框架为获取集权计算资源提供了大量的抽象，但是缺少对分布式内存的运用。这使他们对于一类新兴的应用十分低效：它们在不同的计算阶段<em>重用</em>中间结果。数据重用在<em>迭代式</em>机器学习和图算法中十分常见，包括PageRank、K-means聚类和逻辑回归。另一个明显的用例是<em>交互式</em>数据挖掘，用户在同样的数据子集上进行ad-hoc查询。然而不好的是，对于现在的框架，在不同计算阶段之间重用数据（如，在两个MapReduce的job之间）的唯一方式是将其写入外部稳定存储系统中，如，分布式文件系统。由于数据的复制、硬盘I/O和序列化，导致了大量的成本开销，并占据了应用运行的大部分时间。</p>\n<p>在意识到这个问题之后，研究人员针对需要数据重用的应用开发了专门的框架。比如，迭代式图计算系统Pregel[22]，其将中间数据存放在内存中，而HaLoop[7]提供了一种迭代式MapReduce接口。无论如何，这些框架仅支持特定的计算模式（如，循环一系列的MapReduce步骤），并隐式地提供这些模式的数据共享。它们没有提供更加通用的数据重用的抽象，如，让用户直接向内存中装载数据集，并对其进行ad-hoc查询。</p>\n<p>在这篇论文中，我们提出了一种能够广泛运用于各种应用中高效的数据重用抽象，<em>弹性分布式数据集</em>（RDDs）。RDDs是一个容错的、并行的数据结构，能够让用户明确地在内存中持久化中间结果，控制其分区以优化数据的放置和使用丰富的操作符对其进行处理。</p>\n<p>设计RDDs的主要挑战是，定义一个能够高效容错能力的编程接口。现有的基于集群的内存存储抽象，如分布式共享内存、键值对存储、数据库和Piccolo，提供了基于细粒度更新可变状态的接口（如，表中的cell）。运用这些接口时，达到数据容错的唯一方式是在不同的机器之间进行数据的冗余或者记录更新日志。这两种方法对于数据密集型工作来说代价高昂，因为他们需要在集群网络间复制大量数据，而网络带宽远小于RAM带宽，同时还会产生大量数据存储开销。</p>\n<p>相比于这些系统，RDDs提供基于粗粒度转换的，可用于大量数据项进行相同操作的接口（如，map,filter和join）。这使得RDDs能够通过记录产生数据集的一系列转换操作（称之为lineage），而不是记录真实的数据，来提高容错的效率【1当lineage链过长时，对一些RDDs进行检查点（checkpoint）设置可能更加有用，我们在5.4节对其进行讨论】。如果一个分区的RDD丢失了，它有足够的信息知道自己是如何从其他的RDD产生的，从而重新计算该分区。因此，丢失的数据可以很快地恢复，而不需要代价昂贵的复制。</p>\n<p>基于粗粒度转换的接口乍一看是有局限性的，但RDDs对于许多并行应用都非常适用，因为这些应用本质上就是会对多种数据项进行相同的操作。为此，我们证明了RDDs高效地运用于表达各种已经被现有的分布式系统所实现的集群编程模型，包括MapReduce，DryadLINQ,，SQL，Pregel 和 HaLoop，以及这些系统无法实现的新应用，如交互式数据挖掘。RDDs能够作为被用来解决前面提到的计算需求而引入的新框架的证据，是因为其强大的抽象能力。</p>\n<p>我们已经在一个被用于UC Berkeley的实验环境和许多公司生产环境下的应用–Spark上，实现了RDDs。Spark提供了一个运用Scala语言，类似DryadLINQ的易用的语言集成编程接口。另外，Spark还可以在Scala解释器中进行交互式大数据集的查询。我们相信Spark会是第一个运用通用编程语言完成交互式速度下集群内存数据挖掘的系统。</p>\n<p>我们通过微基准测试和用户应用对RDDs和Spark进行评估。我们得出，Spark在迭代式应用上比Hadoop快20倍，在真实数据报表分析上快40倍，并且能够在5-7秒的延迟内完成1TB数据集的交互式扫描。更加根本地，为了说明RDDs的通用性，我们在Spark上实现了Pregel和HaLoop的编程模型，包括用一些相对较小的库（每个库大概200行代码）实现它们所采用存储优化策略。</p>\n<p>这篇论文首先介绍RDDs的概览（第2部分）和Spark（第3部分），然后讨论RDDs的内部表示（第4部分），实现（第5部分），和一些实验结果（第6部分）。最后，我们讨论了用RDDs实现几个现有的集群编程模型（第7部分），相关研究工作（第8部分）和总结。</p>\n<h2 id=\"2-弹性分布式数据集（RDDs）\"><a href=\"#2-弹性分布式数据集（RDDs）\" class=\"headerlink\" title=\"2 弹性分布式数据集（RDDs）\"></a>2 弹性分布式数据集（RDDs）</h2><p>这一部分提供关于RDDs的概述。先定义RDDs（2.1节），介绍其在Spark中的编程接口（2.2节）。然后将RDDs与细粒度共享内存抽象进行比较（2.3节）。最后讨论RDD模型的限制（2.4节）。</p>\n<h3 id=\"2-1-RDD抽象\"><a href=\"#2-1-RDD抽象\" class=\"headerlink\" title=\"2.1 RDD抽象\"></a>2.1 RDD抽象</h3><p>一个RDD是只读的，是将记录进行分区的集合。RDDs只能由（1）稳定物理存储中的数据集（2）其他RDDs通过明确的操作产生。我们称这些操作为转换（transformations）以区别其他对RDDs的操作。转换的例子包括map，filter和join。【2 虽然单个RDDs是不可变的，但是可以通过多个RDDs来表示不同版本的数据集以实现多状态。我们让RDDs不可变以使lineage图表示更简便，但这也相当于将我们的抽象变成版本化数据并在lineage图中追踪不同版本】</p>\n<p>RDDs不需要都实体化。一个RDD有足够的信息了解自己是如何从其他数据集产生的（lineage）并通过信息从稳定的物理存储中计算出自己的分区。这一强大特性的本质是，程序能够在RDD重建之后对其进行引用。</p>\n<p>最后一点，用户可以对RDDs进行2方面的控制：持久化和分区。用户可以表明将要重用的RDDs并为其选择存储策略（如，内存存储）。也可以将RDDs的元素通过特定键值进行分区。这些功能对于存储优化特别有用，比如保证两个将要进行join操作的数据集都进行了相同的哈希分区。</p>\n<h3 id=\"2-2-Spark编程接口\"><a href=\"#2-2-Spark编程接口\" class=\"headerlink\" title=\"2.2 Spark编程接口\"></a>2.2 Spark编程接口</h3><p>Spark暴露了类似DryadLINQ和FlumeJava的RDDs语言集成API，每个数据集都被表示成一个对象，并通过执行方法在这些对象上进行转换操作（transformations）。</p>\n<p>开发人员通过将物理存储上的数据集进行转换（如，map和filter）来定义一个或多个RDDs。然后可以用动作（actions）对这些RDDs进行操作，这些操作向应用返回值或者向存储系统产生外部数据。动作（actions）的例子包括，count（返回数据集中元素的数目），collect（返回元素本身）和save（将数据集输出到外部存储系统）。像DryadLINQ一样，Spark在遇到一个动作操作时才会真正计算出RDDs，所以其可以对转换（transformations）进行流水线操作。</p>\n<p>此外，开发人员还可以调用persist方法来声明他们想重用的RDDs。Spark默认会将RDDs留存在内存中，但是会在没有足够RAM的情况下将它们溢出到硬盘。用户可以采用其他的持久化策略，如通过persist标价，将特定RDD只存在硬盘上或在机器之间进行备份。最后，用户可以为每个RDD设置优先级来表明当需要时，将哪一个内存中的数据溢出到硬盘中。</p>\n<h4 id=\"2-2-1-例子：控制台日志挖掘\"><a href=\"#2-2-1-例子：控制台日志挖掘\" class=\"headerlink\" title=\"2.2.1 例子：控制台日志挖掘\"></a>2.2.1 例子：控制台日志挖掘</h4><p>假设一个网页服务出现错误，管理员想在HDFS中兆字节规模的日志中找到原因。应用Spark，管理员能够将错误信息从多个结点的日志中导入RAM，并交互式的进行查询。她会键入以下代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lines = spark.textFile(<span class=\"string\">\"hdfs://...\"</span>)</span><br><span class=\"line\">errors = lines.filter(_.startsWith(<span class=\"string\">\"ERROR\"</span>))</span><br><span class=\"line\">errors.persist()</span><br></pre></td></tr></table></figure>\n<p>第一行从一个HDFS文件（文本行集合）定义了一个RDD，并在第二行产生过滤后的RDD。</p>\n<p>第三行将errors持久化在内存中，这样就能被查询。注意filter的参数是一个闭包的Scala语法。</p>\n<p>到目前为止，并没有在集群上运行作业。但是，用户可以对RDD进行动作（actions）操作，如计算消息的数目：</p>\n<p><code>errors.count()</code></p>\n<p>这位用户也可以对该RDD进行进一步的转换操作并运用他们的结果，如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Count errors mentioning MySQL:</span></span><br><span class=\"line\">errors.filter(_.contains(<span class=\"string\">\"MySQL\"</span>)).count()</span><br><span class=\"line\"><span class=\"comment\">// Return the time fields of errors mentioning</span></span><br><span class=\"line\"><span class=\"comment\">// HDFS as an array (assuming time is field</span></span><br><span class=\"line\"><span class=\"comment\">// number 3 in a tab-separated format):</span></span><br><span class=\"line\">errors.filter(_.contains(<span class=\"string\">\"HDFS\"</span>))</span><br><span class=\"line\">      .map(_.split(’\\t’)(<span class=\"number\">3</span>))</span><br><span class=\"line\">      .collect()</span><br></pre></td></tr></table></figure>\n<p>在第一个对errors进行动作操作之后，Spark将在内存中村塾errors的各分区，这极大地加快了下游的计算操作。注意，最原始的RDD，lines，不会加载到RAM中。这是可取的，因为错误消息可能只是数据的一小部分（小到足够放进内存）。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060102174/0\" alt></p>\n<p>最后为了说明模型是如何实现容错的，图1展示了该RDDs的lineage图。在这次查询中，我们首先对lines进行过滤得到errors，然后在运行collect操作前进行更进一步的filter和map操作。Spark调度器将会流水执行后两个转换操作并向缓存了errors分区的结点发送任务的集合来对其进行计算。除此之外，如果errors的一个分区丢失了，Spark只会在相关的lines分区上进行过滤操作来重建丢失的分区。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060116532/0\" alt></p>\n<h3 id=\"2-3-RDD模型的优势\"><a href=\"#2-3-RDD模型的优势\" class=\"headerlink\" title=\"2.3 RDD模型的优势\"></a>2.3 RDD模型的优势</h3><p>为了理解RDDs作为分布式内存抽象的优势，表1将其与分布式共享内存（DSM）进行了比较。在DSM系统中，应用对全局地址空间进行随意位置的读写。请注意，根据此定义，我们不仅包含了传统的共享内存系统[24]，还包括应用可以进行细粒度写共享状态的其他系统，如Piccolo[27]，其提供共享DHT和分布式数据库。DSM是非常通用的抽象，但是这样的通用性使其很难以一种高效且容错的方式运用在商业集群上。</p>\n<p>RDDs与DSM的主要不同是，RDDs只能通过粗粒度的转换操作产生（“写”），而DSM允许在内存中任意位置读写【注意对RDDs的读仍然可以是细粒度的。例如，应用系统可以将一个RDD视为一个大型只读查找表】。这会限制RDDs为执行批量写入的应用程序，但是这使得其具有高效的容错性。特别是，RDD不需要产生检查点（checkpoint）的开销，因为它们可以使用lineage来恢复【在一些应用中，在具有很长lineage链的RDDs中仍然可以使用checkpoint技术，我们将在5.4节讨论。但是，这些可以在后台完成，因为RDDs是不可变的，并且不需要像在DSM中那样保留整个应用程序的快照】。而且，只有丢失的RDD分区才需要在失败时重新计算，并且它们可以在不同的节点上并行计算，而不必回滚整个程序。</p>\n<p>RDDs的第二个优势是，由于它们不可变的特性，通过运行缓慢任务的副本来缓解慢结点对系统拖拽的压力，就和MapReduce一样[10]。使用DSM很难实现备份任务，因为任务的两个副本将访问相同的内存位置并干扰彼此的更新。</p>\n<p>最后，RDDs提供了DSM没有的两个其他好处。其一，在对RDDs的批量操作中，一个运行时可以基于数据的位置进行任务调度以提高性能。其二，当没有足够的内存来存储RDDs时，它就会优雅地降级，使它们仅用于扫描操作。不适合RAM的分区可以存储在磁盘上，并提供与当期数据并行系统类似的性能。</p>\n<h3 id=\"2-4-不适合RDDs的应用\"><a href=\"#2-4-不适合RDDs的应用\" class=\"headerlink\" title=\"2.4 不适合RDDs的应用\"></a>2.4 不适合RDDs的应用</h3><p>正如引言中所讨论的，RDDs非常适用于对数据集中所有元素进行相同操作的批处理应用。在这些情况下，RDDs可以有效地将每个转换（transformations）记录为lineage图中的一个步骤，并且可以在不记录大量数据的情况下恢复丢失的分区。RDDs不太适合对共享状态进行异步细粒度共享状态更新的应用程序，例如Web应用程序的存储系统或增量Web爬网程序。对于这些应用程序，使用执行传统更新日志记录和checkpoint的系统更有效，例如数据库，RAMCloud [25]，Percolator [26]和Piccolo [27]。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。我们的目标是为批量分析提供高效的编程模型，并将这些异步应用程序留给专用系统。</p>\n<h2 id=\"3-Spark编程接口\"><a href=\"#3-Spark编程接口\" class=\"headerlink\" title=\"3 Spark编程接口\"></a>3 Spark编程接口</h2><p>Spark通过Scala [2]提供类似于DryadLINQ [31]语言集成API的RDDs抽象，Scala [2]是基于Java VM的静态类型函数编程语言。我们之所以选择Scala，是因为它结合了简洁（便于交互使用）和效率（静态类型）。但是，关于RDD抽象的任何内容都不需要函数式语言</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060128798/0\" alt></p>\n<p>要使用Spark，开发人员编写一个连接到一组workers的driver程序，如图2所示。驱动程序定义一个或多个RDDs并在其上调用动作操作。驱动程序上的Spark代码也会追踪RDDs的lineage。这些workers是长期存在的过程，能够跨操作在RAM中存储RDDs分区。</p>\n<p>正如我们在2.2.1中的日志挖掘例子中所展示，用户想如map这样的RDD操作传递闭包（函数形式）来提供参数。Scala将每个闭包表示为Java对象，这些对象可以序列化并加载到另一个节点上，以通过网络传递闭包。Scala也将在闭包中绑定的任何变量作为Java对象的域。比如，可以编写代码：var x = 5; rdd.map(_ + x)，将在RDD中的每一个元素都加5【我们在闭包创建时对其进行保存，这样例子中的map操作永远都是加5，即使x发生变化】。</p>\n<p>RDD本身是由元素类型参数化的静态类型对象。例如，RDD [Int]是整数的RDD。但是，由于Scala支持类型推断，因此我们的大多数示例都省略了类型。</p>\n<p>虽然我们在Scala中暴露RDDs的方法在概念上很简单，但我们必须使用反射解决Scala的闭包对象的问题[33]。我们还需要更多的工作来使Spark可以使用Scala解释器，我们将在5.2节中讨论。但是，我们不必修改Scala编译器。</p>\n<h3 id=\"3-1-Spark中的RDD操作\"><a href=\"#3-1-Spark中的RDD操作\" class=\"headerlink\" title=\"3.1 Spark中的RDD操作\"></a>3.1 Spark中的RDD操作</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060140749/0\" alt></p>\n<p>表2列出了Spark中可用的主要RDD转换和动作操作。我们给出每个操作的签名，在方括号中显示类型参数。再次强调转换（transformations）是定义新RDD的延迟操作，而动作（actions）启动计算将向程序返回值或将数据写入外部存储。</p>\n<p>请注意，某些操作（如join）仅适用于键值对的RDDs。此外，我们的函数名称被选择为与Scala和其他函数式语言中的其他API匹配；例如，map是一对一映射，而flatMap将每个输入值映射到一个或多个输出（类似于MapReduce中的映射）。</p>\n<p>除了这些运算符，用户还可以持久化RDD。此外，用户可以获得RDD的分区顺序（由Partitioner类表示），并根据它对另一个数据集进行分区。诸如groupByKey，reduceByKey和sort操作自动地会产生哈希或范围分区的RDD。</p>\n<h3 id=\"3-2-示例应用\"><a href=\"#3-2-示例应用\" class=\"headerlink\" title=\"3.2 示例应用\"></a>3.2 示例应用</h3><p>我们使用两个迭代应用程序补充了2.2.1节中的数据挖掘示例：逻辑回归和PageRank。后者还展示了如何控制RDD的分区以提高性能。</p>\n<h4 id=\"3-2-1-logistic回归\"><a href=\"#3-2-1-logistic回归\" class=\"headerlink\" title=\"3.2.1 logistic回归\"></a>3.2.1 logistic回归</h4><p>许多机器学习算法本质上是迭代的，因为它们运行迭代优化过程，例如梯度下降，以最大化功能。因此，通过将数据保存在内存中，以更快地运行。</p>\n<p>例如，以下程序实现了逻辑回归[14]，这是用于搜索最能分开两类点（例如，垃圾邮件和非垃圾邮件）的超平面w的一个通用算法。该算法使用梯度下降：它以随机值开始w，并且在每次迭代时，它将w的函数与数据相加以沿着改善它的方向移动。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> points = spark.textFile(...)</span><br><span class=\"line\">                  .map(parsePoint).persist()</span><br><span class=\"line\"><span class=\"keyword\">var</span> w = <span class=\"comment\">// random initial vector</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"type\">ITERATIONS</span>) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> gradient = points.map&#123; p =&gt;</span><br><span class=\"line\">    p.x * (<span class=\"number\">1</span>/(<span class=\"number\">1</span>+exp(-p.y*(w dot p.x)))<span class=\"number\">-1</span>)*p.y</span><br><span class=\"line\">  &#125;.reduce((a,b) =&gt; a+b)</span><br><span class=\"line\">  w -= gradient</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们首先定义一个名为points的持久RDD作为文本文件上的map转换的结果，该文本文件将每行文本解析为Point对象。然后，我们通过对当前w的函数求和，对points重复运行map和reduce以计算每一步的梯度。在迭代的过程中将points保存在内存中可以获得20倍的加速，这将在6.1节展示。</p>\n<h4 id=\"3-2-2-PageRank\"><a href=\"#3-2-2-PageRank\" class=\"headerlink\" title=\"3.2.2 PageRank\"></a>3.2.2 PageRank</h4><p>更复杂的数据共享模式发生在PageRank [6]。算法通过累加在文件中对每个文件的应用次数迭代地更新每一文件的rank。在每次迭代时，每个文件都向其邻居发送r/n的贡献值，r是其排名，n是其邻居的数量。然后通过α/N + (1 − α)∑ci式子更新排名，其中求和是它所收到的贡献值，而N是文件的总数。我们可以通过如下代码Spark中实现PageRank：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Load graph as an RDD of (URL, outlinks) pairs</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> links = spark.textFile(...).map(...).persist()</span><br><span class=\"line\"><span class=\"keyword\">var</span> ranks = <span class=\"comment\">// RDD of (URL, rank) pairs</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"type\">ITERATIONS</span>) &#123;</span><br><span class=\"line\"><span class=\"comment\">// Build an RDD of (targetURL, float) pairs</span></span><br><span class=\"line\">  <span class=\"comment\">// with the contributions sent by each page</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> contribs = links.join(ranks).flatMap &#123;</span><br><span class=\"line\">    (url, (links, rank)) =&gt;</span><br><span class=\"line\">      links.map(dest =&gt; (dest, rank/links.size))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Sum contributions by URL and get new ranks</span></span><br><span class=\"line\">  ranks = contribs.reduceByKey((x,y) =&gt; x+y)</span><br><span class=\"line\">             .mapValues(sum =&gt; a/<span class=\"type\">N</span> + (<span class=\"number\">1</span>-a)*sum)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060154806/0\" alt></p>\n<p>该程序产生图3中的RDD lineage图。在每次迭代中，我们基于来自先前的迭代的contribs和ranks以及静态links数据集来创建新的排名数据集【请注意，尽管RDD是不可变的，但程序中的变量rank和contribs指向每次迭代时的不同RDDs】。图3的一个有趣特征是随着迭代数目的增加而不断增长。因此，在具有多次迭代的作业中，可能需要可靠地复制某些版本的ranks以减少故障恢复时间[20]。用户可以使用RELIABLE标志调用persist来执行此操作。但请注意，不需要复制links数据集，因为可以通过在输入文件的块上重新运行map来有效地重建它的分区。此数据集通常比ranks大得多，因为每个文档都有许多链接，但只有一个数字作为其排名，因此使用lineage恢复它比使用checkpoint回复程序整个内存中状态的系统更节省时间。</p>\n<p>最后，我们可以通过控制RDDs的分区来优化PageRank中的通信。如果我们指定links的分区（例如，通过节点间的URL对link lists进行哈希分区），以相同的方式对ranks进行分区，这样就确保links和ranks之间的join操作不需要通信（因为每个URL的排名将与其link list在同一台机器上）。我们还可以编写自定义分区程序类来对相互链接的页面进行分组（例如，按域名对URL进行分区）。在定义links时，可以通过调用partitionBy来表示这两种优化：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">links = spark.textFile(...).map(...)</span><br><span class=\"line\">             .partitionBy(myPartFunc).persist()</span><br></pre></td></tr></table></figure>\n<p>在此初始化之后，links和ranks之间的join操作会自动地将每个URL的贡献值聚集到它link lists所在的机器上，在这台机器上计算新的排名并与它的links进行join操作。这种跨迭代的一致分区是Pregel等专用框架中的主要优化之一。 RDDs让用户直接表达这一目标。</p>\n<h2 id=\"4-表示RDDs\"><a href=\"#4-表示RDDs\" class=\"headerlink\" title=\"4 表示RDDs\"></a>4 表示RDDs</h2><p>提供RDD作为抽象的挑战之一是为它们选择一种能够跟踪各种转换操作的lineage的表示。理想情况下，实现RDD的系统应该提供尽可能丰富的一组转换操作（例如，表2中的转换操作），并让用户以任意方式组合它们。我们为RDD提出了一个简单的基于图的表示，以达到这些目标。我们在Spark中使用这种表示来支持各种转换，而无需为调度器添加针对每个转换的特殊逻辑，这大大简化了系统设计。简而言之，我们建议通过一个公共接口来表示每个RDD，这个接口包含五条信息：一组分区，它们是数据集的原子部分；父RDDs（parent RDDs）的一组依赖关系；基于其父RDDs计算数据集的函数；有关其分区方案和数据放置的元数据。例如，表示HDFS文件的RDD具有文件的每个块的分区，并且知道每个块所在的机器。同时，对于这个RDD进行map操作的结果具有相同的分区，但是在计算其元素时将在父数据上应用map方法。我们在表3中总结了这个接口。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_234219102_1557060167174/0\" alt></p>\n<p>设计此接口时最有趣的问题是如何表示RDDs之间的依赖关系。我们发现将依赖关系分为两类是足够且有用的：窄依赖关系（narrow dependencies），其中父RDD的每个分区最多由子RDD的一个分区使用；宽依赖关系（wide dependencies），其中多个子RDD分区可能依赖一个父DD分区。例如，map导致窄依赖关系，而join导致宽依赖关系（除非父RDD是哈希分区的）。图4显示了其他示例。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_256675758_1557060251325/0\" alt></p>\n<p>这样区分有两个原因。首先，窄依赖关系允许在一个集群节点上进行流水线执行，这可以计算所有父分区。例如，可以逐个元素地应用map，然后应用filter操作。相比之下，宽依赖关系要求所有父分区的数据都已经计算完成，并使用类似MapReduce的操作在节点之间进行shuffle。其次，节点故障后的恢复在窄依赖时更有效，因为只需要重新计算丢失的对应的父分区，并且可以在不同节点上并行地重新计算它们。相反，在具有宽依赖的lineage图中，单个故障节点可能导致RDD的所有祖先丢失某些分区，从而需要完全重新执行计算。</p>\n<p>RDDs的这个通用接口使得可以在少于20行代码中实现Spark中的大多数转换操作。实际上，即使是新的Spark用户也已经实现了新的转换（例如，采样和各种类型的join），而不用知道调度器的细节。我们在下面阐述一些RDD实现。</p>\n<p><strong>HDFS文件：</strong><br>我们样本中的输入RDDs是HDFS中的文件。对于这些RDDs，partitions为文件的每个块返回一个分区（块的偏移量存储在每个Partition对象中），preferredLocations给出块所在的节点，iterator读取块。</p>\n<p><strong>map：</strong><br>在任何RDD上调用map都会返回MappedRDD对象。该操作传递一个函数参数给map，对父RDD上的记录按照iterator的方式执行这个函数，并返回一组符合条件的父RDD分区及其位置。</p>\n<p><strong>union：</strong><br>在两个RDD上执行union操作，返回两个父RDD分区的并集。通过相应父RDD上的窄依赖关系计算每个子RDD分区【7注意union操作不会过滤重复值】。</p>\n<p><strong>join：</strong><br>对两个RDD执行join操作可能产生窄依赖（如果这两个RDD拥有相同的哈希分区或范围分区），可能是宽依赖，也可能两种依赖都有（比如一个父RDD有分区，而另一父RDD没有）。在任何一种情况下，输出RDD都有一个分区程序（从父项继承的分区程序或默认的散列分区程序）。</p>\n<h2 id=\"5-实现\"><a href=\"#5-实现\" class=\"headerlink\" title=\"5 实现\"></a>5 实现</h2><p>我们大约用14000行scala代码实现了Spark。该系统运行在Mesos集群管理器[17]上，允许它与Hadoop，MPI和其他应用程序共享资源。每个Spark程序作为单独的Mesos应用程序运行，具有自己的驱动程序（master）和工作程序（workers），这些应用程序之间的资源共享由Mesos管理的。Spark可以使用Hadoop现有的输入插件API从任何Hadoop输入源（例如，HDFS或HBase）读取数据，并在未经修改的Scala版本上运行。</p>\n<p>我们现在简要介绍系统中几个技术上有趣的部分：我们的作业调度程序（第5.1节），允许交互式使用的Spark解释器（第5.2节），内存管理（第5.3节）和支持检查点（第5.4节）。</p>\n<h3 id=\"5-1-作业调度\"><a href=\"#5-1-作业调度\" class=\"headerlink\" title=\"5.1 作业调度\"></a>5.1 作业调度</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060276767/0\" alt></p>\n<p>Spark的调度程序使用我们在第4节所述的RDD表示。</p>\n<p>总的来说，我们的调度程序类似于Dryad的[19]，但是另外考虑了RDDs在内存中持久化的分区。每当用户在RDD上运行动作（例如，count或save）时，该调度程序就会检查该RDD的lineage图以构建要执行的stage的DAG，如图5所示。每个阶段包含尽可能多的具有窄依赖的流水线转换。阶段的边界是宽依赖所需的shuffle操作，或任何已经计算过的分区，它们可以跳过父RDD的计算。然后，调度程序启动任务以计算每个阶段中丢失的分区，直到计算出目标RDD为止。</p>\n<p>我们的调度程序基于数据位置使用延迟调度将任务分配给机器[32]。如果任务需要处理节点内存中可用的分区，我们会将其发送到该节点。否则，如果任务处理包含RDD提供优选位置的分区（例如，HDFS文件），我们将其发送给那些分区。</p>\n<p>对于宽依赖关系（即，shuffle依赖关系），我们目前在包含父分区的节点上物化中间记录以简化故障恢复，就像MapReduce物化map输出一样。</p>\n<p>如果任务失败，只要其阶段的父项仍然可用，我们就会在另一个节点上重新运行它。如果某些阶段变得不可用（例如，因为来自shuffle的“map side”的输出丢失），我们重新提交任务以并行计算丢失的分区。我们还不能解决调度程序的失败，尽管复制RDD的lineage图会更简单。</p>\n<p>最后，尽管Spark中的所有计算当前都是为响应驱动程序中调用的操作而运行的，但我们也在尝试让集群上的任务（例如，映射）调用lookup操作，其提供根据键值对哈希分区的RDDs中的元素进行随机获取。在这种情况下，任务需要告诉调度程序在缺少时计算所需的分区。</p>\n<h3 id=\"5-2-解释器整合\"><a href=\"#5-2-解释器整合\" class=\"headerlink\" title=\"5.2 解释器整合\"></a>5.2 解释器整合</h3><p>Scala包含一个类似于Ruby和Python的交互式shell。鉴于内存数据的延迟较低，我们希望让用户从解释器主动运行Spark来查询大数据集。</p>\n<p>Scala解释器通常通过为用户键入的每一行编译一个类，将其加载到JVM中，并在其上调用函数来操作。该类包含一个单例对象，该对象包含该行上的变量或函数，并在初始化方法中运行行代码。例如，如果用户输入代码var x = 5，接着又输入println(x)，则解释器会定义一个包含x的Line1类，并将第2行编译为println(Line1.getInstance().x)。</p>\n<p>在Spark中我们对解释器做了两点改动：</p>\n<p>1.类传输：解释器能够支持基于HTTP传输类字节码，这样worker节点就能获取输入每行代码对应的类的字节码。</p>\n<p>2.改进的代码生成逻辑：通常每行上创建的单例对象通过对应类上的静态方法进行访问。也就是说，如果要序列化一个闭包，它引用了前面代码行中变量，比如上面的例子Line1.x，Java不会根据对象关系传输包含x的Line1实例。所以worker节点不会收到x。我们将这种代码生成逻辑改为直接引用各个行对象的实例。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060301153/0\" alt></p>\n<p>图6显示了在我们更改之后，解释器如何将用户键入的一组行转换为Java对象。</p>\n<p>Spark解释器便于跟踪处理大量对象关系引用，并且便利了HDFS数据集的研究。我们计划以Spark解释器为基础，开发提供高级数据分析语言支持的交互式工具，比如SQL。</p>\n<h3 id=\"5-3-内存管理\"><a href=\"#5-3-内存管理\" class=\"headerlink\" title=\"5.3 内存管理\"></a>5.3 内存管理</h3><p>Spark提供了三种持久化RDD的选项：反序列化Java对象的内存存储，序列化数据的内存存储和硬盘存储。第一个选项提供最快的性能，因为Java VM可以原生访问每个RDD元素。第二个选项允许用户在空间有限时选择比Java对象图更高效的内存表示，但代价是性能较低【成本取决于应用程序每个字节数据的计算量，但轻量级处理的最大值可提升2倍】。第三个选项对于太大而无法保留在RAM中但在每次使用时重新计算成本高昂的RDD非常有用</p>\n<p>为了管理可用的有限内存，我们在RDDs层面上使用LRU替换策略。当计算新的RDD分区但没有足够的空间来存储它时，我们从最近最少的RDD中替换出一个分区，除它这与具有新分区的RDD相同。在这种情况下，我们将旧分区保留在内存中，以防止来自同一RDD的分区循环进出。这很重要，因为大多数操作都会在整个RDD上运行任务，因此很可能将来需要已经在内存中的分区。到目前为止，我们发现此默认策略在所有应用程序中都能正常运行，但我们还通过每个RDD的“持久化优先级”为用户提供进一步控制。</p>\n<p>最后，群集上的每个Spark实例当前都有自己独立的内存空间。在未来的工作中，我们计划通过统一的内存管理器来研究跨Spark实例共享RDDs。</p>\n<h3 id=\"5-4-对检查点的支持\"><a href=\"#5-4-对检查点的支持\" class=\"headerlink\" title=\"5.4 对检查点的支持\"></a>5.4 对检查点的支持</h3><p>尽管在故障之后可以始终使用lineage来恢复RDDs，但对于具有长lineage的RDDs来说，这种恢复可能是耗时的。因此，将一些RDDs checkpoint到稳定存储可能会有所帮助。</p>\n<p>通常，检查点技术对于包含宽依赖关系的长lineage图的RDDs很有用，例如我们的PageRank示例（第3.2.2节）中的rank数据集。在这些情况下，集群中的节点故障可能导致每个父RDD丢失一些数据片段，从而需要完全重新计算[20]。相反，对于对稳定存储中的数据具有窄依赖的RDDs，例如我们的逻辑回归示例（第3.2.1节）中的points和PageRank中的link lists，检查点技术可能没多大用。如果节点发生故障，则可以在其他节点上并行重新计算从这些RDD中丢失的分区，而这只是复制整个RDD的成本的一小部分。</p>\n<p>Spark目前提供了一个用于检查点技术的API（一个在persist中的REPLICATE标志），但是由用户决定哪些数据使用检查点。但是，我们还在研究如何进行自动检查。因为我们的调度程序知道每个数据集的大小以及首次计算它所花费的时间，所以它应该能够选择一组最佳RDDs来检查点以最小化系统恢复时间[30]。</p>\n<p>最后，要强调的一点是，RDDs的只读特性使它们比通用共享存储更容易进行检查。由于一致性问题不需要考虑，因此可以在后台写出RDD，而无需程序暂停或采用分布式快照方案。</p>\n<h2 id=\"6-评估\"><a href=\"#6-评估\" class=\"headerlink\" title=\"6 评估\"></a>6 评估</h2><p>我们通过Amazon EC2上的一系列实验以及用户应用程序的基准评估了Spark和RDDs。总的来说，我们的结果显示如下：</p>\n<ul>\n<li>在迭代机器学习和图形应用程序中，Spark的性能比Hadoop高出20倍。加速来自于通过将数据作为Java对象存储在内存中来避免I / O和反序列化成本。</li>\n<li>我们的用户编写的应用程序可以很好地执行和扩展。特别是，我们使用Spark分析报表比在Hadoop上运行快40倍。</li>\n<li>当节点发生故障时，Spark可以通过仅重建丢失的RDD分区来快速恢复。</li>\n<li>Spark可用于交互查询1 TB数据集，延迟仅为5-7秒。</li>\n</ul>\n<p>我们首先与Hadoop进行迭代机器学习（第6.1节）和PageRank（第6.2节）的基准比较。然后，我们评估Spark中的故障恢复（第6.3节）以及数据集不适合存储时的行为（第6.4节）。最后，我们讨论了用户应用（第6.5节）和交互式数据挖掘（第6.6节）的结果。</p>\n<p>除非另有说明，否则我们的测试使用m1.xlarge EC2节点，其中包含4个内核和15 GB RAM。我们使用HDFS进行存储，具有256 MB块。在每次测试之前，我们清除了OS缓冲区高速缓存，以准确测量IO成本。</p>\n<h3 id=\"6-1-迭代式机器学习应用\"><a href=\"#6-1-迭代式机器学习应用\" class=\"headerlink\" title=\"6.1 迭代式机器学习应用\"></a>6.1 迭代式机器学习应用</h3><p>我们实现了两个迭代机器学习应用程序，逻辑回归和k-means，以比较以下系统的性能：</p>\n<ul>\n<li>Hadoop：The Hadoop 0.20.2 stable release。</li>\n<li>HadoopBinMem：在首轮迭代中执行预处理，通过将输入数据转换成为开销较低的二进制格式来减少后续迭代过程中文本解析的开销，在HDFS中加载到内存。</li>\n<li>Spark：基于RDDs的实现。</li>\n</ul>\n<p>我们使用25-100台机器在100 GB数据集上对这两个算法进行了10次迭代。两个应用程序之间的关键区别是它们每个数据字节执行的计算量。k-means的迭代时间由计算决定，而逻辑回归的计算密集度较低，但对反序列化和I / O花费的时间更敏感。</p>\n<p>由于典型的学习算法需要数十次迭代才能收敛，因此我们分别报告第一次迭代和后续迭代的时间。我们发现通过RDDs共享数据可以大大加快未来的迭代速度。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1280192908_1557060314454/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060328473/0\" alt></p>\n<p><strong>首次迭代</strong><br>所有三个系统在第一次迭代中从HDFS读取文本输入。如图7中的浅色柱状图所示，Spark在实验中比Hadoop要快一些。如图7中的灯条所示，Spark在实验中比Hadoop要快一些。HadoopBinMem是最慢的，因为它通过一个额外的MapReduce作业将数据转换成二进制格式，并必须通过网络在HDFS结点间复制数据。</p>\n<p><strong>后续迭代</strong><br>图7还显示了后续迭代的平均运行时间，而图8显示了其随集群大小变化而产生的时间变化。对于逻辑回归，Spark在100台机器上分别比Hadoop和HadoopBinMem快25.3倍和20.7倍。对于更加计算密集型的k-means应用程序来说，Spark仍然实现了1.9倍到3.2倍的加速。</p>\n<p><strong>理解速度提升</strong><br>我们惊讶地发现Spark甚至比内存存储二进制数据的Hadoop（HadoopBinMem）还要快20倍。在HadoopBinMem中，我们使用了Hadoop的标准二进制格式（SequenceFile）和256 MB大小的超大块，并且我们强制HDFS的数据目录位于内存文件系统中。但是，由于以下几个因素，Hadoop仍然运行缓慢：</p>\n<p>1.Hadoop软件堆栈的最小开销，</p>\n<p>2.提供数据时HDFS的开销，</p>\n<p>3.将二进制记录转换为可用的内存中Java对象的反序列化成本。</p>\n<p>我们依次研究了这些因素。为了估测1，我们运行空的Hadoop作业，仅仅执行作业的初始化、启动任务、清理工作就至少耗时25秒。对于2，我们发现为了服务每一个HDFS数据块，HDFS进行了多次复制以及计算校验和操作。</p>\n<p>最后，为了估测3，我们在一台机器上运行微基准测试，以256 MB输入的各种格式运行逻辑回归计算。特别是，我们比较了处理来自HDFS（HDFS堆栈中的开销将给出）和内存本地文件（内核可以非常有效地将数据传递给程序）的文本和二进制输入的时间。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060341567/0\" alt></p>\n<p>结果如图9所示。内存中HDFS和本地文件之间的差异表明，通过HDFS读取产生了2秒的开销，即使数据存储在本地机器上也是如此。文本和二进制输入之间的差异表明解析开销为7秒。最后，即使从内存文件中读取，将预解析的二进制数据转换为Java对象也需要3秒钟，这仍然几乎与逻辑回归本身一样开销高昂。通过将RDD元素直接存储为内存中的Java对象，Spark可以避免所有这些开销。</p>\n<h3 id=\"6-2-PageRank\"><a href=\"#6-2-PageRank\" class=\"headerlink\" title=\"6.2 PageRank\"></a>6.2 PageRank</h3><p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060351959/0\" alt></p>\n<p>我们使用54 GB Wikipedia导出数据比较了Spark与Hadoop 进行PageRank的性能。PageRank算法通过10轮迭代处理了大约400万文章的链接图数据。图10展示了单独的内存存储使Spark在30个节点上的速度比Hadoop提高了2.4倍。此外，如第3.2.2节所述，控制RDD的分区以使其在迭代中保持一致，将提高到7.4倍。加速也能几乎线性地扩展到60个节点上。</p>\n<p>我们还评估了使用Spark实现Pregel版本的PageRank的性能，结果将在7.1节展示。迭代时间与图10中的相似，但是更长约4秒，因为Pregel在每次迭代时运行一个额外的操作，让顶点“投票”是否完成工作。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060363143/0\" alt></p>\n<h3 id=\"6-3-错误恢复\"><a href=\"#6-3-错误恢复\" class=\"headerlink\" title=\"6.3 错误恢复\"></a>6.3 错误恢复</h3><p>我们评估了k-means应用程序中节点故障后使用lineage重建RDD分区的开销。图11比较了正常操作场景中75节点集群上k-means的10次运行的运行时间，其中一个节点在第6次迭代开始时失败。另一个没有任何故障，每次迭代包含400个任务，处理100 GB的数据。</p>\n<p>直到第5次迭代结束，迭代时间约为58秒。在第6次迭代中，其中一台机器被杀死，导致在该机器上运行的任务和存储在那里的RDD分区丢失。Spark在其他机器上并行重新执行这些任务，他们通过lineage重新读取相应的输入数据和重建的RDD，导致操作时间增加到80秒。一旦重建丢失的RDD分区，迭代时间就会回落到58秒。</p>\n<p>请注意，使用基于检查点的故障恢复机制，恢复可能需要重新运行至少几次迭代，具体取决于检查点的频率。此外，系统需要通过网络复制应用程序的100 GB工作集（文本输入数据转换为二进制），并且要么消耗两倍于Spark的内存以将其复制到RAM中，要么必须等待写入100 GB到磁盘。相比之下，我们示例中RDD的lineage图的大小都小于10 KB。</p>\n<h3 id=\"6-4-内存不足时表现\"><a href=\"#6-4-内存不足时表现\" class=\"headerlink\" title=\"6.4 内存不足时表现\"></a>6.4 内存不足时表现</h3><p>到现在为止，我们能保证集群中的每个节点都有足够的内存去缓存迭代过程中使用的RDDs。一个自然的问题是，如果没有足够的内存来存储作业的数据，Spark是如何运行的。在本实验中，我们将Spark配置为不使用超过一定百分比的内存来在每台机器上存储RDD。图12中，我们为的逻辑回归提供了各种存储空间的配置。我们发现性能在控件降低时缓慢降低。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1379495610_1557060374238/0\" alt></p>\n<h3 id=\"6-5-用Spark构建的用户应用程序\"><a href=\"#6-5-用Spark构建的用户应用程序\" class=\"headerlink\" title=\"6.5 用Spark构建的用户应用程序\"></a>6.5 用Spark构建的用户应用程序</h3><p><strong>内存分析</strong><br>Conviva Inc是一家视频发行公司，它使用Spark加速了之前在Hadoop上运行的大量数据分析报告。例如，一份报告作为一系列Hive [1]查询运行，这些查询计算出了客户的各种统计信息。这些查询都在相同的数据子集上工作（与客户提供的过滤器匹配的记录），但在不同的分组字段上执行了聚合（平均值，百分位数和COUNT DISTINCT），需要使用单独的MapReduce作业。通过在Spark中实现查询并将一次共享的数据子集加载到RDD中，该公司能够将报告速度提高40倍。在Hadoop集群上花费20小时的200 GB压缩数据的报告现在仅使用两台Spark计算机就能在30分钟内运行完成。此外，Spark程序只需要96 GB的RAM，因为它只存储与RDD中客户的过滤器匹配的行和列，而不是整个解压缩文件。</p>\n<p><strong>城市交通模型</strong><br>在Berkeley的Mobile Millennium项目[18]中，基于一系列分散的汽车GPS监测数据，研究人员使用并行化机器学习算法来推算公路交通拥堵状况。数据来自市区10000个互联的公路线路网，还有600000个由汽车GPS装置采集到的样本数据，这些数据记录了汽车在两个地点之间行驶的时间（每一条路线的行驶时间可能跨多个公路线路网）。使用一个交通模型，通过推算跨多个公路网行驶耗时预期，系统能够估算拥堵状况。研究人员使用Spark实现了一个可迭代的EM算法，其中包括向Worker节点广播路线网络信息，在E和M阶段之间执行reduceByKey操作，应用从20个节点扩展到80个节点（每个节点4核），如图13（a）所示：</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1557060384098/0\" alt></p>\n<p><strong>推特垃圾邮件分类</strong><br>Berkeley的Monarch项目[29]使用Spark识别Twitter消息上的垃圾链接。他们在Spark上实现了一个类似6.1小节中示例的Logistic回归分类器，不同的是使用分布式的reduceByKey操作并行对梯度向量求和。图13（b）显示了基于50G数据子集训练训练分类器的结果，整个数据集是250000的URL、至少10^7个与网络相关的特征/维度，内容、词性与访问一个URL的页面相关。随着节点的增加，这并不像交通应用程序那样近似线性，主要是因为每轮迭代的固定通信代价较高。</p>\n<h3 id=\"6-6-交互式数据挖掘\"><a href=\"#6-6-交互式数据挖掘\" class=\"headerlink\" title=\"6.6 交互式数据挖掘\"></a>6.6 交互式数据挖掘</h3><p>为了演示Spark交互式查询大数据集的能力，我们用它来分析1TB的维基百科页面浏览日志（2年的数据）。在本次实验中，我们使用了100 m2.4xlarge EC2实例，每个实例有8个内核和68 GB内存。在整个输入数据集上简单地查询如下内容以获取页面浏览总数：（1）全部页面；（2）页面的标题能精确匹配给定的关键词；（3）页面的标题能部分匹配给定的关键词。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1557060393080/0\" alt></p>\n<p>图14显示了分别在整个、1/2、1/10的数据上查询的响应时间，甚至1TB数据在Spark上查询仅耗时5-7秒，这比直接操作磁盘数据快几个数量级。例如，从磁盘上查询1TB数据耗时170秒，这表明了RDD缓存使得Spark成为一个交互式数据挖掘的强大工具。</p>\n<h2 id=\"7-讨论\"><a href=\"#7-讨论\" class=\"headerlink\" title=\"7 讨论\"></a>7 讨论</h2><p>虽然RDD由于其不可变性和粗粒度转换似乎提供有限的编程接口，但我们发现它们适用于广泛的应用。特别地，RDDs可以表达数量惊人的集群编程模型，这些模型迄今已被提议作为单独的框架，允许用户在一个程序中组合这些模型（例如，运行MapReduce操作来构建图形，然后在其上运行Pregel）并在它们之间共享数据。在本节中，我们将讨论RDDs可以表达哪些编程模型以及它们如此广泛适用的原因（第7.1节）。此外，我们讨论了我们正在探索的RDDs中lineage信息的另一个好处，能够促进这些模型的调试（第7.2节）。</p>\n<h3 id=\"7-1-表达现有的编程模型\"><a href=\"#7-1-表达现有的编程模型\" class=\"headerlink\" title=\"7.1 表达现有的编程模型\"></a>7.1 表达现有的编程模型</h3><p>RDDs可以有效地表达迄今为止独立提出的许多集群编程模型。“有效”，意味着RDDs不仅可以用于生成与这些模型中编写的程序相同的输出，而且RDDs还可以实现这些框架具有的优化，例如将特定数据保存在内存中，将其分区为最大限度地减少通信，并有效地从故障中恢复。使用RDD表达的模型包括：</p>\n<p><strong>MapReduce：</strong> 可以使用Spark中的flatMap和groupByKey操作表示此模型，如果存在组合器，则可以使用reduceByKey表示.</p>\n<p><strong>DryadLINQ：</strong><br>与更普遍的Dryad运行时相比，DryadLINQ系统提供了比MapReduce更广泛的运算符，但这些都是直接对应于Spark（map，groupByKey，join等）中可用的RDD转换的批量运算符。</p>\n<p><strong>SQL：</strong><br>与DryadLINQ表达式一样，SQL查询对记录集执行数据并行操作。</p>\n<p><strong>Pregel：</strong><br>Google的Pregel [22]是迭代图形应用程序的专用模型，起初与其他系统中的面向集合的编程模型完全不同。在Pregel中，程序作为一系列协调的“supersteps”运行。在每个supersteps中，图中的每个顶点都运行一个用户函数，可以更新与顶点相关的状态，更改图形拓扑，并将消息发送到其他顶点用于下一个superstep。该模型可以表达许多图算法，包括最短路径，二分匹配和PageRank。</p>\n<p>让我们用RDDs实现这个模型的关键点是Pregel将相同的用户函数应用于每次迭代的所有顶点。因此，我们可以将每次迭代的顶点状态存储在RDD中，并执行批量转换（flatMap）以应用此函数并生成消息的RDD。之后可以将该RDD与顶点状态连接操作以表示消息的转换。同样重要的是，RDDs允许我们像Pregel那样将顶点状态保存在内存中，通过控制其分区来最小化通信，并支持故障时的部分恢复。我们在Spark上实现了Pregel，其作为200行库，并向读者推荐[33]以获取更多详细信息。</p>\n<p>迭代式MapReduce： 最近提出的几个系统，包括HaLoop [7]和Twister [11]，提供了迭代的MapReduce模型，用户可以为系统提供一系列MapReduce作业循环执行。系统使数据在迭代中保持一致，Twister也可以将其保存在内存中。两种优化都很容易用RDDs表达，我们能够使用Spark将HaLoop实现为200行库。</p>\n<p>流式批处理： 研究人员最近提出了几种增量处理系统，用于定期用新数据更新结果的应用[21,15,4]。例如，每15分钟更新一次广告点击统计数据的应用程序应该能够将前一个15分钟窗口的中间状态与新日志中的数据相结合。这些系统执行类似于Dryad的批量操作，但将应用程序状态存储在分布式文件系统中。将中间状态置于RDDs中将加速其处理。</p>\n<p><strong><em>解释RDDs的表达性能</em></strong><br>为什么RDD能够表达这些不同的编程模型？原因是对RDD的限制对许多并行应用程序几乎没有影响。特别是，尽管RDDs只能通过批量转换创建，但许多并行程序本质上就是将相同的操作应用于许多记录，所以使其易于用RDDs表达。类似地，RDD的不变性不是一个障碍，因为可以创建多个RDD来表示同一数据集的不同版本。实际上，今天的许多MapReduce应用程序都运行在不允许更新文件的文件系统上，例如HDFS。</p>\n<p>最后一个问题是为什么以前的框架没有提供相同的一般性。我们认为这是因为这些系统探索了MapReduce和Dryad无法很好处理的特定问题，例如迭代，但是没有观察到这些问题的常见原因是缺乏数据共享抽象。</p>\n<h3 id=\"7-2-利用RDDs进行调试\"><a href=\"#7-2-利用RDDs进行调试\" class=\"headerlink\" title=\"7.2 利用RDDs进行调试\"></a>7.2 利用RDDs进行调试</h3><p>虽然我们最初设计的RDD在确定性方面可以重新计算以实现容错，但这个属性也可以简化调试。特别是，通过记录在作业期间创建的RDD的lineage，可以（1）稍后重建这些RDD并让用户以交互方式查询它们，以及（2）在单个过程调试中重新执行作业的任何任务，通过重新计算它所依赖的RDD分区。与通用分布式系统[13]的传统重放调试器不同，不必捕获或推断跨多节点的时间顺序，这种方法几乎不增加记录开销，因为只需要记录RDD lineage图【9 与这些系统不同，基于RDD的调试器不会重放用户函数中的非确定性行为（例如，非确定性映射），但它至少可以通过校验和数据来报告它】。我们目前正在开发基于这些想法的Spark调试器[33]。</p>\n<h2 id=\"8-相关工作\"><a href=\"#8-相关工作\" class=\"headerlink\" title=\"8 相关工作\"></a>8 相关工作</h2><p><strong>集群编程模型：</strong><br>集群编程模型的相关工作分为几类。首先，MapReduce [10]，Dryad [19]和Ciel [23]等数据流模型支持丰富的运算符集，用于处理数据，但通过稳定的存储系统共享数据。RDDs代表比稳定存储更有效的数据共享抽象，因为它们避免了数据复制，I / O和序列化的成本【10 请注意，在像RAMCloud [25]这样的内存数据存储中运行MapReduce / Dryad仍然需要数据复制和序列化，这对于某些应用程序来说可能代价很高，如6.1节所示】。</p>\n<p>其次，数据流系统的几个高级编程接口，包括DryadLINQ [31]和FlumeJava [8]，提供了语言集成的API，用户通过map和join等操作符操作“并行集合”。但是，在这些系统中，并行集合表示磁盘上的文件或用于表示查询计划的临时数据集。虽然系统会在相同的操作符查询间流水式地处理数据（如，一个map操作接着一个map操作），但是它们并不能在各个查询之间有效地共享数据。我们在并行收集模型上基于Spark的API，因为它的方便性，并没有增加语言集成接口的新颖性，但通过提供RDD作为此接口背后的存储抽象，我们允许它支持更广泛的应用程序。</p>\n<p>第三类系统为需要数据共享的特定类别的应用程序提供高级接口。例如，Pregel [22]支持迭代图应用，而Twister [11]和HaLoop [7]是迭代MapReduce运行时。但是，这些框架隐式地为他们支持的计算模式提供数据共享，并且不提供一般抽象来供用户选择。例如，用户不能使用Pregel或Twister将数据集加载到内存中，然后决定在其上运行哪个查询，RDD明确地提供分布式存储抽象，因此可以支持这些专用系统不支持的应用，例如交互式数据挖掘。</p>\n<p>最后，一些系统暴露共享可变状态以允许用户执行内存计算。例如，Piccolo [27]允许用户运行并行功能以读取和更新分布式哈希表中的单元格。分布式共享存储（DSM）系统[24]和键值存储如RAMCloud [25]提供一个相似的模型。RDD在两个方面与这些系统不同。首先，RDD基于运算符（如map，sort和join）提供更高级别的编程接口，而Piccolo和DSM中的接口只是对表格单元格的读取和更新。其次，Piccolo和DSM系统通过检查点和回滚实现恢复，这比许多应用程序中基于lineage策略的RDDs更昂贵。最后，正如第2.3节所讨论的那样，RDD还提供了其他优于DSM的优势，例如straggler缓解。</p>\n<p><strong>缓存系统：</strong><br>Nectar [12]可以通过程序分析识别常见的子表达式，在DryadLINQ作业中重用中间结果[16]。这种能力对于添加到基于RDD的系统非常有吸引力。但是，Nectar不提供内存中缓存（它将数据放在分布式文件系统中），也不允许用户明确控制要持久化的数据集以及如何对它们进行分区。Ciel [23]和FlumeJava [8]同样可以缓存任务结果，但不提供内存缓存或显式控制缓存哪些数据。Ananthanarayanan等。建议在分布式文件系统中添加内存缓存，以利用数据访问的时间和空间局部性[3]。虽然此解决方案可以更快地访问文件系统中已有的数据，但它不像在RDDs中那样有效地在一个应用中共享中间结果，因为它仍然需要应用程序在不同阶段间将这些结果写入文件系统。</p>\n<p><strong>Lineage：</strong><br>记载数据的lineage或起源信息长期以来一直是科学计算和数据库中的研究课题，应用于解释结果，允许数据可以被别的数据重建，同时如果在工作流中出现了bug或者数据及丢失了，能够重新计算得到数据。我们推荐读者阅读[5]和[9]来了解这些工作。RDDs提供并行编程模型，其中获取细粒度的lineage成本低廉，因此可用于故障恢复。</p>\n<p>我们基于lineage的恢复机制也类似于MapReduce和Dryad中计算（作业）中使用的恢复机制，它跟踪任务的DAG之间的依赖关系。但是，在这些系统中，谱系（lineage）信息在作业结束后丢失，需要使用复制的存储系统来跨计算共享数据。相比之下，RDDs应用lineage来有效地跨计算保留内存数据，而无需复制和磁盘I / O的成本。</p>\n<p><strong>关系型数据库：</strong><br>RDDs在概念上类似于数据库中的视图，而持久化RDDs类似于物化视图[28]。但是，与DSM系统一样，数据库通常允许对所有记录进行细粒度的读写访问，需要记录操作和数据以实现容错，并且需要额外的开销来维护一致性。RDD的粗粒度转换模型不需要这些开销。</p>\n<h2 id=\"9-结论\"><a href=\"#9-结论\" class=\"headerlink\" title=\"9 结论\"></a>9 结论</h2><p>我们提供了弹性分布式数据集（RDDs），这是一种高效，通用和容错的用于在集群应用程序中共享数据的抽象。RDDs可以表达各种并行应用程序，包括已经提出用于迭代计算的许多专用编程模型，以及这些模型还未实现的新应用程序。与现有的集群存储抽象（需要数据复制以实现容错）不同，RDDs提供基于粗粒度转换的API，使其能够使用lineage来有效地恢复数据。我们在Spark中实现了RDDs，它在迭代应用程序中的性能比Hadoop高出20倍，并且可以交互式查询数百GB的数据。</p>\n<p>我们在spark-project.org上提供了开源Spark作为可扩展数据分析和系统研究的工具。</p>\n<h2 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h2><p>我们感谢第一批Spark用户，包括Tim Hunter，Lester Mackey，Dilip Joseph和Jibin Zhan，他们在实际应用中尝试我们的系统，提供了许多好的建议，并指出了一些研究中的挑战。我们还要感谢我们的指导者Ed Nightingale以及审核的反馈。这项研究部分由Berkeley AMP Lab<br>支持，由 Google, SAP, Amazon Web Services, Cloudera, <strong><em>Huawei</em></strong>, IBM, Intel, Microsoft, NEC, NetApp 和 VMWare，DARPA，the Natural Sci- ences 和 Engineering Research Council of Canada赞助。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p>[1] Apache Hive. <a href=\"http://hadoop.apache.org/hive\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/hive</a>.</p>\n<p>[2] Scala. <a href=\"http://www.scala-lang.org\" target=\"_blank\" rel=\"noopener\">http://www.scala-lang.org</a>.</p>\n<p>[3] G.Ananthanarayanan,A.Ghodsi,S.Shenker,andI.Stoica.<br>Disk-locality in datacenter computing considered irrelevant. In<br>HotOS ’11, 2011.</p>\n<p>[4] P.Bhatotia,A.Wieder,R.Rodrigues,U.A.Acar,and<br>R. Pasquin. Incoop: MapReduce for incremental computations.<br>In ACM SOCC ’11, 2011.</p>\n<p>[5] R.BoseandJ.Frew.Lineageretrievalforscientificdata<br>processing: a survey. ACM Computing Surveys, 37:1–28, 2005.</p>\n<p>[6] S.BrinandL.Page.Theanatomyofalarge-scalehypertextual<br>web search engine. In WWW, 1998.</p>\n<p>[7] Y.Bu,B.Howe,M.Balazinska,andM.D.Ernst.HaLoop:<br>efficient iterative data processing on large clusters. Proc. VLDB<br>Endow., 3:285–296, September 2010.</p>\n<p>[8] C.Chambers,A.Raniwala,F.Perry,S.Adams,R.R.Henry,<br>R. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient<br>data-parallel pipelines. In PLDI ’10. ACM, 2010.</p>\n<p>[9] J.Cheney,L.Chiticariu,andW.-C.Tan.Provenancein<br>databases: Why, how, and where. Foundations and Trends in<br>Databases, 1(4):379–474, 2009.</p>\n<p>[10] J.DeanandS.Ghemawat.MapReduce:Simplifieddata<br>processing on large clusters. In OSDI, 2004.</p>\n<p>[11] J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu, and G. Fox. Twister: a runtime for iterative mapreduce. In HPDC ’10, 2010.</p>\n<p>[12] P.K.Gunda,L.Ravindranath,C.A.Thekkath,Y.Yu,and L. Zhuang. Nectar: automatic management of data and computation in datacenters. In OSDI ’10, 2010.</p>\n<p>[13] Z.Guo,X.Wang,J.Tang,X.Liu,Z.Xu,M.Wu,M.F. Kaashoek, and Z. Zhang. R2: an application-level kernel for record and replay. OSDI’08, 2008.</p>\n<p>[14] T.Hastie,R.Tibshirani,andJ.Friedman.TheElementsof Statistical Learning: Data Mining, Inference, and Prediction. Springer Publishing Company, New York, NY, 2009.</p>\n<p>[15] B.He,M.Yang,Z.Guo,R.Chen,B.Su,W.Lin,andL.Zhou. Comet: batched stream processing for data intensive distributed computing. In SoCC ’10.</p>\n<p>[16] A.Heydon,R.Levin,andY.Yu.Cachingfunctioncallsusing precise dependencies. In ACM SIGPLAN Notices, pages 311–320, 2000.</p>\n<p>[17] B.Hindman,A.Konwinski,M.Zaharia,A.Ghodsi,A.D. Joseph, R. H. Katz, S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI ’11.</p>\n<p>[18] T.Hunter,T.Moldovan,M.Zaharia,S.Merzgui,J.Ma,M.J. Franklin, P. Abbeel, and A. M. Bayen. Scaling the Mobile Millennium system in the cloud. In SOCC ’11, 2011.</p>\n<p>[19] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In EuroSys ’07, 2007.</p>\n<p>[20] S.Y.Ko,I.Hoque,B.Cho,andI.Gupta.Onavailabilityof intermediate data in cloud computations. In HotOS ’09, 2009.</p>\n<p>[21] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum. Stateful bulk processing for incremental analytics. SoCC ’10. [22] G.Malewicz,M.H.Austern,A.J.Bik,J.C.Dehnert,I.Horn,<br>N. Leiser, and G. Czajkowski. Pregel: a system for large-scale<br>graph processing. In SIGMOD, 2010.</p>\n<p>[23] D.G.Murray,M.Schwarzkopf,C.Smowton,S.Smith,<br>A. Madhavapeddy, and S. Hand. Ciel: a universal execution<br>engine for distributed data-flow computing. In NSDI, 2011. </p>\n<p>[24] B.NitzbergandV.Lo.Distributedsharedmemory:asurveyof<br>issues and algorithms. Computer, 24(8):52 –60, Aug 1991. </p>\n<p>[25] J.Ousterhout,P.Agrawal,D.Erickson,C.Kozyrakis,<br>J. Leverich, D. Mazie`res, S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman. The case for RAMClouds: scalable high-performance storage entirely in DRAM. SIGOPS Op. Sys. Rev., 43:92–105, Jan 2010.</p>\n<p>[26] D.PengandF.Dabek.Large-scaleincrementalprocessingusing distributed transactions and notifications. In OSDI 2010.</p>\n<p>[27] R.PowerandJ.Li.Piccolo:Buildingfast,distributedprograms with partitioned tables. In Proc. OSDI 2010, 2010.</p>\n<p>[28] R.RamakrishnanandJ.Gehrke.DatabaseManagement Systems. McGraw-Hill, Inc., 3 edition, 2003.</p>\n<p>[29] K.Thomas,C.Grier,J.Ma,V.Paxson,andD.Song.Designand evaluation of a real-time URL spam filtering service. In IEEE Symposium on Security and Privacy, 2011.</p>\n<p>[30] J.W.Young.Afirstorderapproximationtotheoptimum checkpoint interval. Commun. ACM, 17:530–531, Sept 1974.</p>\n<p>[31] Y.Yu,M.Isard,D.Fetterly,M.Budiu,U ́.Erlingsson,P.K. Gunda, and J. Currey. DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language. In OSDI ’08, 2008.</p>\n<p>[32] M.Zaharia,D.Borthakur,J.SenSarma,K.Elmeleegy,<br>S. Shenker, and I. Stoica. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys ’10, 2010.</p>\n<p>[33] M.Zaharia,M.Chowdhury,T.Das,A.Dave,J.Ma,<br>M. McCauley, M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. Technical Report UCB/EECS-2011-82, EECS Department, UC Berkeley, 2011.</p>\n"},{"title":"基于Spark Streaming的网站日志实时流处理","date":"2019-05-31T16:00:00.000Z","_content":"\n## 摘要\n随着大数据时代的到来，数据总量不断增加，同时分布式存储与计算技术也在快速发展，如何从海量、分散的数据中提取出有价值的信息，已经成为各个行业的迫切需求，对数据的实时处理和响应变得越来越重要。\n\n本文设计了基于Spark Streaming的网站日志实时流处理系统，针对通用音乐网站两种类型的日志：网页浏览日志和网页行为日志，进行了实时采集、传输、处理、分析、持久化和可视化。本文完成了以下几个功能：1）使用Flume采集日志数据，解决了分布式日志采集和聚合的问题，同时对日志进行了分流；2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源；3）使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力，并将分析结果数据持久化至HBase中；4）使用前后端分离的开发模式完成分析结果的可视化展示。系统实现了将大量分散的日志数据进行实时整合，并完成分析计算，最后从多种维度进行信息展示。\n\n关键词：实时流处理；分布式计算；Spark Streaming；Flume；前后端分离\n\n## ABSTRACT\n\nWith the advent of the era of big data, the total amount of data continues to increase, and distributed storage and computing technologies are also developing rapidly. How to extract valuable information from massive and decentralized data has become an urgent need of various industries. Real-time processing and response of data is becoming increasingly important.\n\nThis paper designs a real-time stream processing of website logs system based on Spark Streaming. It performs real-time collection, transmission, processing, analysis, persistence and visualization for two types of logs: universal web browsing logs and web page behavior logs. This paper has completed the following functions: 1) Using Flume to collect log data, solving the problem of distributed log collection and aggregation, and diverting the log at the same time; 2) Using message queue Kafka to solve a large number of log data transmission problems, which providing real-time log data source to the downstream application to process and analysis data; 3) using Spark Streaming to perform real-time processing and analysis of various types of log data, achieving the capability of second-level processing and phase data processing, and persisting the analysis result data to HBase; 4)Visualization of the results of the analysis using a development model with front and rear separation. The system realizes the real-time integration of a large number of scattered log data, completes the analysis and calculation, and finally displays information from various dimensions.\n\nKEYWORDS: Real-time stream processing; Distributed Computing; Spark Streaming; Flume; Front and rear separation\n\n## 1绪论\n\n### 1.1研究背景与意义\n\n随着DT（Data Technology）时代的到来，数据总量不断。预计到2020年，全球数据总量将超过44ZB，同时根据图灵奖获得者杰姆·格雷提出的“新摩尔定律”，每18个月，全球新增数据总量是自计算机诞生以来所有数据量之和。数据作为一种新的能源，被各行各业广为利用，为企业生产经营提供有价值的信息。同时，“爆炸式”增长的数据也对现有的数据采集、传输、分析和存储技术提出了挑战。\n\n互联网行业可以从多种多样的数据源采集信息，例如，服务器日志、网关日志、数据库日志和业务信息等。这些数据通过企业商业智能平台的挖掘分析，为企业内部以及各样相关者提供有价值的信息，例如，电商行业对于用户画像进行分析，进行精准营销和“千人千面”的召回数据，提高转化率；金融行业对于风险监控的要求较高，通过大数据分析，将提高企业的风控效率和准确率。而浏览器的页面型产品/服务的日志在众多数据源十分突出，一是其自然地提供了页面浏览（展现）的详细情况，以及具有定制化采集页面交互日志的能力，所以企业能够在这些日志数据中提取到大量可用有用的信息；二是部分日志信息具有很高的时效性，这是指采集到的日志数据需要实时分析、处理，并召回实时响应数据，为用户提供更好的体验。数据的离线和实时分析都是互联网行业“闭环”的重要部分。\n\n各大公司以及开源社区都提出了各种可用的计算模型和解决方案，为数据采集、传输、存储和分析提供了多种有用的工具。而目前最为火爆的工具之一就是Spark，它在Spark Core的基础上提供了多种高级实现，其中Spark Streaming使数据的实时处理成为可能。\n\n本文以一个通用音乐网站为背景，基于Spark Streaming对网站浏览日志和交互行为日志进行实时流处理，完成了从数据采集、传输、清洗、处理、持久化以及可视化的全流程。系统实现了网站服务器浏览日志的采集，以及通过前端埋点的方式采集可定制化的数据信息，使用Flume和Kafka向下游分流和传输数据。使用Spark Streaming清洗和处理数据，并将相关业务指标数据持久化到HBase中。可视化使用Springboot和React进行前后端分离的实现。\n\n本系统提供的数据有：\n* 页面浏览数据，包括Page View和网站各类目访问量，访问趋势等多维度的指标可视化数据，帮助相关人员进行网站的优化以及搜索引擎优化。\n* 页面交互信息数据，包括音乐网站情景下多种用户操作行为的记录与分析，并实时展示多维度的分析结果。\n\n### 1.2研究现状\n\n日志分析是在海量离散的日志数据中提取出有价值的，符合人们认知的信息。网站日志分析则是在页面浏览、页面行为等日志中得到多种维度的数据，供网站不同职能人员使用。例如，运营人员根据网站浏览量、各类目的访问情况等制定相应的运营方案；高层团队领导根据网站各业务转化率等指标灵活制定网站产品发展战略；开发人员根据网站服务性能数据优化产品性能，保障产品质量。如今已有许多商业上可行的方案，比如百度统计、神策数据等，但是它们要么关注仅仅关注访问量的统计，要么关注用户页面行为的统计。\n\n日志分析在国内外发展都十分迅速。最开始由于网站用户访问量不大，业务量规模也相对较小，整个网站的各个功能模块的日志都由专员进行分析。一般情况是远程登录各个节点来分析日志。随着业务量的不断增大，服务器以及功能模块的不断增多，日志分散在大量机器实体上，这给日志数据的采集、传输、分析和存储带来了挑战。\n\nGoogle用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。但是随着业务场景的复杂化，如互联网金融中的实时风控、电商中的个性化推荐数据召回等的出现，运用Hadoop MapReduce完成的离线数据分析已经不能满足企业发展的需求。Spark于2009年诞生于UC Berkeley的AMP实验室，并与2013年贡献至Apache社区，成为近年来最火热的大数据开源项目。\n\nSpark是基于内存的并行计算框架，也是基于MapReduce计算模型的分布式计算框架，其解决了Hadoop MapReduce的诸多问题，例如不能重用在迭代式机器学习和图计算中经常需要的中间结果，Hadoop MapReduce需要将这些中间数据进行复制、硬盘I/O和序列化。而Spark实现了弹性分布式数据集（RDD），一种基于内存的集群计算容错性抽象，能够很好地适应各种业务场景。Spark Streaming是在Spark Core之上的高级API。Spark Streaming提供了表示连续数据流的、高度抽象的被称为离散流的Dstream，可以通过多种数据源创建Dstream，为下游实时流处理使用。所以Spark Streaming为当前实时流处理的场景需求提供了可靠的上下文。\n\n此外，使用Flume与Kafka为海量日志的采集和传输提供了解决方案。Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，通过Flume可以将分散在各个服务器上的日志收集起来，以供下游日志数据处理分析的使用。Kafka是一种高吞吐量的分布式发布订阅消息系统，在大型网络中运用其对网站的服务性能进行扩展，能够保证所有服务请求不丢失，同时Kafka作为大型网站的基础设施，也为实时日志流处理提供消息队列的服务。\n\n目前对于实时流处理有许多解决方案，日志采集、传输、分析和存储的技术也有许多种选择。不同的解决方案/工具是由特定场景下提出的，寻找最合适的工具完成系统设计十分关键。\n\n### 1.3论文结构\n\n本文将全文内容划分为五个章节进行展开，其组织结构如下：\n\n第一章：绪论部分。简要介绍了网站日志实时流处理的研究背景和意义，并对现有大数据分析技术进行了简单的介绍，同时描述了本文设计的主要工作。\n\n第二章：系统相关理论和技术介绍。简要介绍了数据处理和日志分析的出发点和要点，针对日志采集、传输、处理、分析的难点进行介绍，并提供了现有可靠的解决方案或工具。\n\n第三章：系统需求分析与架构设计。基于本文设计的系统出了分模块的需求，并整合最佳的解决方案，设计了系统整体架构。\n\n第四章：从日志数据产生与采集、日志数据传输、处理、分析与持久化以及分析结果可视化三个方面进行了详细设计与实现。\n\n第五章：对本文设计的系统进行总结，并提出接下来要进一步完善的需求。\n\n## 2相关理论与技术\n\n### 2.1互联网中的闭环理论\n\n#### 2.1.1闭环理论\n\n闭环理论是源于系统论和控制论的一种管理思想，其核心是通过有效的管理和控制手段，构成一个连续封闭的管理回路，使得系统中的每一个环节都能有效的衔接。最终达到提升管理水平，实现既定目标的目的。闭环系统的典型特征有：可控性强、首尾相连、循环往复。闭环式的组织能够高效地利用资源，灵活应对外界变化，达成组织目标，并在这个过程中不断自我提升。\n\n闭环理论强调“可控性”，在组织的运行流程中，尽可能的收集信息，并与原计划进行匹配，出现偏差则进行调整和修正。从而实现系统的高效演进与自我提升。\n\n#### 2.1.2互联网闭环\n\n闭环理论最初由休哈特于1930年提出，现在已经衍生运用于各行各业。对于计算机互联网这样的强调系统和控制的行业，闭环理论运用十分广泛。在互联网行业中运用闭环理论的系统通常包括：验证、反馈、分析和控制这几个部分。直观的运用实例如下：\n\n* 产品开发、运营优化。如产品新特性发布前的灰度测试、电商及内容类产品的实时推荐系统等。\n* 互联网服务。如O2O线上到线下等各类分销平台、金融风控系统等。\n\n为了达到产品的不断提升，并对外提供更好地服务，互联网行业必须将整个系统的各部分形成完整闭环，而数据挖掘与分析是补完闭环的关键环节。网站日志实时分析作为网站系统闭环的一部分，通过分析来自用户的真实访问情况和行为情况反馈，为运营和决策人员提供有价值的信息，用以不断完善产品质量。此外通过收集数据为产品的其他功能，如推荐投放系统提供一手数据资源，完善相关功能，使得反馈结果实时触达用户。整个系统各个部分都为系统的提升做出贡献，并使之成为一个可提升的完整闭环。\n\n### 2.2网站日志采集\n\n浏览器的页面型产品/服务的日志采集可以分为如下两大类。\n\n#### 2.2.1页面浏览（展现）日志采集\n\n页面浏览日志是指当一个页面被浏览器加载呈现时多采集的日志。此类日志是最基础的互联网日志，也是目前所有互联网产品的两大基础指标：页面浏览量（Page View，PV）和访客数（Unique Visitors，UV）的统计基础。页面浏览日志是目前成熟度最高和完备度最高，同时也是最具挑战性的日志采集任务。\n\n页面浏览日志通常是记录在应用服务器上，如Nginx的access.log。\n\n#### 2.2.2页面交互日志采集\n\n当页面加载和渲染完成之后，用户可以在页面上执行各种操作，随着前端技术的发展用户可以与页面上的元素完成很多互动操作，如启动、点击、拖拽和曝光等，这些行为信息价值巨大，设计者通常会要求采集用户的互动行为日志，以便通过量化获知用户的兴趣点或者优化体验。而行为数据属于低价值密度数据，其数量十分巨大，需要强大的数据处理能力和分析能力才能有效利用。\n\n行为日志的记录有多种方式，网页应用常用前端埋点的方式进行采集。\n\n### 2.3 ETL过程与Flume\n\n#### 2.3.1 ETL\n\nETL（Extract-Transform-Load）是一种数据仓库技术，用来描述将数据从数据源抽取（extract）、转换（transform）和加载（load）的过程。这一过程常用于数据仓库技术，但是在目前大数据应用中也十分重要。具体到日志分析类应用，由于服务器众多，同一类型的日志分散在不同的机器实体上。同时不同类型的日志有不同的结构，为了便于分析通常情况下需要同一结构。此外对于各种行为日志需要又不同的下游数据分析应用来处理，这就涉及到一定程度上的分流。因此在网站日志实时流处理中需要有一个ETL过程来进行分散数据的采集和一定程度的过滤清洗，使得下游应用能够高效运作。\n\n目前，Apache Flume十分适合这类ETL过程的业务场景。\n\n#### 2.3.2 Flume\n\nFlume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，其能够将不同源的日志数据汇聚到统一的数据存储中；Flume具有基于流式数据的简单且灵活的架构，同时具有优秀的容错机制和故障恢复机制；Flume运用简洁又灵活的数据模型，使得其十分适合在线分析应用。\n\nFlume不仅可用于日志数据的聚合。由于数据源是可定制的，Flume还可以传输海量的事件数据，包括但不限于，网络流数据、社交媒体产生的数据、电子邮件数据和各种可能形式的数据。\n\nFlume处理数据的最小单位是一个Flume Event。Flume Agent是一个常驻JVM的进程，用于处理从外部源传输来的event。Agent由source、channel和sink组成，如图2.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366186537/0)\n\n用户可以选择多种数据源作为source，如avro source、thrift source和exec source等。在本文设计中，对于页面浏览日志使用exec source，执行系统读取指令采集log文件中新增的日志信息。\n\n对于页面行为日志，采用avro source监听特定的网络端口采集行为信息数据。\n\n对于采集信息的转换，Flume为source中提供了interceptor（过滤器）这个功能。在本文设计中，将运用过滤器对页面行为信息根据行为类型进行分流，以便下游不同数据处理应用使用。\n\nFlume的各个单位事件数据通过sink传递给下游应用使用。本文设计中使用kafka sink传递给下游kafka生产者使用。此外，为了防止事件数据过多，使用channel起到缓冲的作用，可以通过设置channel缓冲的大小来确定其承载力。\n\n### 2.4 Kafka\n\n消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋、日志处理等问题，实现高性能，高可用，可伸缩和最终一致性架构。\n\nKafka是经常用于日志处理的消息中间件，解决大量日志传输的问题。上游日志采集模块将采集到的数据写入到Kafka队列中，Kafka消息队列负责日志数据的接受、存储和转发，下游日志分析处理应用订阅kafka队列中的日志数据。\n\n本文设计中下游日志分析应用即是Spark Streaming作业。由于日志的数量很多，不同类型日志的处理分析逻辑不同，所以引入kafka消息队列。一方面防止日志数据过多淹没下游应用，另一方面按消息的topic进行分流消费。\n\n### 2.5 Hadoop与Spark\n\nGoogle用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。\n\nHadoop是一个分布式系统基础框架，用户可以在不了解分布式底层细节的情况下，开发分布式程序，充分利用集群的威力进行告诉运算和存储。Hadoop诞生于2005年，在社区和众多技术人员的支持下，Hadoop生态圈逐步扩大，形成了一整套用于分布式数据处理分析存储等功能组件。其中最重要的组件有，分布式文件系统HDFS、分布式计算框架MapReduce、分布式列存储数据库HBase、分布式协作服务ZooKeeper、日志收集工具Flume、分布式资源管理器Yarn等。这些组件提供了各种业务场景下数据处理需求的解决方案。其中MapReduce是一种分布式计算模型，用以进行大数据量的计算。其屏蔽了分布式计算框架的细节，将计算抽象成map和reduce两个过程，具有高可靠性、高扩展性、成本低等特点。因此MapReduce被广泛运用与大规模数据分析。\n\n但是随着更多复杂数据分析处理场景的出现，如迭代式计算和交互式数据挖掘等，MapReduce计算模型的出现了缺陷。首先，MapReduce将数据处理的过程抽象成2个阶段，map和reduce，虽然高度抽象，技术人员只需要编写两个函数即可。但是高度抽象带来的问题是表达能力有限，当出现更复杂的计算要求时，不得不将map和reduce两个过程复杂化或者增加更多的MapReduce任务。同时，MapReduce过程中的计算结果都需要进行磁盘I/O，这样当需要多个MapReduce任务写作时，导致任务之间的衔接涉及I/O开销，不能直接重用中间结果。此外，在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务。\n\nMapReduce缺少对分布式内存的应用，而Spark在借鉴了Hadoop MapReduce有点的同时，很好地解决了MapReduce所面临的问题。首先Spark实现了一个分布式内存抽象的概念--弹性分布式数据集（以下称为RDD），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDD是分布式内存的一个抽象概念，提供了高度受限的共享内存模型，这使得Spark计算任务能够在内存中重用中间计算结果。Spark的计算模式也属于MapReduce，但是不限于map和reduce操作，将数据操作抽象成对RDD的一系列粗粒度转换，如map、filter、reduce、group等，比MapReduce更加灵活。由前两个特性，Spark实现的RDD能够拥有很好的容错性能，Spark计算模型将对RDD的操作记录成lineage，即血缘关系，在真实计算任务时并不立即对数据进行转换操作。这样当出现中间计算结果缺失时，只需要根据lineage重新计算数据即可。此外，Spark是基于RDD的lineage形成的有向无环图DAG来进行任务调度的，要优于MapReduce的迭代执行机制，如图2.2所示，实线框为RDD，不同RDD之间的转换形成DAG，Spark通过这个DAG进行任务调度。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366311701/0)\n\n针对流式数据处理场景，Spark在Spark Core的基础上实现了更高级的API--Spark Streaming。Spark Streaming是提供了表示连续数据流的、高度抽象的离散流（DStream）。DStream本质上是对RDD的一层封装，其中的每一个RDD都包含来自一个时间间隔的数据，如图2.3所示。DStream是一个没有边界的集合，没有大小限制，其代表了一个时空的概念。对DStream的操作，具体到每个时间段，就是空间的操作，也就是对时间间隔的对应批次数据的处理。Spark Streaming对于DStream的操作实质上就是对每个RDD应用批量操作。Spark Streaming将对流数据的处理抽象成对DStream的处理，而这种抽象是的开发人员能够像操作RDD一样操作DStream，所以说Spark Streaming提供了实时流处理的上下文。此外Spark Streaming支持从多个数据源创建DStream，如图2.4所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366347719/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366361643/0)\n\n### 2.6前后端分离\n\n前后端分离是指在网站开发阶段前端与后端各司其职，前端只要根据设计还原出页面的逻辑展示，后端只要处理根网页数据相关的业务逻辑，并向前端透出需要的接口即可。这种模式能够使前后端并行开发，大大提高效率，并将前后端一定程度上解耦。本文设计中网页技术部分采用前后端分离的模式，模拟用户页面行为的产生，并且进行分析数据结果的可视化。前端采用react技术栈，后端采用SpringBoot。\n\nReact是Facebook公司开源的前端框架，是近年最火热的技术，国外Facebook、Instagram和国内知乎、蚂蚁金服等都在使用。React的思想是将前端所有元素组件化，将所有需要的UI控件甚至逻辑控件抽象成组件进行开发。每个组件维护自己的状态逻辑变化。React技术不断发展，社区中也不断贡献出特定场景下优秀的解决方案。\n\nSpringBoot是由Pivotal团队开发的框架，其能够简化新Spring应用的初始搭建以及开发过程。SpringBoot使用很多特定或默认的方式进行配置，简化了开发人员的准备工作时间，可以专注于真正业务的实现。SpringBoot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、产品级别的Spring应用，特别适合实现目前最火热的微服务架构。本文设计选择SpringBoot作为后端服务框架。\n\n### 2.7 本章小结\n\n本章介绍了互联网中的闭环理论以及针对日志数据采集、传输、处理、分析、持久化以及可视化的可靠的开源工具，为本文设计的系统中的各个需求场景提供了最优的解决方案。\n\n## 3系统需求分析与架构设计\n\n### 3.1系统需求分析\n\n#### 3.1.1系统概述\n\n本文设计以一个通用音乐网站的日志为分析对象的流处理系统，完成对日志进行实时采集、传输、清洗、处理、分析、持久化和可视化，最终得到网站页面浏览和页面行为日志数据的相关数据分析指标结果等有价值的信息，展示实时流处理的能力。按照数据流处理流程可将本系统分为以下几个部分：日志数据产生与采集模块、日志数据传输模块、日志数据处理分析模块、分析结果持久化与可视化模块。数据流程如图3.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366469597/0)\n\n音乐网站的日志分为两种类型，页面浏览日志和页面行为日志。对于两种类型的日志需要使用统一的日志收集工具进行收集，以便下游组件的使用；数据量庞大的日志数据需要使用消息队列进行传输，达到流量削峰的目的；从消息队列得到的日志数据进过分布式计算工具进行处理计算分析，持久化至存储设施中，最后将分析结果与网站业务相关的结构化数据进行更具体的连接操作完成分析结果的可视化展示。\n\n#### 3.1.2数据产生与采集需求分析\n\n两类不同的日志，页面浏览日志和页面行为日志，采用不同的方式产生。对于页面浏览日志中的每一条浏览日志需要是可配置的结构化数据，这样便于下游应用的清洗等操作,这些日志数据还需要存储在特定的位置，便于采集；对于页面行为日志，同样需要有一定的数据结构，便于在下游对不同的行为日志数据进行分流，但是页面行为日志可以有一定的定制化内容，因为不同的行为可能要采集的数据是不同的，因此行为日志的结构要保留一定的灵活性。行为日志数据的产生不能对用户体验产生影响，也就是说对于用户来说是透明的。\n\n日志数据的采集需要考虑到网站服务分布式的特点，也就是各类型日志都是分布在不同的服务器实体上，因此需要使用一个能够将分布的日志快速收集为全量日志数据的工具，同时为了与下游数据传输组件进行配合，采集工具需要具有一定的分流功能，即是能够将不同种类的日志信息先进行汇总，再向同一目的分发。\n\n#### 3.1.3数据传输与处理模块需求分析\n\n海量日志数据如果不加控制地发送至下游应用进行处理分析，会造成很多问题，最严重的就是超过应用的处理限度而不能正常服务，从而丢失数据。所以必须使用消息中间件解决大量日志数据传输的问题。消息中间件还需要有高可靠性和容错的能力。\n\n对于日志数据的处理分析模块，需要根据不同的日志类型和日志的种类进行不同的处理分析，也就是说页面浏览日志与页面行为日志分析的逻辑是不同的，对于不同行为的分析也是不同的。分析处理模块需要具有实时流处理的能力，以及一定的容错能力。同时还能分析复杂的数据场景，比如处理一定时间窗口内的所有数据。\n\n#### 3.1.4数据持久化与可视化模块需求分析\n\n分时结果数据的持久化需要考虑海量数据的特点，同时对于数据存储内容还要有一定的灵活性，这是指存储的分析结果内容应该能够根据业务的需求进行调整，能够动态地增加字段而不用修改表结构。\n\n可视化模块作为系统最终对外展示的部分，体现了整个系统的风格。优雅直观的界面和简洁直接的交互，不会让使用者产生疲倦感，同时有意愿继续使用和对产品提出改进意见。分析结果数据应该从各个维度进行数据的展示，维度由大到小，逐渐细化，帮助用户更好的观察数据。可视化模块是将一条条结构化的数据反映成直观形式表示的最后一步。\n\n可视化部分分为：今日数据概览、历史数据查询和网页行为分析。详细需求如下：\n\n* 今日数据概览\n\n展示网站各维度的今日实时数据，包括实时PageView、七日PageView走势、今日实时PageView走势、网站各类目访问量、各类目访问量走势、今日来源网站数据汇总和热搜词云。以供运营人员优化网站和优化搜索引擎结果等。\n\n* 历史数据查询\n\n可以选择所要查询的日期，查看所选日期的各项数据指标，同时增加以天为维度的30日访问量比较，可用于帮助网站各级别管理人员制定网站今后的内容发展方向，辅助决策。\n\n* 网页行为分析\n\n能够模拟音乐网站上的用户行为，如歌曲播放、收藏和评论等。从各个维度展现行为日志分析数据，如今日歌曲播放量排行，近一小时内热点歌曲排行、歌曲风格统计、热门歌曲标签云、热评歌曲和热门收藏歌曲等。各维度数据供网站运营人员优化网站歌曲内容。\n\n### 3.2系统架构设计\n\n#### 3.2.1总体架构\n\n本文设计的网站日志实时流处理系统，采取分布式的架构，遵循低耦合的原则。总体架构分为四部分：日志数据产生、日志采集与传输、日志处理分析和持久化以及分析结果可视化。总体架构如图3.2所示，页面浏览日志与页面行为日志采用不同的方案产生；使用Flume与Kafka来进行日志数据的采集与传输；实时日志数据通过Spark Streaming进行处理与分析并将分析结果持久化至HBase中；最后采用前后端分离的模式，整个MySQL中的业务数据完成分析结果的可视化展示。总体架构如图3.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366982662/0)\n\n#### 3.2.2 日志数据产生模块架构设计\n\n网站页面浏览日志是存储在服务器上access.log文件里面的结构化数据，并且这些浏览日志数据的结构是可配置的。本文设计使用Python脚本完成页面浏览日志的产生，并结合Linux的crontab功能完成周期性的调用。页面行为日志使用前端埋点的方式产生，在用户产生某种行为时调用js代码，向后端发出请求。后端使用SpringBoot搭建服务，通过解析请求和log4j产生行为日志数据。该模块架构如图3.3所示，产生的日志数据会被下游模块采集。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367132763/0)\n\n#### 3.2.3 日志采集与传输模块架构设计\n\n上一个模块已经产生了两类日志数据，本模块使用Flume和Kafka完成日志数据的采集与传输。页面浏览日志分布式的存储在网站服务器的access.log文件中，在每个服务器上需要部署一个Flume Agent完成access.log新增日志数据的采集，并将这些日志数据对接至下游Kafka生产者；页面行为日志是由服务器产生的log4j日志数据，通过配置会将这些数据发送至指定地址上的特定端口，在该地址上的服务器部署了Flume Agent，其会监听特定端口来采集日志数据。该Flume Agent会根据日志数据的内容进行分流，对接不同的Kafka生产者，使得不同的行为日志数据以不同的方法被实时处理。消息队列Kafka将Flume采集来的数据通过使用不同的topic传输至下游Spark Streaming。该模块架构如图3.4所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367178577/0)\n\n#### 3.2.4 日志处理分析和持久化模块架构设计\n\n本模块使用Spark Streaming进行日志数据的处理与分析，针对不同的日志向Spark集群提交相应的Spark Streaming应用，这些应用会根据topic拉取Kafka消息队列中的消息，之后进行处理与分析，分析结果将持久化至HBase中。该模块架构如图3.5所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367225228/0)\n\n#### 3.2.5 分析结果可视化模块架构设计\n\n分析结果可视化模块采用react作为前端框架，并结合蚂蚁金服技术体验部的UI组件库ant design、ant design Pro以及商业场景下数据可视化解决方案Bizcharts完成数据的多维度展示。后端使用SpringBoot连接HBase，将日志分析结果透出至前端。此外由于网站具体业务的复杂性，前一阶段的分析结果是比较抽象的，还需要结合存有具体业务数据的结构化数据库MySQL来实现更细化的分析结果数据展示，本次设计音乐网站中较重要的结构化数据是歌曲信息的数据。该模块架构如图3.6所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367274523/0)\n\n### 3.3 本章小结\n\n本章首先对要设计的系统进行总体概括，并分模块进行了需求分析，然后给出了总体架构设计和详细的分模块架构设计。根据各模块的需求给出了可行且合适的解决方案，为下一章系统的具体实现提供了清晰的方向。\n\n## 4设计与实现\n\n### 4.1日志数据产生与采集模块\n\n#### 4.1.1日志数据产生与采集模块设计\n\n由于时间和成本等现实原因，本文设计不能采用真实线上的日志数据，而是采用模拟的方式产生。模拟方式产生的日志数据与真实线上日志完全相同，首先保证浏览日志的数据格式如下：\n\n```java\n98.29.167.156 2019-05-10 14:31:01 \"GET /music/145.html HTTP/1.1\"  200 http://www.baidu.com/s?wd=新近发布  \n```\n\n包括请求源地址、时间、方法、请求页面、HTTP协议、请求状态以及来源网站。所以需要设计脚本完成access.log的生成，为了防止日志文件过大，需要定时清理。对于页面行为日志，采用前端埋点的方式产生。首先根据产品设计的需求找到需要记录的用户行为，比如点击、滑动、停留、键入内容等，细化到音乐网站上，用户的行为可以是点击播放音乐、评论音乐、收藏音乐和切换音乐等。这些日志需要在产生这些行为时调用相关后端接口，并向后端传送打点的内容，由后端服务器来完成日志的产生。后端服务器通过log4j产生格式化的日志供下游采集。\n\n这两类日志的采集都使用Flume完成，但是应用不同配置的Flume agent来完成采集。首先两种agent都最重要将日志数据传送给kafka消息队列，但是应为不同的topic，此外对于不同的页面行为日志也要是不同的topic。对于浏览日志，Flume agent使用avro source读取access.log中新增的日志信息，并通过kafka sink传至下游。对于行为日志统一使用一个Flume agent进行采集，使用avro source监听特定的端口，由后端服务器配置log4j发送至指定端口。此外还需要Flume对所有因为日志按照行为的种类进行分流，使下游产生不同topic的消息。\n\n#### 4.1.2日志数据产生与采集模块实现\n\n* 页面浏览日志产生\n\naccess.log使用python脚本产生，使用linux的crontab生成定时任务，每个一分钟产生一次access.log。关键代码如下：\n\n```java\nf = open(\"/home/hadoop/data/project/logs/access.log\",\"w+\")  \n    while count >= 1:   \n        query_log = \"{ip}\\t{local_time}\\t\\\"GET /{url} HTTP/1.1\\\"\\t{status_code}\\t{referer}\".format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)  \n        f.write(query_log + \"\\n\")  \n        count = count - 1   \n```\n\n最终access.log产生的数据结构如下：\n\n```java\n187.156.143.55  2019-05-10 15:48:01 \"GET /music/128.html HTTP/1.1\"  404 -  \n87.187.167.124  2019-05-10 15:48:01 \"GET /music/128.html HTTP/1.1\"  500 -  \n156.29.187.168  2019-05-10 15:48:01 \"GET /music/112.html HTTP/1.1\"  200 http://www.baidu.com/s?wd=新近发布  \n```\n\n* 页面行为日志产生\n\n前端埋点的方式由于技术选型的不同有很多方式，本文设计使用的是前后端分离的开发模式，所以在前端要记录行为操作数据的部分编写相应的js代码，调用后端业务接口。后端服务器处理接口调用请求的同时使用log4j产生日志数据。具体实现如下：\n\n前端埋点js代码：\n\n```java\n//播放歌曲记录  \n    songplay(songInfo){  \n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", { \n            params:{  \n                K_topic: 'minions_songplay',//行为种类  \n                songId: songInfo.songID//该行为日志需要记录的信息  \n            }  \n        }).then(function (response) {  \n            if (response.data === 1){  \n                message.success('添加歌曲播放记录成功');  \n                console.log(\"日志记录成功\")  \n            } else console.log(\"日志记录错误\")  \n        }).catch(function (error) {  \n            console.log(error)  \n        })  \n    }  \n```\n\n后端接口日志产生：\n\n```java\n@RestController  \n@EnableAutoConfiguration  \npublic class ActionLogController {  \n  \n    Logger logger = Logger.getLogger(ActionLogController.class.getName());  \n    @GetMapping(\"actionLogger\")  \n    private int actionLogger(HttpServletRequest request) throws UnsupportedEncodingException {  \n        int res = 0;  \n        String k_topic = request.getParameter(\"K_topic\");  \n        String songID = request.getParameter(\"songId\");  \n        try {  \n            logger.info(\"topic:\" + k_topic + \" songID:\" + songID);  \n            res = 1;  \n        }catch (Exception e){  \n            logger.error(\"error:\" + e);  \n            e.printStackTrace();  \n            res = 0;  \n        }  \n        return res;  \n    }  \n}  \n```\n\n由于需要与Flume对接，log4j的配置如下：\n\n```java\nlog4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender  \nlog4j.appender.flume.Hostname = hadoop000  \nlog4j.appender.flume.Port = 41415  \nlog4j.appender.flume.UnsafeMode = true  \nlog4j.appender.flume.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.flume.layout.ConversionPattern= %d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n  \n```\n\n这样后端产生log4j日志则会向运行着Flume机器的41415端口发送。最终发送的日志数据格式如下：\n\n```java\n2019-05-10 16:23:40,743 [http-nio-8080-exec-6] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songplay songID:17  \n```\n\n* Flume配置方案\n\n采集页面浏览日志的Flume agent使用命令从access.log文件中读取新增加的浏览日志数据。注意需要agent的channel选择memory-channel，需要将缓存的容量（capacity，默认值为100）设置一个较大的值，否则当日志数据过多时，会出现异常。本文设计memory-channel.capacity设置为10000。此外使用kafka-sink，将日志分发到指定topic的kafka消息队列中，本文设计页面浏览日志将被分发至topic为streamingtopic的消息队列中。关键配置项如下：\n\n```java\nexec-memory-kafka.sources.exec-source.type = exec  \nexec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log  \n  \nexec-memory-kafka.channels.memory-channel.capacity = 10000  \nexec-memory-kafka.channels.memory-channel.transactionCapacity = 10000 \n  \nexec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink  \nexec-memory-kafka.sinks.kafka-sink.topic = streamingtopic  \n```\n\n页面行为大类的日志使用同一的Flume agent采集，使用avro-source监听特定端口发来的日志消息数据。为了能够将不同的行为分流，需要使用过滤器interceptors将日志数据中用于分流的标记提取出来，在交给kafka-sink分发至不同topic的消息队列中。interceptor使用正则表达式将日志数据中的标记提取出来命名为topic，在kafka-sink的配置中使用topic即可。interceptor配置如下：\n\n```java\n#定义source中的过滤器  \nagent1.sources.avro-source.interceptors=i1  \nagent1.sources.avro-source.interceptors.i1.type=regex_extractor  \nagent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) songID:(.*?)  \nagent1.sources.avro-source.interceptors.i1.serializers=s1 s2   \nagent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic  \n  \nagent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink  \nagent1.sinks.kafka-sink.topic = %{topic}  \n```\n\n### 4.2日志处理、分析与持久化模块\n\n#### 4.2.1日志处理、分析与持久化模块设计\n\n使用Spark Streaming和HBase完成日志数据的处理、分析和持久化。首先不同类型的日志需要用不同的Spark Streaming应用作业来处理，因为不同的日志分析的逻辑要求是不同的（如处理时间的要求），结构也是不同的，持久化的策略也是不同的。为了便于程序的可靠运行，减少不同类型日志之间处理逻辑的耦合情况，需要将日志处理分析的逻辑按照kafka topic进行划分。日志实时流数据可以根据日志数据结构的字段进行过滤，因为日志数据是结构化的，也是可以定制的。使用HBase进行持久化，一方面支持大量数据的存储，同时还能够动态的修改表结构，增加需要记录的字段十分方便。HBase的表根据时间、类目等主要维度进行设计。\n\n#### 4.2.2日志处理、分析与持久化模块实现\n\n* 应用初始化与流数据获取\n\n每个Spark Streaming应用作业与一个Kafka topic相对应。创建Spark Streaming处理上下文，指定实时流处理的时间间隔，并从Kafka消息队列中消费对应topic的日志数据，如下：\n\n```java\nval Array(zkQuorum, group, topics, numThreads) = args  \nval sparkConf = new SparkConf().setAppName(\"MyStreamingApp\").setMaster(\"local[5]\")  \nval ssc = new StreamingContext(sparkConf, Seconds(60))  \nval topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap  \nval messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)  \n```\n\n* 日志数据模板化与清洗\n\n以上就根据日志分析的需求得到了实时的日志流数据，接下来就是对这些日志数据进行必要的过滤。首先对于浏览日志需要将所有不成功的请求清洗掉，保留成功的请求日志。由于浏览日志的数据结构是定制化的，所以我们可以根据特定字段进行清洗，本文设计首先要将每一条数据转换成Scala的模板类对象，后续对每一条数据的操作就是针对这个模板类对象完成的。其次还要将请求状态不成功的日志数据过滤掉。方法如下：\n\n```java\nval logs = messages.map(_._2)  \nval cleanData = logs.map(line => {  \n  val infos = line.split(\"\\t\")//把每行日志根据\\t分隔符分割  \n  val url = infos(2).split(\" \")(1)  \n  // url为/music/128.html  \n  var itemID = 0  \n  if (url.startsWith(\"/music\")){//music开头的把编号拿出来  \n    val itemIdHTML = url.split(\"/\")(2)  \n    itemID = itemIdHTML.substring(0, itemIdHTML.lastIndexOf(\".\")).toInt  \n  }  \n  //将每一条日志数据转换成Scala中的模板类，并将请求状态为200的日志数据保留  \n  ClickLog(infos(0), DateUtils.parseToMinute(infos(1)), itemID, infos(3).toInt, infos(4))  \n}).filter( clicklog => clicklog.itemId != 0 && clicklog.statusCode == 200)\n```\n\n最终得到的结构化数据为：\n\n```java\nClickLog(87.30.187.55,20190219090601,145,200,https://www.sogou.com/web?query=今日专辑)  \n```\n\n从左到右分别表示请求源地址、时间、请求页面编号、请求状态、来源网站。\n\n* 实时统计访问量\n\n进行业务功能开发时需要兼顾到持久化和后续可视化的可行性。对于访问量的统计需要考虑维度的选择，因为本文设计需要以天为维度和以类目维度记录访问量，即今日到当前时间为止的各类目的访问量和所有页面的访问总量，所以HBase中关于访问量的表中rowkey设计要以日期和类目编号为维度，本文设计访问量表中的rowkey形如20190511_146，列族记录到当前时间为止的访问量。方法如下：\n\n```java\ncleanData.map(x => {  \n      (x.time.substring(0,8)+\"_\"+x.itemId, 1)//记录日期与类目编号  \n    }).reduceByKey(_+_).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[itemClickCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(itemClickCount(pair._1, pair._2))  \n        })  \n        //将实时统计得到的类目访问量写入HBase  \n        itemClickCountDAO.save(list)  \n      })  \n    })  \n```\n\nHBase中行数据的更新操作使用incrementColumnValue方法完成，这样可以直接将当前时间段得到的访问总量直接与之前统计的访问总量相加，方法如下：\n\n```java\ndef save(list: ListBuffer[itemClickCount]):Unit = {  \n    //单例模式获取对表操作的实例  \n    val table = HBaseUtils.getInstance().getTable(tableName)  \n    for (ele <- list){//每一行的统计信息进行写入  \n     table.incrementColumnValue(Bytes.toBytes(ele.day_item),  \n        Bytes.toBytes(cf),  \n        Bytes.toBytes(qualifer),  \n        ele.click_count)  \n    }  \n  }  \n```\n\n以上就完成了在日期和类目编号维度下的实时访问量统计，HBase中记录的数据结构如下：\n\n```java\n20190511_130              column=info:click_count, timestamp=1557551220479, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00\\x0B\\xDF \n20190511_131              column=info:click_count, timestamp=1557551220479, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x0D          \n20190511_145              column=info:click_count, timestamp=1557551220483, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00-\\xFD\n20190511_146              column=info:click_count, timestamp=1557551220481, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x009\\x9C  \n```\n\n* 实时统计窗口时间内访问量\n\n以上完成了实时统计访问量的需求，但是使用者还需要从更细化的维度观察使用数据。对于访问量，还需要在一天内以24小时为维度进行统计，也就是展示访问量在一天之内的趋势。可以预测网站在一天内的不同时间访问量是不同的，在某个时段可能有高峰，某个时段可能是低谷，这样网站是运营人员就可以选择在不同的时间完成不同是任务，比如在高峰时加大某项业务的投放力度，在访问比较少时完成新版本的发布等。本文设计使用Spark Streaming的window操作接口，分析窗口时间内所有流数据，完成每30分钟窗口期内类目访问量的统计。window的时间间隔和滑动时间均可自定义。实现方法如下：\n\n```java\ncleanData.map(x => {  \n      (x.time.substring(0,8)+\"_\"+x.courseId, 1)  \n    }).reduceByKeyAndWindow((x: Int, y: Int) => x + y,  \n      Seconds(1800), Seconds(1800)).foreachRDD(rdd => {//设置窗口大小为1800秒（30分钟）、滑动时间间隔为30分钟  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[ClickCountTrend]  \n        val time = FastDateFormat.getInstance(\"HHmm\").format(new Date())//窗口时间戳  \n        partitionRecords.foreach(pair => {  \n          list.append(ClickCountTrend(  \n            pair._1.substring(0,8)+time+pair._1.substring(8), \n            pair._2))  \n        })  \n        ClickCountTrendDAO.save(list)  \n      })  \n    })  \n```\n\n以上就实现了每30分钟统计一次各类目在前30分钟内的访问量，对这个数据的积分即是当前时间的总访问量。HBase中持久化的rowkey结构为201905111337_146。这样就能够详细的观察访问量在一天中趋势，以及每个类目访问的趋势。但是目前这个实现方案有一个问题就是，当30分钟窗口时间跨越0点时，会丢失前一天的数据。比如，窗口期为5月10日23：45至11日0：15，由于rowkey设计的原因前15分钟的数据会计算到10日0：15的数据上。为了解决这个问题可以让应用任务在整点时启动。但是解决方案还是需要寻找的。\n\n访问量趋势的可视化展示参见可视化实现部分。\n\n* 来源网站实时统计\n\n针对从搜索引擎来的流量，需要统计来源网站和搜索关键词，这样网站可以进行SEO（搜索引擎优化），提高网站竞争力。对于来源网站的统计依然需要从清洗处理过后的模板数据中完成，这个维度的表中的rowkey以日期、来源网站和类目编号为维度。对于搜索关键词的统计也是以日期为维度，统计当前日期的热门搜索词。\n\n* 页面行为日志分析\n\n对于页面上各种不同的行为日志数据，为了便于后续的处理分析，都需要先将数据格式化为模板类对象。之后再进行日志数据的分析和持久化。以音乐播放为例，模板化方法如下：\n\n```java\n  //messages为kafka产生的消息数据  \n  val logs = messages.map(_._2)  \n  val songPlayLog = logs.map(line => {  \n  \tval infos = line.split(\" \")  \n  \tval time = DateUtils.parseToMinute(infos(0)+\" \"+infos(1))  \n  \tval UID = infos(1).split(\":\")(1).dropRight(1)  \n  \tval songID = infos.last.split(\":\")(1).dropRight(1)  \n  \tSongPlayLog(time, UID, songID) //最终音乐播放行为模板类结构SongPlayLog(20190401125947, 00001, 15)  \n})  \n```\n\n对于不同行为日志数据的分析需要根据实际业务需求进行个性化设计。同时还要注意使用实时流处理时要避免复杂的连接操作，实时流处理可以选择记录维度较简单、在原始数据上进行简单抽象的数据结果，后续复杂的分析和连接操作可以放在对实时性要求不高的流程中完成。一下以音乐播放行为为例，进行两个维度的数据分析与持久化。首先统计至当前时间的音乐播放量，方法如下：\n\n```java\nsongPlayLog.map(x => {  \n    \t  //rowkey设计为日期与歌曲编号维度  \n      (x.time.substring(0,8) + \"_\" + x.songID, 1)  \n    }).reduceByKey(_+_).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[SongPlayDailyCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(SongPlayDailyCount(pair._1, pair._2))  \n        })  \n        SongPlayDailyCountDAO.save(list)  \n      })  \n    })  \n```\n\n同样地对于音乐播放是会有实时热点产生，所以需要实时统计窗口时间内的歌曲播放数据，并在较短时间内更新，以响应热点。首先需要进行HBase表设计，本文设计对于时间段内歌曲播放这类热点数据只保留最新时间的分析结果即可。其次，本文设计针对近一个小时内的歌曲播放进行统计，滑动间隔为10分钟，即每十分钟更新一次近一个小时的歌曲播放数据。在进行HBase表创建时需要设置表中数据的存活时间TTL，本文设计设置为600s（10分钟），即表中数据产生十分钟后失效，最新的数据计算后会被写入表中。分析数据方法如下：\n\n```java\nsongPlayLog.map(x => {  \n      (x.time.substring(0,8) + \"_\" + x.songID, 1)  \n    }).reduceByKeyAndWindow((x: Int, y: Int) => x + y,  \n      Seconds(3600), Seconds(600)).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[RecentlySongPlayCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(RecentlySongPlayCount(pair._1,  \n            pair._2))  \n        })  \n        RecentlyPlaySongDAO.save(list)  \n      })  \n    })  \n```\n\n以上即得到了实时的歌曲播放统计数据，以及时间段内热点歌曲的播放统计。其他的行为日志的分析需要根据实际需求来实现，本文设计还有歌曲收藏和歌曲评论两个行为日志分析，分别从其他不同的角度反映网站内容数据。\n\n#### 4.2.3 Spark Streaming应用提交集群机器执行\n\n将编写好的Spark Streaming作业程序编写好后使用mvn编译，将编译好的jar包拷贝至集群的机器上，在使用spark-submit指令进行任务的提交，指令如下：\n\n```java\nspark-submit --master local[5] \\  \n--jars $(echo /home/hadoop/app/hbase-1.2.0-cdh5.7.0/lib/*.jar | tr ' ' ',') \\ \n--class com.chaoyue.spark.project.scala.MyStreamingApp \\  \n--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \\  \n/home/hadoop/lib/sparktrain-1.0.jar \\  \nhadoop000:2181 test streamingtopic 1  \n```\n\n此时Spark Streaming作业即运行在集群上了，可以再控制台查看运行日志如下：\n\n```java\n19/05/12 16:33:00 INFO ShuffledDStream: Time 1557649980000 ms is invalid as zeroTime is 1557649260000 ms , slideDuration is 1800000 ms and difference is 720000 ms  \n19/05/12 16:33:00 INFO JobScheduler: Starting job streaming job 1557649980000 ms.0 from job set of time 1557649980000 ms  \n19/05/12 16:33:00 INFO SparkContext: Starting job: foreachPartition at MyStreamingApp.scala:59  \n19/05/12 16:33:00 INFO JobScheduler: Added jobs for time 1557649980000 ms  \n19/05/12 16:33:00 INFO DAGScheduler: Registering RDD 115 (map at MyStreamingApp.scala:56)  \n19/05/12 16:33:00 INFO MemoryStore: Block broadcast_43 stored   \n19/05/12 16:33:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 192.168.1.108:42189 (size: 1869.0 B, free: 366.1 MB)  \n...  \n19/05/12 16:33:05 INFO BlockGenerator: Pushed block input-0-1557649985600  \n19/05/12 16:33:08 INFO MemoryStore: Block input-0-1557649988600 stored as bytes in memory (estimated size 305.0 B, free 365.9 MB)  \n19/05/12 16:33:08 INFO BlockManagerInfo: Added input-0-1557649988600 in memory on 192.168.1.108:42189 (size: 305.0 B, free: 366.0 MB)  \n19/05/12 16:33:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.  \n19/05/12 16:33:08 WARN BlockManager: Block input-0-1557649988600 replicated to only 0 peer(s) instead of 1 peers  \n19/05/12 16:33:08 INFO BlockGenerator: Pushed block input-0-1557649988600  \n```\n\n### 4.3分析结果可视化模块\n\n#### 4.3.1分析结果可视化模块设计\n\n可视化部分采用前后端分离的开发模式，最终通过web网页的方式展示可视化结果。可视化模块不只是数据库中数据的展示，还要完成在实时流处理场景中不适合实时处理的其他维度的操作，即完成上个模块没有完成的复杂维度的分析。例如实时流处理中记录的歌曲播放信息只包括时间戳、用户ID和歌曲ID，但是歌曲本身的信息是丰富的，要展示的维度也是丰富的，所以在可视化模块进行复杂的连接操作，完成最终更丰富的分析结果展示。\n\n前端使用react为基础框架，react组件的思想非常适合复用，对于不同数据进行相同类型的展示时可以更换数据源复用组件。此外使用蚂蚁金服的开源react UI组件库ant-design和ant-design Pro，以及商业场景下的数据可视化解决方案Bizcharts，完成丰富的数据可视化展示。后端使用SpringBoot透出相关接口，给前端组件提供需要展示的数据，此外后端还负责实时流处理过程中没有完成的连接操作和其他处理数据的操作。\n\n#### 4.3.2分析结果可视化模块实现\n\n* 可视化web界面概览\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368878922/0)\n\n如4.1所示，今日数据概览页面展示当日实时数据，包括：实时PageView、七日PV走势、今日PV走势，各类目实时访问统计、各类目访问趋势、今日来源网站统计和今日热搜词云。页面根据维度逐渐细化来布局，从上到下从左到右，首先是PV的总量，然后从一天中的时间为维度和类目为维度细化PV值，再到其他杂项指标。今日PV走势组件展示了网站访问在一天内的趋势，每30分钟统计30分钟内访问量，可以看出音乐网站的用户群体在中午和傍晚到晚上十分活跃。类目实时访问统计展示了用户对哪一个子类目更偏爱，哪些类目需要优化。七日PV比较给一周内的PV比较，可以大致看到某一天的网站浏览状况，需要历史日期更细化的数据可以到历史数据查询里查看。类目访问趋势展示了各类目的今日访问趋势，是前面两个维度的横向和纵向细化，帮助网站运营人员从细节查看网站数据。杂项数据包括来源网站统计和热搜词云，这些展示的是从搜索引擎过来的流量的分析，根据搜索引擎的热搜词，可以帮助网站进行SEO（搜索引擎优化），提高网站排名。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368954251/0)\n\n如图4.2所示，历史数据查询提供选择日期的组件，可以查看所选日期当天的相关数据。提供30天PV比较，可以看到更长时间区间内网站浏览量的变化，通过这些变化的观察可以对网站的运营策略进行适当的调整。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369327550/0)\n\n图4.3是音乐网站行为的模拟，具有典型的歌曲资源位组件，可以模拟的行为有歌曲播放、收藏和评论。点击按钮则会调用后端相关接口，后端通过log4j记录下行为日志数据。一个歌曲资源包括歌名、歌手、专辑、封面地址、音乐资源地址、风格和标签等，使用MySQL保存这些资源信息。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369422528/0)\n\n图4.4是页面行为日志数据的实时处理分析展示页面。展示的内容包括，今日歌曲播放一览、近一小时最热播放歌曲、播放歌曲类型统计、今日加权热门歌曲、今日歌曲评论和收藏统计。今日歌曲播放一览展示今日实时的歌曲播放量统计。近一小时最热播放歌曲展示一小时窗口时间的歌曲播放量排行，是前一个统计维度的子集。歌曲播放类型统计使用南丁格尔玫瑰花环展示实时歌曲播放不同类型数量的统计，这个维度即是对实时流处理中得到的分析结果进行更高一级的抽象，在可视化的后端将歌曲播放量与歌曲资源进行连接操作得到歌曲类型统计。今日加权热门歌曲通过将三中行为的统计量进行加权，得到热门歌曲，加权权值为播放0.5：收藏1：评论0.8。另外两个展示的是实时歌曲收藏量与评论量。\n\n* 可视化部分详细展示\n\n下面将上节涉及的主要功能组件进行展示。\n\n* 页面实时浏览量\n        \n实时展示网站今日总浏览量，周同比展示今日与前一周当日的浏览量百分比，日环比展示与前一日的浏览量百分比，日均展示一周七天的浏览量均值。如图4.5所示。\n        \n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369535329/0)\n\n* 七日浏览量走势\n\n展示七天前至当日的浏览量走势，如图4.6所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369649506/0)\n\n* 今日实时浏览量走势\n\n以半小时为维度展示今日浏览量数据，每隔半小时统计半小时窗口期中的浏览量，能够清晰展示浏览量在一天中的变化。如图4.7所示，可以看出在凌晨时分访问量逐渐减少，到了早上访问量开始逐渐上升。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369685204/0)\n\n* 各类目实时浏览量统计\n\n以网站不同类目页面为维度进行浏览量数据以及占比的实时展示，音乐网站下设6个大类目：瞩目艺人、最近播放、今日专辑、为你推荐、今日歌单和新近发布。如图4.8所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369719484/0)\n\n* 今日各类目访问趋势\n\n以时间与类目为维度进行浏览量数据的展示，是前两个数据展示组件的细化。如图4.9所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369747929/0)\n\n* 来源网站实时统计\n\n展示今日来源网站的数据统计，如从百度搜索进入主站则将百度搜索记录下来。如图4.10所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369788944/0)\n\n* 今日热搜词云\n\n从搜索引擎进入主站时所进行的搜索词数据统计，如在搜索引擎中搜索“欧美音乐”进入主站，则对“欧美音乐”这个词条进行一次记录，用于搜索引擎优化。如图4.11所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369815950/0)\n\n* 月度浏览量比较\n\n以月为维度对浏览量数据进行展示比较。如图4.12所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369855984/0)\n\n* 页面行为模拟模块\n\n音乐网站页面用户行为的模拟组件，可模拟音乐播放、收藏、评论三种行为，在用户进行页面行为时则会进行日志的记录。如图4.13所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369965341/0)\n\n* 今日歌曲播放一览\n\n今日实时歌曲播放量实时数据展示，如图4.14所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369987974/0)\n\n* 近一小时播放Top歌曲\n\n近一小时窗口时间内歌曲播放量展示，每十分钟进行一小时窗口时间播放量的统计，展示当下实时热播歌曲。如图4.15所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370044063/0)\n\n* 播放音乐类型统计\n\n以音乐风格为维度进行歌曲播放量数据实时展示，如图4.16所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370078824/0)\n\n* 多权重今日热门歌曲\n\n从播放、收藏和评论三个维度进行不同权重的加权计算，得出今日热门歌曲进行展示，可以设计更复杂的加权算法得到热门歌曲，便于运营人员推送歌曲内容。如图4.17所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370164164/0)\n\n* 今日歌曲收藏一览\n\n今日实时歌曲收藏量数据展示，如图4.18所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1559370195372/0)\n\n* 今日歌曲评论一览\n\n今日实时歌曲评论量数据展示，如图4.19所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370221681/0)\n\n### 4.4 本章小结\n\n本章在系统需求分析与架构设计的基础上，分模块进行了详细的设计与实现，给出了每个部分的核心代码与解释，并在可视化模块介绍了各个分析数据的展现形式以及其中的意义。\n\n## 5总结与展望\n\n### 5.1 总结\n\n人类可以基于各种信息进行判断、决策，最终创造出价值，而数据作为信息的载体，正在被大量产生。然而数据的形式是多样的，而且是分散的，如何将分布的数据聚合并分析提取出有价值的信息，是大数据时代的挑战。海量数据分析作为互联网行业的闭环模式中的一环，其价值和重要性已经被广泛关注，越来越多的企业可是设立数据部门。一个网站的各个环节都会产生日志数据，这些数据形式多样且分散在不同机器实体上，如何高效采集、聚合、处理、分析和持久化给大数据分析带来了挑战。而现如今，更多的大数据分析需求还要加上“实时处理”这个需求。使用Spark作为分布式数据处理工具，整合海量日志采集工具Flume、可靠消息队列Kafka和海量数据存储HBase，满足了网站日志实时流处理的需求。Flume可定制化的从多种数据源分部式地采集日志，同时具备一定的数据处理能力，可以向多种下游数据目的地传输日志数据。消息队列Kafka保证了海量的日志数据能够有序、不丢失的被消费，同时运用topic机制使得在上游聚合的日志数据以不同的策略被处理和分析。Spark Streaming是整个系统的核心，其是在Spark Core之上的高级API，为整个数据处理流程提供了实时流处理的上下文。Spark的计算模型核心是弹性分布式数据集RDD，提供个基于内存的集群计算的容错性抽象。Spark Streaming的实时流处理能力是基于DStream，其是RDD的一个在时间和空间上的高层抽象，使得我们可以像处理RDD一样处理DStream。同时Spark Streaming支持从多种数据源创建DStream。可以说，Spark Streaming提供了基础实时流处理的解决方案。HBase提供了基于列的海量数据存储，不同于关系型数据库，HBase在大数据处理分析数据持久化方面有许多优点。本文基于Spark Streaming的音乐网站实时流处理，使用上述提及的几个工具和解决方案，同时使用react和SpringBoot进行了分析数据的可视化展示。\n\n本设计完成的主要成果如下：\n\n1）使用Flume实时采集服务器上access.log中的网页浏览日志和使用前端埋点采集到的页面行为日志。解决了分布式日志采集和聚合的问题，同时对日志进行了分流以供下游Kafka使用。\n\n2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源。使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力。在实时流处理中分析出基础业务指标数据，以供上层最这些数据进行其他复杂的操作，并将这些分析结果数据持久化至HBase中。\n\n3）使用React和SpringBoot构建前后端分离项目，同时在后端结合MySQL完成数据分析上层复杂的连接操作。最终从今日数据概览、历史数据查询和页面行为分析模拟三方面进行展示。分析数据展示包括：实时网站访问量、当日实时访问量走势、周和月维度访问量比较、各类目实时访问量及走势、来源网站统计、搜索引擎热搜词统计、歌曲实时播放量、近一小时热门歌曲播放量、歌曲播放类型比较、加权热门歌曲、实时歌曲收藏和评论统计。\n\n本文系统提供的数据可以帮助网站的管理、运营和开发维护人员更好地优化网站，比如优化网站类目内容、优化搜索引擎结果以及选择最合适的时间升级网站服务等。\n\n### 5.2 展望\n\n实时流处理不仅仅是进行简单的数据分析，还有许多实时响应的场景下有更复杂的需求。比如，电商搜索推荐系统在搜索导购的各个场景都需要能够秒级召回个性化推荐数据，而这需要大量的离线和实时数据处理相结合和完成。Spark作为分布式数据处理框架，不仅提供了秒级实时流处理的高级抽象Spark Streaming，还提供了用于机器学习的MLlib、图处理的GraphX、交互式查询SparkSQL，对这些高级抽象进行整合应用可以处理更复杂场景下的需求。","source":"_posts/graduation-design-paper.md","raw":"---\ntitle: 基于Spark Streaming的网站日志实时流处理\ndate: 2019/06/01\ncategories: \n    - 毕业设计\n---\n\n## 摘要\n随着大数据时代的到来，数据总量不断增加，同时分布式存储与计算技术也在快速发展，如何从海量、分散的数据中提取出有价值的信息，已经成为各个行业的迫切需求，对数据的实时处理和响应变得越来越重要。\n\n本文设计了基于Spark Streaming的网站日志实时流处理系统，针对通用音乐网站两种类型的日志：网页浏览日志和网页行为日志，进行了实时采集、传输、处理、分析、持久化和可视化。本文完成了以下几个功能：1）使用Flume采集日志数据，解决了分布式日志采集和聚合的问题，同时对日志进行了分流；2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源；3）使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力，并将分析结果数据持久化至HBase中；4）使用前后端分离的开发模式完成分析结果的可视化展示。系统实现了将大量分散的日志数据进行实时整合，并完成分析计算，最后从多种维度进行信息展示。\n\n关键词：实时流处理；分布式计算；Spark Streaming；Flume；前后端分离\n\n## ABSTRACT\n\nWith the advent of the era of big data, the total amount of data continues to increase, and distributed storage and computing technologies are also developing rapidly. How to extract valuable information from massive and decentralized data has become an urgent need of various industries. Real-time processing and response of data is becoming increasingly important.\n\nThis paper designs a real-time stream processing of website logs system based on Spark Streaming. It performs real-time collection, transmission, processing, analysis, persistence and visualization for two types of logs: universal web browsing logs and web page behavior logs. This paper has completed the following functions: 1) Using Flume to collect log data, solving the problem of distributed log collection and aggregation, and diverting the log at the same time; 2) Using message queue Kafka to solve a large number of log data transmission problems, which providing real-time log data source to the downstream application to process and analysis data; 3) using Spark Streaming to perform real-time processing and analysis of various types of log data, achieving the capability of second-level processing and phase data processing, and persisting the analysis result data to HBase; 4)Visualization of the results of the analysis using a development model with front and rear separation. The system realizes the real-time integration of a large number of scattered log data, completes the analysis and calculation, and finally displays information from various dimensions.\n\nKEYWORDS: Real-time stream processing; Distributed Computing; Spark Streaming; Flume; Front and rear separation\n\n## 1绪论\n\n### 1.1研究背景与意义\n\n随着DT（Data Technology）时代的到来，数据总量不断。预计到2020年，全球数据总量将超过44ZB，同时根据图灵奖获得者杰姆·格雷提出的“新摩尔定律”，每18个月，全球新增数据总量是自计算机诞生以来所有数据量之和。数据作为一种新的能源，被各行各业广为利用，为企业生产经营提供有价值的信息。同时，“爆炸式”增长的数据也对现有的数据采集、传输、分析和存储技术提出了挑战。\n\n互联网行业可以从多种多样的数据源采集信息，例如，服务器日志、网关日志、数据库日志和业务信息等。这些数据通过企业商业智能平台的挖掘分析，为企业内部以及各样相关者提供有价值的信息，例如，电商行业对于用户画像进行分析，进行精准营销和“千人千面”的召回数据，提高转化率；金融行业对于风险监控的要求较高，通过大数据分析，将提高企业的风控效率和准确率。而浏览器的页面型产品/服务的日志在众多数据源十分突出，一是其自然地提供了页面浏览（展现）的详细情况，以及具有定制化采集页面交互日志的能力，所以企业能够在这些日志数据中提取到大量可用有用的信息；二是部分日志信息具有很高的时效性，这是指采集到的日志数据需要实时分析、处理，并召回实时响应数据，为用户提供更好的体验。数据的离线和实时分析都是互联网行业“闭环”的重要部分。\n\n各大公司以及开源社区都提出了各种可用的计算模型和解决方案，为数据采集、传输、存储和分析提供了多种有用的工具。而目前最为火爆的工具之一就是Spark，它在Spark Core的基础上提供了多种高级实现，其中Spark Streaming使数据的实时处理成为可能。\n\n本文以一个通用音乐网站为背景，基于Spark Streaming对网站浏览日志和交互行为日志进行实时流处理，完成了从数据采集、传输、清洗、处理、持久化以及可视化的全流程。系统实现了网站服务器浏览日志的采集，以及通过前端埋点的方式采集可定制化的数据信息，使用Flume和Kafka向下游分流和传输数据。使用Spark Streaming清洗和处理数据，并将相关业务指标数据持久化到HBase中。可视化使用Springboot和React进行前后端分离的实现。\n\n本系统提供的数据有：\n* 页面浏览数据，包括Page View和网站各类目访问量，访问趋势等多维度的指标可视化数据，帮助相关人员进行网站的优化以及搜索引擎优化。\n* 页面交互信息数据，包括音乐网站情景下多种用户操作行为的记录与分析，并实时展示多维度的分析结果。\n\n### 1.2研究现状\n\n日志分析是在海量离散的日志数据中提取出有价值的，符合人们认知的信息。网站日志分析则是在页面浏览、页面行为等日志中得到多种维度的数据，供网站不同职能人员使用。例如，运营人员根据网站浏览量、各类目的访问情况等制定相应的运营方案；高层团队领导根据网站各业务转化率等指标灵活制定网站产品发展战略；开发人员根据网站服务性能数据优化产品性能，保障产品质量。如今已有许多商业上可行的方案，比如百度统计、神策数据等，但是它们要么关注仅仅关注访问量的统计，要么关注用户页面行为的统计。\n\n日志分析在国内外发展都十分迅速。最开始由于网站用户访问量不大，业务量规模也相对较小，整个网站的各个功能模块的日志都由专员进行分析。一般情况是远程登录各个节点来分析日志。随着业务量的不断增大，服务器以及功能模块的不断增多，日志分散在大量机器实体上，这给日志数据的采集、传输、分析和存储带来了挑战。\n\nGoogle用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。但是随着业务场景的复杂化，如互联网金融中的实时风控、电商中的个性化推荐数据召回等的出现，运用Hadoop MapReduce完成的离线数据分析已经不能满足企业发展的需求。Spark于2009年诞生于UC Berkeley的AMP实验室，并与2013年贡献至Apache社区，成为近年来最火热的大数据开源项目。\n\nSpark是基于内存的并行计算框架，也是基于MapReduce计算模型的分布式计算框架，其解决了Hadoop MapReduce的诸多问题，例如不能重用在迭代式机器学习和图计算中经常需要的中间结果，Hadoop MapReduce需要将这些中间数据进行复制、硬盘I/O和序列化。而Spark实现了弹性分布式数据集（RDD），一种基于内存的集群计算容错性抽象，能够很好地适应各种业务场景。Spark Streaming是在Spark Core之上的高级API。Spark Streaming提供了表示连续数据流的、高度抽象的被称为离散流的Dstream，可以通过多种数据源创建Dstream，为下游实时流处理使用。所以Spark Streaming为当前实时流处理的场景需求提供了可靠的上下文。\n\n此外，使用Flume与Kafka为海量日志的采集和传输提供了解决方案。Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，通过Flume可以将分散在各个服务器上的日志收集起来，以供下游日志数据处理分析的使用。Kafka是一种高吞吐量的分布式发布订阅消息系统，在大型网络中运用其对网站的服务性能进行扩展，能够保证所有服务请求不丢失，同时Kafka作为大型网站的基础设施，也为实时日志流处理提供消息队列的服务。\n\n目前对于实时流处理有许多解决方案，日志采集、传输、分析和存储的技术也有许多种选择。不同的解决方案/工具是由特定场景下提出的，寻找最合适的工具完成系统设计十分关键。\n\n### 1.3论文结构\n\n本文将全文内容划分为五个章节进行展开，其组织结构如下：\n\n第一章：绪论部分。简要介绍了网站日志实时流处理的研究背景和意义，并对现有大数据分析技术进行了简单的介绍，同时描述了本文设计的主要工作。\n\n第二章：系统相关理论和技术介绍。简要介绍了数据处理和日志分析的出发点和要点，针对日志采集、传输、处理、分析的难点进行介绍，并提供了现有可靠的解决方案或工具。\n\n第三章：系统需求分析与架构设计。基于本文设计的系统出了分模块的需求，并整合最佳的解决方案，设计了系统整体架构。\n\n第四章：从日志数据产生与采集、日志数据传输、处理、分析与持久化以及分析结果可视化三个方面进行了详细设计与实现。\n\n第五章：对本文设计的系统进行总结，并提出接下来要进一步完善的需求。\n\n## 2相关理论与技术\n\n### 2.1互联网中的闭环理论\n\n#### 2.1.1闭环理论\n\n闭环理论是源于系统论和控制论的一种管理思想，其核心是通过有效的管理和控制手段，构成一个连续封闭的管理回路，使得系统中的每一个环节都能有效的衔接。最终达到提升管理水平，实现既定目标的目的。闭环系统的典型特征有：可控性强、首尾相连、循环往复。闭环式的组织能够高效地利用资源，灵活应对外界变化，达成组织目标，并在这个过程中不断自我提升。\n\n闭环理论强调“可控性”，在组织的运行流程中，尽可能的收集信息，并与原计划进行匹配，出现偏差则进行调整和修正。从而实现系统的高效演进与自我提升。\n\n#### 2.1.2互联网闭环\n\n闭环理论最初由休哈特于1930年提出，现在已经衍生运用于各行各业。对于计算机互联网这样的强调系统和控制的行业，闭环理论运用十分广泛。在互联网行业中运用闭环理论的系统通常包括：验证、反馈、分析和控制这几个部分。直观的运用实例如下：\n\n* 产品开发、运营优化。如产品新特性发布前的灰度测试、电商及内容类产品的实时推荐系统等。\n* 互联网服务。如O2O线上到线下等各类分销平台、金融风控系统等。\n\n为了达到产品的不断提升，并对外提供更好地服务，互联网行业必须将整个系统的各部分形成完整闭环，而数据挖掘与分析是补完闭环的关键环节。网站日志实时分析作为网站系统闭环的一部分，通过分析来自用户的真实访问情况和行为情况反馈，为运营和决策人员提供有价值的信息，用以不断完善产品质量。此外通过收集数据为产品的其他功能，如推荐投放系统提供一手数据资源，完善相关功能，使得反馈结果实时触达用户。整个系统各个部分都为系统的提升做出贡献，并使之成为一个可提升的完整闭环。\n\n### 2.2网站日志采集\n\n浏览器的页面型产品/服务的日志采集可以分为如下两大类。\n\n#### 2.2.1页面浏览（展现）日志采集\n\n页面浏览日志是指当一个页面被浏览器加载呈现时多采集的日志。此类日志是最基础的互联网日志，也是目前所有互联网产品的两大基础指标：页面浏览量（Page View，PV）和访客数（Unique Visitors，UV）的统计基础。页面浏览日志是目前成熟度最高和完备度最高，同时也是最具挑战性的日志采集任务。\n\n页面浏览日志通常是记录在应用服务器上，如Nginx的access.log。\n\n#### 2.2.2页面交互日志采集\n\n当页面加载和渲染完成之后，用户可以在页面上执行各种操作，随着前端技术的发展用户可以与页面上的元素完成很多互动操作，如启动、点击、拖拽和曝光等，这些行为信息价值巨大，设计者通常会要求采集用户的互动行为日志，以便通过量化获知用户的兴趣点或者优化体验。而行为数据属于低价值密度数据，其数量十分巨大，需要强大的数据处理能力和分析能力才能有效利用。\n\n行为日志的记录有多种方式，网页应用常用前端埋点的方式进行采集。\n\n### 2.3 ETL过程与Flume\n\n#### 2.3.1 ETL\n\nETL（Extract-Transform-Load）是一种数据仓库技术，用来描述将数据从数据源抽取（extract）、转换（transform）和加载（load）的过程。这一过程常用于数据仓库技术，但是在目前大数据应用中也十分重要。具体到日志分析类应用，由于服务器众多，同一类型的日志分散在不同的机器实体上。同时不同类型的日志有不同的结构，为了便于分析通常情况下需要同一结构。此外对于各种行为日志需要又不同的下游数据分析应用来处理，这就涉及到一定程度上的分流。因此在网站日志实时流处理中需要有一个ETL过程来进行分散数据的采集和一定程度的过滤清洗，使得下游应用能够高效运作。\n\n目前，Apache Flume十分适合这类ETL过程的业务场景。\n\n#### 2.3.2 Flume\n\nFlume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，其能够将不同源的日志数据汇聚到统一的数据存储中；Flume具有基于流式数据的简单且灵活的架构，同时具有优秀的容错机制和故障恢复机制；Flume运用简洁又灵活的数据模型，使得其十分适合在线分析应用。\n\nFlume不仅可用于日志数据的聚合。由于数据源是可定制的，Flume还可以传输海量的事件数据，包括但不限于，网络流数据、社交媒体产生的数据、电子邮件数据和各种可能形式的数据。\n\nFlume处理数据的最小单位是一个Flume Event。Flume Agent是一个常驻JVM的进程，用于处理从外部源传输来的event。Agent由source、channel和sink组成，如图2.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366186537/0)\n\n用户可以选择多种数据源作为source，如avro source、thrift source和exec source等。在本文设计中，对于页面浏览日志使用exec source，执行系统读取指令采集log文件中新增的日志信息。\n\n对于页面行为日志，采用avro source监听特定的网络端口采集行为信息数据。\n\n对于采集信息的转换，Flume为source中提供了interceptor（过滤器）这个功能。在本文设计中，将运用过滤器对页面行为信息根据行为类型进行分流，以便下游不同数据处理应用使用。\n\nFlume的各个单位事件数据通过sink传递给下游应用使用。本文设计中使用kafka sink传递给下游kafka生产者使用。此外，为了防止事件数据过多，使用channel起到缓冲的作用，可以通过设置channel缓冲的大小来确定其承载力。\n\n### 2.4 Kafka\n\n消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋、日志处理等问题，实现高性能，高可用，可伸缩和最终一致性架构。\n\nKafka是经常用于日志处理的消息中间件，解决大量日志传输的问题。上游日志采集模块将采集到的数据写入到Kafka队列中，Kafka消息队列负责日志数据的接受、存储和转发，下游日志分析处理应用订阅kafka队列中的日志数据。\n\n本文设计中下游日志分析应用即是Spark Streaming作业。由于日志的数量很多，不同类型日志的处理分析逻辑不同，所以引入kafka消息队列。一方面防止日志数据过多淹没下游应用，另一方面按消息的topic进行分流消费。\n\n### 2.5 Hadoop与Spark\n\nGoogle用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。\n\nHadoop是一个分布式系统基础框架，用户可以在不了解分布式底层细节的情况下，开发分布式程序，充分利用集群的威力进行告诉运算和存储。Hadoop诞生于2005年，在社区和众多技术人员的支持下，Hadoop生态圈逐步扩大，形成了一整套用于分布式数据处理分析存储等功能组件。其中最重要的组件有，分布式文件系统HDFS、分布式计算框架MapReduce、分布式列存储数据库HBase、分布式协作服务ZooKeeper、日志收集工具Flume、分布式资源管理器Yarn等。这些组件提供了各种业务场景下数据处理需求的解决方案。其中MapReduce是一种分布式计算模型，用以进行大数据量的计算。其屏蔽了分布式计算框架的细节，将计算抽象成map和reduce两个过程，具有高可靠性、高扩展性、成本低等特点。因此MapReduce被广泛运用与大规模数据分析。\n\n但是随着更多复杂数据分析处理场景的出现，如迭代式计算和交互式数据挖掘等，MapReduce计算模型的出现了缺陷。首先，MapReduce将数据处理的过程抽象成2个阶段，map和reduce，虽然高度抽象，技术人员只需要编写两个函数即可。但是高度抽象带来的问题是表达能力有限，当出现更复杂的计算要求时，不得不将map和reduce两个过程复杂化或者增加更多的MapReduce任务。同时，MapReduce过程中的计算结果都需要进行磁盘I/O，这样当需要多个MapReduce任务写作时，导致任务之间的衔接涉及I/O开销，不能直接重用中间结果。此外，在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务。\n\nMapReduce缺少对分布式内存的应用，而Spark在借鉴了Hadoop MapReduce有点的同时，很好地解决了MapReduce所面临的问题。首先Spark实现了一个分布式内存抽象的概念--弹性分布式数据集（以下称为RDD），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDD是分布式内存的一个抽象概念，提供了高度受限的共享内存模型，这使得Spark计算任务能够在内存中重用中间计算结果。Spark的计算模式也属于MapReduce，但是不限于map和reduce操作，将数据操作抽象成对RDD的一系列粗粒度转换，如map、filter、reduce、group等，比MapReduce更加灵活。由前两个特性，Spark实现的RDD能够拥有很好的容错性能，Spark计算模型将对RDD的操作记录成lineage，即血缘关系，在真实计算任务时并不立即对数据进行转换操作。这样当出现中间计算结果缺失时，只需要根据lineage重新计算数据即可。此外，Spark是基于RDD的lineage形成的有向无环图DAG来进行任务调度的，要优于MapReduce的迭代执行机制，如图2.2所示，实线框为RDD，不同RDD之间的转换形成DAG，Spark通过这个DAG进行任务调度。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366311701/0)\n\n针对流式数据处理场景，Spark在Spark Core的基础上实现了更高级的API--Spark Streaming。Spark Streaming是提供了表示连续数据流的、高度抽象的离散流（DStream）。DStream本质上是对RDD的一层封装，其中的每一个RDD都包含来自一个时间间隔的数据，如图2.3所示。DStream是一个没有边界的集合，没有大小限制，其代表了一个时空的概念。对DStream的操作，具体到每个时间段，就是空间的操作，也就是对时间间隔的对应批次数据的处理。Spark Streaming对于DStream的操作实质上就是对每个RDD应用批量操作。Spark Streaming将对流数据的处理抽象成对DStream的处理，而这种抽象是的开发人员能够像操作RDD一样操作DStream，所以说Spark Streaming提供了实时流处理的上下文。此外Spark Streaming支持从多个数据源创建DStream，如图2.4所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366347719/0)\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366361643/0)\n\n### 2.6前后端分离\n\n前后端分离是指在网站开发阶段前端与后端各司其职，前端只要根据设计还原出页面的逻辑展示，后端只要处理根网页数据相关的业务逻辑，并向前端透出需要的接口即可。这种模式能够使前后端并行开发，大大提高效率，并将前后端一定程度上解耦。本文设计中网页技术部分采用前后端分离的模式，模拟用户页面行为的产生，并且进行分析数据结果的可视化。前端采用react技术栈，后端采用SpringBoot。\n\nReact是Facebook公司开源的前端框架，是近年最火热的技术，国外Facebook、Instagram和国内知乎、蚂蚁金服等都在使用。React的思想是将前端所有元素组件化，将所有需要的UI控件甚至逻辑控件抽象成组件进行开发。每个组件维护自己的状态逻辑变化。React技术不断发展，社区中也不断贡献出特定场景下优秀的解决方案。\n\nSpringBoot是由Pivotal团队开发的框架，其能够简化新Spring应用的初始搭建以及开发过程。SpringBoot使用很多特定或默认的方式进行配置，简化了开发人员的准备工作时间，可以专注于真正业务的实现。SpringBoot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、产品级别的Spring应用，特别适合实现目前最火热的微服务架构。本文设计选择SpringBoot作为后端服务框架。\n\n### 2.7 本章小结\n\n本章介绍了互联网中的闭环理论以及针对日志数据采集、传输、处理、分析、持久化以及可视化的可靠的开源工具，为本文设计的系统中的各个需求场景提供了最优的解决方案。\n\n## 3系统需求分析与架构设计\n\n### 3.1系统需求分析\n\n#### 3.1.1系统概述\n\n本文设计以一个通用音乐网站的日志为分析对象的流处理系统，完成对日志进行实时采集、传输、清洗、处理、分析、持久化和可视化，最终得到网站页面浏览和页面行为日志数据的相关数据分析指标结果等有价值的信息，展示实时流处理的能力。按照数据流处理流程可将本系统分为以下几个部分：日志数据产生与采集模块、日志数据传输模块、日志数据处理分析模块、分析结果持久化与可视化模块。数据流程如图3.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366469597/0)\n\n音乐网站的日志分为两种类型，页面浏览日志和页面行为日志。对于两种类型的日志需要使用统一的日志收集工具进行收集，以便下游组件的使用；数据量庞大的日志数据需要使用消息队列进行传输，达到流量削峰的目的；从消息队列得到的日志数据进过分布式计算工具进行处理计算分析，持久化至存储设施中，最后将分析结果与网站业务相关的结构化数据进行更具体的连接操作完成分析结果的可视化展示。\n\n#### 3.1.2数据产生与采集需求分析\n\n两类不同的日志，页面浏览日志和页面行为日志，采用不同的方式产生。对于页面浏览日志中的每一条浏览日志需要是可配置的结构化数据，这样便于下游应用的清洗等操作,这些日志数据还需要存储在特定的位置，便于采集；对于页面行为日志，同样需要有一定的数据结构，便于在下游对不同的行为日志数据进行分流，但是页面行为日志可以有一定的定制化内容，因为不同的行为可能要采集的数据是不同的，因此行为日志的结构要保留一定的灵活性。行为日志数据的产生不能对用户体验产生影响，也就是说对于用户来说是透明的。\n\n日志数据的采集需要考虑到网站服务分布式的特点，也就是各类型日志都是分布在不同的服务器实体上，因此需要使用一个能够将分布的日志快速收集为全量日志数据的工具，同时为了与下游数据传输组件进行配合，采集工具需要具有一定的分流功能，即是能够将不同种类的日志信息先进行汇总，再向同一目的分发。\n\n#### 3.1.3数据传输与处理模块需求分析\n\n海量日志数据如果不加控制地发送至下游应用进行处理分析，会造成很多问题，最严重的就是超过应用的处理限度而不能正常服务，从而丢失数据。所以必须使用消息中间件解决大量日志数据传输的问题。消息中间件还需要有高可靠性和容错的能力。\n\n对于日志数据的处理分析模块，需要根据不同的日志类型和日志的种类进行不同的处理分析，也就是说页面浏览日志与页面行为日志分析的逻辑是不同的，对于不同行为的分析也是不同的。分析处理模块需要具有实时流处理的能力，以及一定的容错能力。同时还能分析复杂的数据场景，比如处理一定时间窗口内的所有数据。\n\n#### 3.1.4数据持久化与可视化模块需求分析\n\n分时结果数据的持久化需要考虑海量数据的特点，同时对于数据存储内容还要有一定的灵活性，这是指存储的分析结果内容应该能够根据业务的需求进行调整，能够动态地增加字段而不用修改表结构。\n\n可视化模块作为系统最终对外展示的部分，体现了整个系统的风格。优雅直观的界面和简洁直接的交互，不会让使用者产生疲倦感，同时有意愿继续使用和对产品提出改进意见。分析结果数据应该从各个维度进行数据的展示，维度由大到小，逐渐细化，帮助用户更好的观察数据。可视化模块是将一条条结构化的数据反映成直观形式表示的最后一步。\n\n可视化部分分为：今日数据概览、历史数据查询和网页行为分析。详细需求如下：\n\n* 今日数据概览\n\n展示网站各维度的今日实时数据，包括实时PageView、七日PageView走势、今日实时PageView走势、网站各类目访问量、各类目访问量走势、今日来源网站数据汇总和热搜词云。以供运营人员优化网站和优化搜索引擎结果等。\n\n* 历史数据查询\n\n可以选择所要查询的日期，查看所选日期的各项数据指标，同时增加以天为维度的30日访问量比较，可用于帮助网站各级别管理人员制定网站今后的内容发展方向，辅助决策。\n\n* 网页行为分析\n\n能够模拟音乐网站上的用户行为，如歌曲播放、收藏和评论等。从各个维度展现行为日志分析数据，如今日歌曲播放量排行，近一小时内热点歌曲排行、歌曲风格统计、热门歌曲标签云、热评歌曲和热门收藏歌曲等。各维度数据供网站运营人员优化网站歌曲内容。\n\n### 3.2系统架构设计\n\n#### 3.2.1总体架构\n\n本文设计的网站日志实时流处理系统，采取分布式的架构，遵循低耦合的原则。总体架构分为四部分：日志数据产生、日志采集与传输、日志处理分析和持久化以及分析结果可视化。总体架构如图3.2所示，页面浏览日志与页面行为日志采用不同的方案产生；使用Flume与Kafka来进行日志数据的采集与传输；实时日志数据通过Spark Streaming进行处理与分析并将分析结果持久化至HBase中；最后采用前后端分离的模式，整个MySQL中的业务数据完成分析结果的可视化展示。总体架构如图3.1所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366982662/0)\n\n#### 3.2.2 日志数据产生模块架构设计\n\n网站页面浏览日志是存储在服务器上access.log文件里面的结构化数据，并且这些浏览日志数据的结构是可配置的。本文设计使用Python脚本完成页面浏览日志的产生，并结合Linux的crontab功能完成周期性的调用。页面行为日志使用前端埋点的方式产生，在用户产生某种行为时调用js代码，向后端发出请求。后端使用SpringBoot搭建服务，通过解析请求和log4j产生行为日志数据。该模块架构如图3.3所示，产生的日志数据会被下游模块采集。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367132763/0)\n\n#### 3.2.3 日志采集与传输模块架构设计\n\n上一个模块已经产生了两类日志数据，本模块使用Flume和Kafka完成日志数据的采集与传输。页面浏览日志分布式的存储在网站服务器的access.log文件中，在每个服务器上需要部署一个Flume Agent完成access.log新增日志数据的采集，并将这些日志数据对接至下游Kafka生产者；页面行为日志是由服务器产生的log4j日志数据，通过配置会将这些数据发送至指定地址上的特定端口，在该地址上的服务器部署了Flume Agent，其会监听特定端口来采集日志数据。该Flume Agent会根据日志数据的内容进行分流，对接不同的Kafka生产者，使得不同的行为日志数据以不同的方法被实时处理。消息队列Kafka将Flume采集来的数据通过使用不同的topic传输至下游Spark Streaming。该模块架构如图3.4所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367178577/0)\n\n#### 3.2.4 日志处理分析和持久化模块架构设计\n\n本模块使用Spark Streaming进行日志数据的处理与分析，针对不同的日志向Spark集群提交相应的Spark Streaming应用，这些应用会根据topic拉取Kafka消息队列中的消息，之后进行处理与分析，分析结果将持久化至HBase中。该模块架构如图3.5所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367225228/0)\n\n#### 3.2.5 分析结果可视化模块架构设计\n\n分析结果可视化模块采用react作为前端框架，并结合蚂蚁金服技术体验部的UI组件库ant design、ant design Pro以及商业场景下数据可视化解决方案Bizcharts完成数据的多维度展示。后端使用SpringBoot连接HBase，将日志分析结果透出至前端。此外由于网站具体业务的复杂性，前一阶段的分析结果是比较抽象的，还需要结合存有具体业务数据的结构化数据库MySQL来实现更细化的分析结果数据展示，本次设计音乐网站中较重要的结构化数据是歌曲信息的数据。该模块架构如图3.6所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367274523/0)\n\n### 3.3 本章小结\n\n本章首先对要设计的系统进行总体概括，并分模块进行了需求分析，然后给出了总体架构设计和详细的分模块架构设计。根据各模块的需求给出了可行且合适的解决方案，为下一章系统的具体实现提供了清晰的方向。\n\n## 4设计与实现\n\n### 4.1日志数据产生与采集模块\n\n#### 4.1.1日志数据产生与采集模块设计\n\n由于时间和成本等现实原因，本文设计不能采用真实线上的日志数据，而是采用模拟的方式产生。模拟方式产生的日志数据与真实线上日志完全相同，首先保证浏览日志的数据格式如下：\n\n```java\n98.29.167.156 2019-05-10 14:31:01 \"GET /music/145.html HTTP/1.1\"  200 http://www.baidu.com/s?wd=新近发布  \n```\n\n包括请求源地址、时间、方法、请求页面、HTTP协议、请求状态以及来源网站。所以需要设计脚本完成access.log的生成，为了防止日志文件过大，需要定时清理。对于页面行为日志，采用前端埋点的方式产生。首先根据产品设计的需求找到需要记录的用户行为，比如点击、滑动、停留、键入内容等，细化到音乐网站上，用户的行为可以是点击播放音乐、评论音乐、收藏音乐和切换音乐等。这些日志需要在产生这些行为时调用相关后端接口，并向后端传送打点的内容，由后端服务器来完成日志的产生。后端服务器通过log4j产生格式化的日志供下游采集。\n\n这两类日志的采集都使用Flume完成，但是应用不同配置的Flume agent来完成采集。首先两种agent都最重要将日志数据传送给kafka消息队列，但是应为不同的topic，此外对于不同的页面行为日志也要是不同的topic。对于浏览日志，Flume agent使用avro source读取access.log中新增的日志信息，并通过kafka sink传至下游。对于行为日志统一使用一个Flume agent进行采集，使用avro source监听特定的端口，由后端服务器配置log4j发送至指定端口。此外还需要Flume对所有因为日志按照行为的种类进行分流，使下游产生不同topic的消息。\n\n#### 4.1.2日志数据产生与采集模块实现\n\n* 页面浏览日志产生\n\naccess.log使用python脚本产生，使用linux的crontab生成定时任务，每个一分钟产生一次access.log。关键代码如下：\n\n```java\nf = open(\"/home/hadoop/data/project/logs/access.log\",\"w+\")  \n    while count >= 1:   \n        query_log = \"{ip}\\t{local_time}\\t\\\"GET /{url} HTTP/1.1\\\"\\t{status_code}\\t{referer}\".format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)  \n        f.write(query_log + \"\\n\")  \n        count = count - 1   \n```\n\n最终access.log产生的数据结构如下：\n\n```java\n187.156.143.55  2019-05-10 15:48:01 \"GET /music/128.html HTTP/1.1\"  404 -  \n87.187.167.124  2019-05-10 15:48:01 \"GET /music/128.html HTTP/1.1\"  500 -  \n156.29.187.168  2019-05-10 15:48:01 \"GET /music/112.html HTTP/1.1\"  200 http://www.baidu.com/s?wd=新近发布  \n```\n\n* 页面行为日志产生\n\n前端埋点的方式由于技术选型的不同有很多方式，本文设计使用的是前后端分离的开发模式，所以在前端要记录行为操作数据的部分编写相应的js代码，调用后端业务接口。后端服务器处理接口调用请求的同时使用log4j产生日志数据。具体实现如下：\n\n前端埋点js代码：\n\n```java\n//播放歌曲记录  \n    songplay(songInfo){  \n        axios.get(Utils.defaultURIdefaultURI+\"/actionLogger\", { \n            params:{  \n                K_topic: 'minions_songplay',//行为种类  \n                songId: songInfo.songID//该行为日志需要记录的信息  \n            }  \n        }).then(function (response) {  \n            if (response.data === 1){  \n                message.success('添加歌曲播放记录成功');  \n                console.log(\"日志记录成功\")  \n            } else console.log(\"日志记录错误\")  \n        }).catch(function (error) {  \n            console.log(error)  \n        })  \n    }  \n```\n\n后端接口日志产生：\n\n```java\n@RestController  \n@EnableAutoConfiguration  \npublic class ActionLogController {  \n  \n    Logger logger = Logger.getLogger(ActionLogController.class.getName());  \n    @GetMapping(\"actionLogger\")  \n    private int actionLogger(HttpServletRequest request) throws UnsupportedEncodingException {  \n        int res = 0;  \n        String k_topic = request.getParameter(\"K_topic\");  \n        String songID = request.getParameter(\"songId\");  \n        try {  \n            logger.info(\"topic:\" + k_topic + \" songID:\" + songID);  \n            res = 1;  \n        }catch (Exception e){  \n            logger.error(\"error:\" + e);  \n            e.printStackTrace();  \n            res = 0;  \n        }  \n        return res;  \n    }  \n}  \n```\n\n由于需要与Flume对接，log4j的配置如下：\n\n```java\nlog4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender  \nlog4j.appender.flume.Hostname = hadoop000  \nlog4j.appender.flume.Port = 41415  \nlog4j.appender.flume.UnsafeMode = true  \nlog4j.appender.flume.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.flume.layout.ConversionPattern= %d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n  \n```\n\n这样后端产生log4j日志则会向运行着Flume机器的41415端口发送。最终发送的日志数据格式如下：\n\n```java\n2019-05-10 16:23:40,743 [http-nio-8080-exec-6] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songplay songID:17  \n```\n\n* Flume配置方案\n\n采集页面浏览日志的Flume agent使用命令从access.log文件中读取新增加的浏览日志数据。注意需要agent的channel选择memory-channel，需要将缓存的容量（capacity，默认值为100）设置一个较大的值，否则当日志数据过多时，会出现异常。本文设计memory-channel.capacity设置为10000。此外使用kafka-sink，将日志分发到指定topic的kafka消息队列中，本文设计页面浏览日志将被分发至topic为streamingtopic的消息队列中。关键配置项如下：\n\n```java\nexec-memory-kafka.sources.exec-source.type = exec  \nexec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log  \n  \nexec-memory-kafka.channels.memory-channel.capacity = 10000  \nexec-memory-kafka.channels.memory-channel.transactionCapacity = 10000 \n  \nexec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink  \nexec-memory-kafka.sinks.kafka-sink.topic = streamingtopic  \n```\n\n页面行为大类的日志使用同一的Flume agent采集，使用avro-source监听特定端口发来的日志消息数据。为了能够将不同的行为分流，需要使用过滤器interceptors将日志数据中用于分流的标记提取出来，在交给kafka-sink分发至不同topic的消息队列中。interceptor使用正则表达式将日志数据中的标记提取出来命名为topic，在kafka-sink的配置中使用topic即可。interceptor配置如下：\n\n```java\n#定义source中的过滤器  \nagent1.sources.avro-source.interceptors=i1  \nagent1.sources.avro-source.interceptors.i1.type=regex_extractor  \nagent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) songID:(.*?)  \nagent1.sources.avro-source.interceptors.i1.serializers=s1 s2   \nagent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic  \n  \nagent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink  \nagent1.sinks.kafka-sink.topic = %{topic}  \n```\n\n### 4.2日志处理、分析与持久化模块\n\n#### 4.2.1日志处理、分析与持久化模块设计\n\n使用Spark Streaming和HBase完成日志数据的处理、分析和持久化。首先不同类型的日志需要用不同的Spark Streaming应用作业来处理，因为不同的日志分析的逻辑要求是不同的（如处理时间的要求），结构也是不同的，持久化的策略也是不同的。为了便于程序的可靠运行，减少不同类型日志之间处理逻辑的耦合情况，需要将日志处理分析的逻辑按照kafka topic进行划分。日志实时流数据可以根据日志数据结构的字段进行过滤，因为日志数据是结构化的，也是可以定制的。使用HBase进行持久化，一方面支持大量数据的存储，同时还能够动态的修改表结构，增加需要记录的字段十分方便。HBase的表根据时间、类目等主要维度进行设计。\n\n#### 4.2.2日志处理、分析与持久化模块实现\n\n* 应用初始化与流数据获取\n\n每个Spark Streaming应用作业与一个Kafka topic相对应。创建Spark Streaming处理上下文，指定实时流处理的时间间隔，并从Kafka消息队列中消费对应topic的日志数据，如下：\n\n```java\nval Array(zkQuorum, group, topics, numThreads) = args  \nval sparkConf = new SparkConf().setAppName(\"MyStreamingApp\").setMaster(\"local[5]\")  \nval ssc = new StreamingContext(sparkConf, Seconds(60))  \nval topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap  \nval messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)  \n```\n\n* 日志数据模板化与清洗\n\n以上就根据日志分析的需求得到了实时的日志流数据，接下来就是对这些日志数据进行必要的过滤。首先对于浏览日志需要将所有不成功的请求清洗掉，保留成功的请求日志。由于浏览日志的数据结构是定制化的，所以我们可以根据特定字段进行清洗，本文设计首先要将每一条数据转换成Scala的模板类对象，后续对每一条数据的操作就是针对这个模板类对象完成的。其次还要将请求状态不成功的日志数据过滤掉。方法如下：\n\n```java\nval logs = messages.map(_._2)  \nval cleanData = logs.map(line => {  \n  val infos = line.split(\"\\t\")//把每行日志根据\\t分隔符分割  \n  val url = infos(2).split(\" \")(1)  \n  // url为/music/128.html  \n  var itemID = 0  \n  if (url.startsWith(\"/music\")){//music开头的把编号拿出来  \n    val itemIdHTML = url.split(\"/\")(2)  \n    itemID = itemIdHTML.substring(0, itemIdHTML.lastIndexOf(\".\")).toInt  \n  }  \n  //将每一条日志数据转换成Scala中的模板类，并将请求状态为200的日志数据保留  \n  ClickLog(infos(0), DateUtils.parseToMinute(infos(1)), itemID, infos(3).toInt, infos(4))  \n}).filter( clicklog => clicklog.itemId != 0 && clicklog.statusCode == 200)\n```\n\n最终得到的结构化数据为：\n\n```java\nClickLog(87.30.187.55,20190219090601,145,200,https://www.sogou.com/web?query=今日专辑)  \n```\n\n从左到右分别表示请求源地址、时间、请求页面编号、请求状态、来源网站。\n\n* 实时统计访问量\n\n进行业务功能开发时需要兼顾到持久化和后续可视化的可行性。对于访问量的统计需要考虑维度的选择，因为本文设计需要以天为维度和以类目维度记录访问量，即今日到当前时间为止的各类目的访问量和所有页面的访问总量，所以HBase中关于访问量的表中rowkey设计要以日期和类目编号为维度，本文设计访问量表中的rowkey形如20190511_146，列族记录到当前时间为止的访问量。方法如下：\n\n```java\ncleanData.map(x => {  \n      (x.time.substring(0,8)+\"_\"+x.itemId, 1)//记录日期与类目编号  \n    }).reduceByKey(_+_).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[itemClickCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(itemClickCount(pair._1, pair._2))  \n        })  \n        //将实时统计得到的类目访问量写入HBase  \n        itemClickCountDAO.save(list)  \n      })  \n    })  \n```\n\nHBase中行数据的更新操作使用incrementColumnValue方法完成，这样可以直接将当前时间段得到的访问总量直接与之前统计的访问总量相加，方法如下：\n\n```java\ndef save(list: ListBuffer[itemClickCount]):Unit = {  \n    //单例模式获取对表操作的实例  \n    val table = HBaseUtils.getInstance().getTable(tableName)  \n    for (ele <- list){//每一行的统计信息进行写入  \n     table.incrementColumnValue(Bytes.toBytes(ele.day_item),  \n        Bytes.toBytes(cf),  \n        Bytes.toBytes(qualifer),  \n        ele.click_count)  \n    }  \n  }  \n```\n\n以上就完成了在日期和类目编号维度下的实时访问量统计，HBase中记录的数据结构如下：\n\n```java\n20190511_130              column=info:click_count, timestamp=1557551220479, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00\\x0B\\xDF \n20190511_131              column=info:click_count, timestamp=1557551220479, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x0D          \n20190511_145              column=info:click_count, timestamp=1557551220483, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x00-\\xFD\n20190511_146              column=info:click_count, timestamp=1557551220481, \nvalue=\\x00\\x00\\x00\\x00\\x00\\x009\\x9C  \n```\n\n* 实时统计窗口时间内访问量\n\n以上完成了实时统计访问量的需求，但是使用者还需要从更细化的维度观察使用数据。对于访问量，还需要在一天内以24小时为维度进行统计，也就是展示访问量在一天之内的趋势。可以预测网站在一天内的不同时间访问量是不同的，在某个时段可能有高峰，某个时段可能是低谷，这样网站是运营人员就可以选择在不同的时间完成不同是任务，比如在高峰时加大某项业务的投放力度，在访问比较少时完成新版本的发布等。本文设计使用Spark Streaming的window操作接口，分析窗口时间内所有流数据，完成每30分钟窗口期内类目访问量的统计。window的时间间隔和滑动时间均可自定义。实现方法如下：\n\n```java\ncleanData.map(x => {  \n      (x.time.substring(0,8)+\"_\"+x.courseId, 1)  \n    }).reduceByKeyAndWindow((x: Int, y: Int) => x + y,  \n      Seconds(1800), Seconds(1800)).foreachRDD(rdd => {//设置窗口大小为1800秒（30分钟）、滑动时间间隔为30分钟  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[ClickCountTrend]  \n        val time = FastDateFormat.getInstance(\"HHmm\").format(new Date())//窗口时间戳  \n        partitionRecords.foreach(pair => {  \n          list.append(ClickCountTrend(  \n            pair._1.substring(0,8)+time+pair._1.substring(8), \n            pair._2))  \n        })  \n        ClickCountTrendDAO.save(list)  \n      })  \n    })  \n```\n\n以上就实现了每30分钟统计一次各类目在前30分钟内的访问量，对这个数据的积分即是当前时间的总访问量。HBase中持久化的rowkey结构为201905111337_146。这样就能够详细的观察访问量在一天中趋势，以及每个类目访问的趋势。但是目前这个实现方案有一个问题就是，当30分钟窗口时间跨越0点时，会丢失前一天的数据。比如，窗口期为5月10日23：45至11日0：15，由于rowkey设计的原因前15分钟的数据会计算到10日0：15的数据上。为了解决这个问题可以让应用任务在整点时启动。但是解决方案还是需要寻找的。\n\n访问量趋势的可视化展示参见可视化实现部分。\n\n* 来源网站实时统计\n\n针对从搜索引擎来的流量，需要统计来源网站和搜索关键词，这样网站可以进行SEO（搜索引擎优化），提高网站竞争力。对于来源网站的统计依然需要从清洗处理过后的模板数据中完成，这个维度的表中的rowkey以日期、来源网站和类目编号为维度。对于搜索关键词的统计也是以日期为维度，统计当前日期的热门搜索词。\n\n* 页面行为日志分析\n\n对于页面上各种不同的行为日志数据，为了便于后续的处理分析，都需要先将数据格式化为模板类对象。之后再进行日志数据的分析和持久化。以音乐播放为例，模板化方法如下：\n\n```java\n  //messages为kafka产生的消息数据  \n  val logs = messages.map(_._2)  \n  val songPlayLog = logs.map(line => {  \n  \tval infos = line.split(\" \")  \n  \tval time = DateUtils.parseToMinute(infos(0)+\" \"+infos(1))  \n  \tval UID = infos(1).split(\":\")(1).dropRight(1)  \n  \tval songID = infos.last.split(\":\")(1).dropRight(1)  \n  \tSongPlayLog(time, UID, songID) //最终音乐播放行为模板类结构SongPlayLog(20190401125947, 00001, 15)  \n})  \n```\n\n对于不同行为日志数据的分析需要根据实际业务需求进行个性化设计。同时还要注意使用实时流处理时要避免复杂的连接操作，实时流处理可以选择记录维度较简单、在原始数据上进行简单抽象的数据结果，后续复杂的分析和连接操作可以放在对实时性要求不高的流程中完成。一下以音乐播放行为为例，进行两个维度的数据分析与持久化。首先统计至当前时间的音乐播放量，方法如下：\n\n```java\nsongPlayLog.map(x => {  \n    \t  //rowkey设计为日期与歌曲编号维度  \n      (x.time.substring(0,8) + \"_\" + x.songID, 1)  \n    }).reduceByKey(_+_).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[SongPlayDailyCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(SongPlayDailyCount(pair._1, pair._2))  \n        })  \n        SongPlayDailyCountDAO.save(list)  \n      })  \n    })  \n```\n\n同样地对于音乐播放是会有实时热点产生，所以需要实时统计窗口时间内的歌曲播放数据，并在较短时间内更新，以响应热点。首先需要进行HBase表设计，本文设计对于时间段内歌曲播放这类热点数据只保留最新时间的分析结果即可。其次，本文设计针对近一个小时内的歌曲播放进行统计，滑动间隔为10分钟，即每十分钟更新一次近一个小时的歌曲播放数据。在进行HBase表创建时需要设置表中数据的存活时间TTL，本文设计设置为600s（10分钟），即表中数据产生十分钟后失效，最新的数据计算后会被写入表中。分析数据方法如下：\n\n```java\nsongPlayLog.map(x => {  \n      (x.time.substring(0,8) + \"_\" + x.songID, 1)  \n    }).reduceByKeyAndWindow((x: Int, y: Int) => x + y,  \n      Seconds(3600), Seconds(600)).foreachRDD(rdd => {  \n      rdd.foreachPartition(partitionRecords => {  \n        val list = new ListBuffer[RecentlySongPlayCount]  \n        partitionRecords.foreach(pair => {  \n          list.append(RecentlySongPlayCount(pair._1,  \n            pair._2))  \n        })  \n        RecentlyPlaySongDAO.save(list)  \n      })  \n    })  \n```\n\n以上即得到了实时的歌曲播放统计数据，以及时间段内热点歌曲的播放统计。其他的行为日志的分析需要根据实际需求来实现，本文设计还有歌曲收藏和歌曲评论两个行为日志分析，分别从其他不同的角度反映网站内容数据。\n\n#### 4.2.3 Spark Streaming应用提交集群机器执行\n\n将编写好的Spark Streaming作业程序编写好后使用mvn编译，将编译好的jar包拷贝至集群的机器上，在使用spark-submit指令进行任务的提交，指令如下：\n\n```java\nspark-submit --master local[5] \\  \n--jars $(echo /home/hadoop/app/hbase-1.2.0-cdh5.7.0/lib/*.jar | tr ' ' ',') \\ \n--class com.chaoyue.spark.project.scala.MyStreamingApp \\  \n--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \\  \n/home/hadoop/lib/sparktrain-1.0.jar \\  \nhadoop000:2181 test streamingtopic 1  \n```\n\n此时Spark Streaming作业即运行在集群上了，可以再控制台查看运行日志如下：\n\n```java\n19/05/12 16:33:00 INFO ShuffledDStream: Time 1557649980000 ms is invalid as zeroTime is 1557649260000 ms , slideDuration is 1800000 ms and difference is 720000 ms  \n19/05/12 16:33:00 INFO JobScheduler: Starting job streaming job 1557649980000 ms.0 from job set of time 1557649980000 ms  \n19/05/12 16:33:00 INFO SparkContext: Starting job: foreachPartition at MyStreamingApp.scala:59  \n19/05/12 16:33:00 INFO JobScheduler: Added jobs for time 1557649980000 ms  \n19/05/12 16:33:00 INFO DAGScheduler: Registering RDD 115 (map at MyStreamingApp.scala:56)  \n19/05/12 16:33:00 INFO MemoryStore: Block broadcast_43 stored   \n19/05/12 16:33:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 192.168.1.108:42189 (size: 1869.0 B, free: 366.1 MB)  \n...  \n19/05/12 16:33:05 INFO BlockGenerator: Pushed block input-0-1557649985600  \n19/05/12 16:33:08 INFO MemoryStore: Block input-0-1557649988600 stored as bytes in memory (estimated size 305.0 B, free 365.9 MB)  \n19/05/12 16:33:08 INFO BlockManagerInfo: Added input-0-1557649988600 in memory on 192.168.1.108:42189 (size: 305.0 B, free: 366.0 MB)  \n19/05/12 16:33:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.  \n19/05/12 16:33:08 WARN BlockManager: Block input-0-1557649988600 replicated to only 0 peer(s) instead of 1 peers  \n19/05/12 16:33:08 INFO BlockGenerator: Pushed block input-0-1557649988600  \n```\n\n### 4.3分析结果可视化模块\n\n#### 4.3.1分析结果可视化模块设计\n\n可视化部分采用前后端分离的开发模式，最终通过web网页的方式展示可视化结果。可视化模块不只是数据库中数据的展示，还要完成在实时流处理场景中不适合实时处理的其他维度的操作，即完成上个模块没有完成的复杂维度的分析。例如实时流处理中记录的歌曲播放信息只包括时间戳、用户ID和歌曲ID，但是歌曲本身的信息是丰富的，要展示的维度也是丰富的，所以在可视化模块进行复杂的连接操作，完成最终更丰富的分析结果展示。\n\n前端使用react为基础框架，react组件的思想非常适合复用，对于不同数据进行相同类型的展示时可以更换数据源复用组件。此外使用蚂蚁金服的开源react UI组件库ant-design和ant-design Pro，以及商业场景下的数据可视化解决方案Bizcharts，完成丰富的数据可视化展示。后端使用SpringBoot透出相关接口，给前端组件提供需要展示的数据，此外后端还负责实时流处理过程中没有完成的连接操作和其他处理数据的操作。\n\n#### 4.3.2分析结果可视化模块实现\n\n* 可视化web界面概览\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368878922/0)\n\n如4.1所示，今日数据概览页面展示当日实时数据，包括：实时PageView、七日PV走势、今日PV走势，各类目实时访问统计、各类目访问趋势、今日来源网站统计和今日热搜词云。页面根据维度逐渐细化来布局，从上到下从左到右，首先是PV的总量，然后从一天中的时间为维度和类目为维度细化PV值，再到其他杂项指标。今日PV走势组件展示了网站访问在一天内的趋势，每30分钟统计30分钟内访问量，可以看出音乐网站的用户群体在中午和傍晚到晚上十分活跃。类目实时访问统计展示了用户对哪一个子类目更偏爱，哪些类目需要优化。七日PV比较给一周内的PV比较，可以大致看到某一天的网站浏览状况，需要历史日期更细化的数据可以到历史数据查询里查看。类目访问趋势展示了各类目的今日访问趋势，是前面两个维度的横向和纵向细化，帮助网站运营人员从细节查看网站数据。杂项数据包括来源网站统计和热搜词云，这些展示的是从搜索引擎过来的流量的分析，根据搜索引擎的热搜词，可以帮助网站进行SEO（搜索引擎优化），提高网站排名。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368954251/0)\n\n如图4.2所示，历史数据查询提供选择日期的组件，可以查看所选日期当天的相关数据。提供30天PV比较，可以看到更长时间区间内网站浏览量的变化，通过这些变化的观察可以对网站的运营策略进行适当的调整。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369327550/0)\n\n图4.3是音乐网站行为的模拟，具有典型的歌曲资源位组件，可以模拟的行为有歌曲播放、收藏和评论。点击按钮则会调用后端相关接口，后端通过log4j记录下行为日志数据。一个歌曲资源包括歌名、歌手、专辑、封面地址、音乐资源地址、风格和标签等，使用MySQL保存这些资源信息。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369422528/0)\n\n图4.4是页面行为日志数据的实时处理分析展示页面。展示的内容包括，今日歌曲播放一览、近一小时最热播放歌曲、播放歌曲类型统计、今日加权热门歌曲、今日歌曲评论和收藏统计。今日歌曲播放一览展示今日实时的歌曲播放量统计。近一小时最热播放歌曲展示一小时窗口时间的歌曲播放量排行，是前一个统计维度的子集。歌曲播放类型统计使用南丁格尔玫瑰花环展示实时歌曲播放不同类型数量的统计，这个维度即是对实时流处理中得到的分析结果进行更高一级的抽象，在可视化的后端将歌曲播放量与歌曲资源进行连接操作得到歌曲类型统计。今日加权热门歌曲通过将三中行为的统计量进行加权，得到热门歌曲，加权权值为播放0.5：收藏1：评论0.8。另外两个展示的是实时歌曲收藏量与评论量。\n\n* 可视化部分详细展示\n\n下面将上节涉及的主要功能组件进行展示。\n\n* 页面实时浏览量\n        \n实时展示网站今日总浏览量，周同比展示今日与前一周当日的浏览量百分比，日环比展示与前一日的浏览量百分比，日均展示一周七天的浏览量均值。如图4.5所示。\n        \n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369535329/0)\n\n* 七日浏览量走势\n\n展示七天前至当日的浏览量走势，如图4.6所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369649506/0)\n\n* 今日实时浏览量走势\n\n以半小时为维度展示今日浏览量数据，每隔半小时统计半小时窗口期中的浏览量，能够清晰展示浏览量在一天中的变化。如图4.7所示，可以看出在凌晨时分访问量逐渐减少，到了早上访问量开始逐渐上升。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369685204/0)\n\n* 各类目实时浏览量统计\n\n以网站不同类目页面为维度进行浏览量数据以及占比的实时展示，音乐网站下设6个大类目：瞩目艺人、最近播放、今日专辑、为你推荐、今日歌单和新近发布。如图4.8所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369719484/0)\n\n* 今日各类目访问趋势\n\n以时间与类目为维度进行浏览量数据的展示，是前两个数据展示组件的细化。如图4.9所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369747929/0)\n\n* 来源网站实时统计\n\n展示今日来源网站的数据统计，如从百度搜索进入主站则将百度搜索记录下来。如图4.10所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369788944/0)\n\n* 今日热搜词云\n\n从搜索引擎进入主站时所进行的搜索词数据统计，如在搜索引擎中搜索“欧美音乐”进入主站，则对“欧美音乐”这个词条进行一次记录，用于搜索引擎优化。如图4.11所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369815950/0)\n\n* 月度浏览量比较\n\n以月为维度对浏览量数据进行展示比较。如图4.12所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369855984/0)\n\n* 页面行为模拟模块\n\n音乐网站页面用户行为的模拟组件，可模拟音乐播放、收藏、评论三种行为，在用户进行页面行为时则会进行日志的记录。如图4.13所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369965341/0)\n\n* 今日歌曲播放一览\n\n今日实时歌曲播放量实时数据展示，如图4.14所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369987974/0)\n\n* 近一小时播放Top歌曲\n\n近一小时窗口时间内歌曲播放量展示，每十分钟进行一小时窗口时间播放量的统计，展示当下实时热播歌曲。如图4.15所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370044063/0)\n\n* 播放音乐类型统计\n\n以音乐风格为维度进行歌曲播放量数据实时展示，如图4.16所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370078824/0)\n\n* 多权重今日热门歌曲\n\n从播放、收藏和评论三个维度进行不同权重的加权计算，得出今日热门歌曲进行展示，可以设计更复杂的加权算法得到热门歌曲，便于运营人员推送歌曲内容。如图4.17所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370164164/0)\n\n* 今日歌曲收藏一览\n\n今日实时歌曲收藏量数据展示，如图4.18所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1359855289_1559370195372/0)\n\n* 今日歌曲评论一览\n\n今日实时歌曲评论量数据展示，如图4.19所示。\n\n![](https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370221681/0)\n\n### 4.4 本章小结\n\n本章在系统需求分析与架构设计的基础上，分模块进行了详细的设计与实现，给出了每个部分的核心代码与解释，并在可视化模块介绍了各个分析数据的展现形式以及其中的意义。\n\n## 5总结与展望\n\n### 5.1 总结\n\n人类可以基于各种信息进行判断、决策，最终创造出价值，而数据作为信息的载体，正在被大量产生。然而数据的形式是多样的，而且是分散的，如何将分布的数据聚合并分析提取出有价值的信息，是大数据时代的挑战。海量数据分析作为互联网行业的闭环模式中的一环，其价值和重要性已经被广泛关注，越来越多的企业可是设立数据部门。一个网站的各个环节都会产生日志数据，这些数据形式多样且分散在不同机器实体上，如何高效采集、聚合、处理、分析和持久化给大数据分析带来了挑战。而现如今，更多的大数据分析需求还要加上“实时处理”这个需求。使用Spark作为分布式数据处理工具，整合海量日志采集工具Flume、可靠消息队列Kafka和海量数据存储HBase，满足了网站日志实时流处理的需求。Flume可定制化的从多种数据源分部式地采集日志，同时具备一定的数据处理能力，可以向多种下游数据目的地传输日志数据。消息队列Kafka保证了海量的日志数据能够有序、不丢失的被消费，同时运用topic机制使得在上游聚合的日志数据以不同的策略被处理和分析。Spark Streaming是整个系统的核心，其是在Spark Core之上的高级API，为整个数据处理流程提供了实时流处理的上下文。Spark的计算模型核心是弹性分布式数据集RDD，提供个基于内存的集群计算的容错性抽象。Spark Streaming的实时流处理能力是基于DStream，其是RDD的一个在时间和空间上的高层抽象，使得我们可以像处理RDD一样处理DStream。同时Spark Streaming支持从多种数据源创建DStream。可以说，Spark Streaming提供了基础实时流处理的解决方案。HBase提供了基于列的海量数据存储，不同于关系型数据库，HBase在大数据处理分析数据持久化方面有许多优点。本文基于Spark Streaming的音乐网站实时流处理，使用上述提及的几个工具和解决方案，同时使用react和SpringBoot进行了分析数据的可视化展示。\n\n本设计完成的主要成果如下：\n\n1）使用Flume实时采集服务器上access.log中的网页浏览日志和使用前端埋点采集到的页面行为日志。解决了分布式日志采集和聚合的问题，同时对日志进行了分流以供下游Kafka使用。\n\n2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源。使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力。在实时流处理中分析出基础业务指标数据，以供上层最这些数据进行其他复杂的操作，并将这些分析结果数据持久化至HBase中。\n\n3）使用React和SpringBoot构建前后端分离项目，同时在后端结合MySQL完成数据分析上层复杂的连接操作。最终从今日数据概览、历史数据查询和页面行为分析模拟三方面进行展示。分析数据展示包括：实时网站访问量、当日实时访问量走势、周和月维度访问量比较、各类目实时访问量及走势、来源网站统计、搜索引擎热搜词统计、歌曲实时播放量、近一小时热门歌曲播放量、歌曲播放类型比较、加权热门歌曲、实时歌曲收藏和评论统计。\n\n本文系统提供的数据可以帮助网站的管理、运营和开发维护人员更好地优化网站，比如优化网站类目内容、优化搜索引擎结果以及选择最合适的时间升级网站服务等。\n\n### 5.2 展望\n\n实时流处理不仅仅是进行简单的数据分析，还有许多实时响应的场景下有更复杂的需求。比如，电商搜索推荐系统在搜索导购的各个场景都需要能够秒级召回个性化推荐数据，而这需要大量的离线和实时数据处理相结合和完成。Spark作为分布式数据处理框架，不仅提供了秒级实时流处理的高级抽象Spark Streaming，还提供了用于机器学习的MLlib、图处理的GraphX、交互式查询SparkSQL，对这些高级抽象进行整合应用可以处理更复杂场景下的需求。","slug":"graduation-design-paper","published":1,"updated":"2019-06-04T01:06:18.978Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy1diwkr002i48upomlhg5cp","content":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>随着大数据时代的到来，数据总量不断增加，同时分布式存储与计算技术也在快速发展，如何从海量、分散的数据中提取出有价值的信息，已经成为各个行业的迫切需求，对数据的实时处理和响应变得越来越重要。</p>\n<p>本文设计了基于Spark Streaming的网站日志实时流处理系统，针对通用音乐网站两种类型的日志：网页浏览日志和网页行为日志，进行了实时采集、传输、处理、分析、持久化和可视化。本文完成了以下几个功能：1）使用Flume采集日志数据，解决了分布式日志采集和聚合的问题，同时对日志进行了分流；2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源；3）使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力，并将分析结果数据持久化至HBase中；4）使用前后端分离的开发模式完成分析结果的可视化展示。系统实现了将大量分散的日志数据进行实时整合，并完成分析计算，最后从多种维度进行信息展示。</p>\n<p>关键词：实时流处理；分布式计算；Spark Streaming；Flume；前后端分离</p>\n<h2 id=\"ABSTRACT\"><a href=\"#ABSTRACT\" class=\"headerlink\" title=\"ABSTRACT\"></a>ABSTRACT</h2><p>With the advent of the era of big data, the total amount of data continues to increase, and distributed storage and computing technologies are also developing rapidly. How to extract valuable information from massive and decentralized data has become an urgent need of various industries. Real-time processing and response of data is becoming increasingly important.</p>\n<p>This paper designs a real-time stream processing of website logs system based on Spark Streaming. It performs real-time collection, transmission, processing, analysis, persistence and visualization for two types of logs: universal web browsing logs and web page behavior logs. This paper has completed the following functions: 1) Using Flume to collect log data, solving the problem of distributed log collection and aggregation, and diverting the log at the same time; 2) Using message queue Kafka to solve a large number of log data transmission problems, which providing real-time log data source to the downstream application to process and analysis data; 3) using Spark Streaming to perform real-time processing and analysis of various types of log data, achieving the capability of second-level processing and phase data processing, and persisting the analysis result data to HBase; 4)Visualization of the results of the analysis using a development model with front and rear separation. The system realizes the real-time integration of a large number of scattered log data, completes the analysis and calculation, and finally displays information from various dimensions.</p>\n<p>KEYWORDS: Real-time stream processing; Distributed Computing; Spark Streaming; Flume; Front and rear separation</p>\n<h2 id=\"1绪论\"><a href=\"#1绪论\" class=\"headerlink\" title=\"1绪论\"></a>1绪论</h2><h3 id=\"1-1研究背景与意义\"><a href=\"#1-1研究背景与意义\" class=\"headerlink\" title=\"1.1研究背景与意义\"></a>1.1研究背景与意义</h3><p>随着DT（Data Technology）时代的到来，数据总量不断。预计到2020年，全球数据总量将超过44ZB，同时根据图灵奖获得者杰姆·格雷提出的“新摩尔定律”，每18个月，全球新增数据总量是自计算机诞生以来所有数据量之和。数据作为一种新的能源，被各行各业广为利用，为企业生产经营提供有价值的信息。同时，“爆炸式”增长的数据也对现有的数据采集、传输、分析和存储技术提出了挑战。</p>\n<p>互联网行业可以从多种多样的数据源采集信息，例如，服务器日志、网关日志、数据库日志和业务信息等。这些数据通过企业商业智能平台的挖掘分析，为企业内部以及各样相关者提供有价值的信息，例如，电商行业对于用户画像进行分析，进行精准营销和“千人千面”的召回数据，提高转化率；金融行业对于风险监控的要求较高，通过大数据分析，将提高企业的风控效率和准确率。而浏览器的页面型产品/服务的日志在众多数据源十分突出，一是其自然地提供了页面浏览（展现）的详细情况，以及具有定制化采集页面交互日志的能力，所以企业能够在这些日志数据中提取到大量可用有用的信息；二是部分日志信息具有很高的时效性，这是指采集到的日志数据需要实时分析、处理，并召回实时响应数据，为用户提供更好的体验。数据的离线和实时分析都是互联网行业“闭环”的重要部分。</p>\n<p>各大公司以及开源社区都提出了各种可用的计算模型和解决方案，为数据采集、传输、存储和分析提供了多种有用的工具。而目前最为火爆的工具之一就是Spark，它在Spark Core的基础上提供了多种高级实现，其中Spark Streaming使数据的实时处理成为可能。</p>\n<p>本文以一个通用音乐网站为背景，基于Spark Streaming对网站浏览日志和交互行为日志进行实时流处理，完成了从数据采集、传输、清洗、处理、持久化以及可视化的全流程。系统实现了网站服务器浏览日志的采集，以及通过前端埋点的方式采集可定制化的数据信息，使用Flume和Kafka向下游分流和传输数据。使用Spark Streaming清洗和处理数据，并将相关业务指标数据持久化到HBase中。可视化使用Springboot和React进行前后端分离的实现。</p>\n<p>本系统提供的数据有：</p>\n<ul>\n<li>页面浏览数据，包括Page View和网站各类目访问量，访问趋势等多维度的指标可视化数据，帮助相关人员进行网站的优化以及搜索引擎优化。</li>\n<li>页面交互信息数据，包括音乐网站情景下多种用户操作行为的记录与分析，并实时展示多维度的分析结果。</li>\n</ul>\n<h3 id=\"1-2研究现状\"><a href=\"#1-2研究现状\" class=\"headerlink\" title=\"1.2研究现状\"></a>1.2研究现状</h3><p>日志分析是在海量离散的日志数据中提取出有价值的，符合人们认知的信息。网站日志分析则是在页面浏览、页面行为等日志中得到多种维度的数据，供网站不同职能人员使用。例如，运营人员根据网站浏览量、各类目的访问情况等制定相应的运营方案；高层团队领导根据网站各业务转化率等指标灵活制定网站产品发展战略；开发人员根据网站服务性能数据优化产品性能，保障产品质量。如今已有许多商业上可行的方案，比如百度统计、神策数据等，但是它们要么关注仅仅关注访问量的统计，要么关注用户页面行为的统计。</p>\n<p>日志分析在国内外发展都十分迅速。最开始由于网站用户访问量不大，业务量规模也相对较小，整个网站的各个功能模块的日志都由专员进行分析。一般情况是远程登录各个节点来分析日志。随着业务量的不断增大，服务器以及功能模块的不断增多，日志分散在大量机器实体上，这给日志数据的采集、传输、分析和存储带来了挑战。</p>\n<p>Google用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。但是随着业务场景的复杂化，如互联网金融中的实时风控、电商中的个性化推荐数据召回等的出现，运用Hadoop MapReduce完成的离线数据分析已经不能满足企业发展的需求。Spark于2009年诞生于UC Berkeley的AMP实验室，并与2013年贡献至Apache社区，成为近年来最火热的大数据开源项目。</p>\n<p>Spark是基于内存的并行计算框架，也是基于MapReduce计算模型的分布式计算框架，其解决了Hadoop MapReduce的诸多问题，例如不能重用在迭代式机器学习和图计算中经常需要的中间结果，Hadoop MapReduce需要将这些中间数据进行复制、硬盘I/O和序列化。而Spark实现了弹性分布式数据集（RDD），一种基于内存的集群计算容错性抽象，能够很好地适应各种业务场景。Spark Streaming是在Spark Core之上的高级API。Spark Streaming提供了表示连续数据流的、高度抽象的被称为离散流的Dstream，可以通过多种数据源创建Dstream，为下游实时流处理使用。所以Spark Streaming为当前实时流处理的场景需求提供了可靠的上下文。</p>\n<p>此外，使用Flume与Kafka为海量日志的采集和传输提供了解决方案。Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，通过Flume可以将分散在各个服务器上的日志收集起来，以供下游日志数据处理分析的使用。Kafka是一种高吞吐量的分布式发布订阅消息系统，在大型网络中运用其对网站的服务性能进行扩展，能够保证所有服务请求不丢失，同时Kafka作为大型网站的基础设施，也为实时日志流处理提供消息队列的服务。</p>\n<p>目前对于实时流处理有许多解决方案，日志采集、传输、分析和存储的技术也有许多种选择。不同的解决方案/工具是由特定场景下提出的，寻找最合适的工具完成系统设计十分关键。</p>\n<h3 id=\"1-3论文结构\"><a href=\"#1-3论文结构\" class=\"headerlink\" title=\"1.3论文结构\"></a>1.3论文结构</h3><p>本文将全文内容划分为五个章节进行展开，其组织结构如下：</p>\n<p>第一章：绪论部分。简要介绍了网站日志实时流处理的研究背景和意义，并对现有大数据分析技术进行了简单的介绍，同时描述了本文设计的主要工作。</p>\n<p>第二章：系统相关理论和技术介绍。简要介绍了数据处理和日志分析的出发点和要点，针对日志采集、传输、处理、分析的难点进行介绍，并提供了现有可靠的解决方案或工具。</p>\n<p>第三章：系统需求分析与架构设计。基于本文设计的系统出了分模块的需求，并整合最佳的解决方案，设计了系统整体架构。</p>\n<p>第四章：从日志数据产生与采集、日志数据传输、处理、分析与持久化以及分析结果可视化三个方面进行了详细设计与实现。</p>\n<p>第五章：对本文设计的系统进行总结，并提出接下来要进一步完善的需求。</p>\n<h2 id=\"2相关理论与技术\"><a href=\"#2相关理论与技术\" class=\"headerlink\" title=\"2相关理论与技术\"></a>2相关理论与技术</h2><h3 id=\"2-1互联网中的闭环理论\"><a href=\"#2-1互联网中的闭环理论\" class=\"headerlink\" title=\"2.1互联网中的闭环理论\"></a>2.1互联网中的闭环理论</h3><h4 id=\"2-1-1闭环理论\"><a href=\"#2-1-1闭环理论\" class=\"headerlink\" title=\"2.1.1闭环理论\"></a>2.1.1闭环理论</h4><p>闭环理论是源于系统论和控制论的一种管理思想，其核心是通过有效的管理和控制手段，构成一个连续封闭的管理回路，使得系统中的每一个环节都能有效的衔接。最终达到提升管理水平，实现既定目标的目的。闭环系统的典型特征有：可控性强、首尾相连、循环往复。闭环式的组织能够高效地利用资源，灵活应对外界变化，达成组织目标，并在这个过程中不断自我提升。</p>\n<p>闭环理论强调“可控性”，在组织的运行流程中，尽可能的收集信息，并与原计划进行匹配，出现偏差则进行调整和修正。从而实现系统的高效演进与自我提升。</p>\n<h4 id=\"2-1-2互联网闭环\"><a href=\"#2-1-2互联网闭环\" class=\"headerlink\" title=\"2.1.2互联网闭环\"></a>2.1.2互联网闭环</h4><p>闭环理论最初由休哈特于1930年提出，现在已经衍生运用于各行各业。对于计算机互联网这样的强调系统和控制的行业，闭环理论运用十分广泛。在互联网行业中运用闭环理论的系统通常包括：验证、反馈、分析和控制这几个部分。直观的运用实例如下：</p>\n<ul>\n<li>产品开发、运营优化。如产品新特性发布前的灰度测试、电商及内容类产品的实时推荐系统等。</li>\n<li>互联网服务。如O2O线上到线下等各类分销平台、金融风控系统等。</li>\n</ul>\n<p>为了达到产品的不断提升，并对外提供更好地服务，互联网行业必须将整个系统的各部分形成完整闭环，而数据挖掘与分析是补完闭环的关键环节。网站日志实时分析作为网站系统闭环的一部分，通过分析来自用户的真实访问情况和行为情况反馈，为运营和决策人员提供有价值的信息，用以不断完善产品质量。此外通过收集数据为产品的其他功能，如推荐投放系统提供一手数据资源，完善相关功能，使得反馈结果实时触达用户。整个系统各个部分都为系统的提升做出贡献，并使之成为一个可提升的完整闭环。</p>\n<h3 id=\"2-2网站日志采集\"><a href=\"#2-2网站日志采集\" class=\"headerlink\" title=\"2.2网站日志采集\"></a>2.2网站日志采集</h3><p>浏览器的页面型产品/服务的日志采集可以分为如下两大类。</p>\n<h4 id=\"2-2-1页面浏览（展现）日志采集\"><a href=\"#2-2-1页面浏览（展现）日志采集\" class=\"headerlink\" title=\"2.2.1页面浏览（展现）日志采集\"></a>2.2.1页面浏览（展现）日志采集</h4><p>页面浏览日志是指当一个页面被浏览器加载呈现时多采集的日志。此类日志是最基础的互联网日志，也是目前所有互联网产品的两大基础指标：页面浏览量（Page View，PV）和访客数（Unique Visitors，UV）的统计基础。页面浏览日志是目前成熟度最高和完备度最高，同时也是最具挑战性的日志采集任务。</p>\n<p>页面浏览日志通常是记录在应用服务器上，如Nginx的access.log。</p>\n<h4 id=\"2-2-2页面交互日志采集\"><a href=\"#2-2-2页面交互日志采集\" class=\"headerlink\" title=\"2.2.2页面交互日志采集\"></a>2.2.2页面交互日志采集</h4><p>当页面加载和渲染完成之后，用户可以在页面上执行各种操作，随着前端技术的发展用户可以与页面上的元素完成很多互动操作，如启动、点击、拖拽和曝光等，这些行为信息价值巨大，设计者通常会要求采集用户的互动行为日志，以便通过量化获知用户的兴趣点或者优化体验。而行为数据属于低价值密度数据，其数量十分巨大，需要强大的数据处理能力和分析能力才能有效利用。</p>\n<p>行为日志的记录有多种方式，网页应用常用前端埋点的方式进行采集。</p>\n<h3 id=\"2-3-ETL过程与Flume\"><a href=\"#2-3-ETL过程与Flume\" class=\"headerlink\" title=\"2.3 ETL过程与Flume\"></a>2.3 ETL过程与Flume</h3><h4 id=\"2-3-1-ETL\"><a href=\"#2-3-1-ETL\" class=\"headerlink\" title=\"2.3.1 ETL\"></a>2.3.1 ETL</h4><p>ETL（Extract-Transform-Load）是一种数据仓库技术，用来描述将数据从数据源抽取（extract）、转换（transform）和加载（load）的过程。这一过程常用于数据仓库技术，但是在目前大数据应用中也十分重要。具体到日志分析类应用，由于服务器众多，同一类型的日志分散在不同的机器实体上。同时不同类型的日志有不同的结构，为了便于分析通常情况下需要同一结构。此外对于各种行为日志需要又不同的下游数据分析应用来处理，这就涉及到一定程度上的分流。因此在网站日志实时流处理中需要有一个ETL过程来进行分散数据的采集和一定程度的过滤清洗，使得下游应用能够高效运作。</p>\n<p>目前，Apache Flume十分适合这类ETL过程的业务场景。</p>\n<h4 id=\"2-3-2-Flume\"><a href=\"#2-3-2-Flume\" class=\"headerlink\" title=\"2.3.2 Flume\"></a>2.3.2 Flume</h4><p>Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，其能够将不同源的日志数据汇聚到统一的数据存储中；Flume具有基于流式数据的简单且灵活的架构，同时具有优秀的容错机制和故障恢复机制；Flume运用简洁又灵活的数据模型，使得其十分适合在线分析应用。</p>\n<p>Flume不仅可用于日志数据的聚合。由于数据源是可定制的，Flume还可以传输海量的事件数据，包括但不限于，网络流数据、社交媒体产生的数据、电子邮件数据和各种可能形式的数据。</p>\n<p>Flume处理数据的最小单位是一个Flume Event。Flume Agent是一个常驻JVM的进程，用于处理从外部源传输来的event。Agent由source、channel和sink组成，如图2.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366186537/0\" alt></p>\n<p>用户可以选择多种数据源作为source，如avro source、thrift source和exec source等。在本文设计中，对于页面浏览日志使用exec source，执行系统读取指令采集log文件中新增的日志信息。</p>\n<p>对于页面行为日志，采用avro source监听特定的网络端口采集行为信息数据。</p>\n<p>对于采集信息的转换，Flume为source中提供了interceptor（过滤器）这个功能。在本文设计中，将运用过滤器对页面行为信息根据行为类型进行分流，以便下游不同数据处理应用使用。</p>\n<p>Flume的各个单位事件数据通过sink传递给下游应用使用。本文设计中使用kafka sink传递给下游kafka生产者使用。此外，为了防止事件数据过多，使用channel起到缓冲的作用，可以通过设置channel缓冲的大小来确定其承载力。</p>\n<h3 id=\"2-4-Kafka\"><a href=\"#2-4-Kafka\" class=\"headerlink\" title=\"2.4 Kafka\"></a>2.4 Kafka</h3><p>消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋、日志处理等问题，实现高性能，高可用，可伸缩和最终一致性架构。</p>\n<p>Kafka是经常用于日志处理的消息中间件，解决大量日志传输的问题。上游日志采集模块将采集到的数据写入到Kafka队列中，Kafka消息队列负责日志数据的接受、存储和转发，下游日志分析处理应用订阅kafka队列中的日志数据。</p>\n<p>本文设计中下游日志分析应用即是Spark Streaming作业。由于日志的数量很多，不同类型日志的处理分析逻辑不同，所以引入kafka消息队列。一方面防止日志数据过多淹没下游应用，另一方面按消息的topic进行分流消费。</p>\n<h3 id=\"2-5-Hadoop与Spark\"><a href=\"#2-5-Hadoop与Spark\" class=\"headerlink\" title=\"2.5 Hadoop与Spark\"></a>2.5 Hadoop与Spark</h3><p>Google用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。</p>\n<p>Hadoop是一个分布式系统基础框架，用户可以在不了解分布式底层细节的情况下，开发分布式程序，充分利用集群的威力进行告诉运算和存储。Hadoop诞生于2005年，在社区和众多技术人员的支持下，Hadoop生态圈逐步扩大，形成了一整套用于分布式数据处理分析存储等功能组件。其中最重要的组件有，分布式文件系统HDFS、分布式计算框架MapReduce、分布式列存储数据库HBase、分布式协作服务ZooKeeper、日志收集工具Flume、分布式资源管理器Yarn等。这些组件提供了各种业务场景下数据处理需求的解决方案。其中MapReduce是一种分布式计算模型，用以进行大数据量的计算。其屏蔽了分布式计算框架的细节，将计算抽象成map和reduce两个过程，具有高可靠性、高扩展性、成本低等特点。因此MapReduce被广泛运用与大规模数据分析。</p>\n<p>但是随着更多复杂数据分析处理场景的出现，如迭代式计算和交互式数据挖掘等，MapReduce计算模型的出现了缺陷。首先，MapReduce将数据处理的过程抽象成2个阶段，map和reduce，虽然高度抽象，技术人员只需要编写两个函数即可。但是高度抽象带来的问题是表达能力有限，当出现更复杂的计算要求时，不得不将map和reduce两个过程复杂化或者增加更多的MapReduce任务。同时，MapReduce过程中的计算结果都需要进行磁盘I/O，这样当需要多个MapReduce任务写作时，导致任务之间的衔接涉及I/O开销，不能直接重用中间结果。此外，在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务。</p>\n<p>MapReduce缺少对分布式内存的应用，而Spark在借鉴了Hadoop MapReduce有点的同时，很好地解决了MapReduce所面临的问题。首先Spark实现了一个分布式内存抽象的概念–弹性分布式数据集（以下称为RDD），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDD是分布式内存的一个抽象概念，提供了高度受限的共享内存模型，这使得Spark计算任务能够在内存中重用中间计算结果。Spark的计算模式也属于MapReduce，但是不限于map和reduce操作，将数据操作抽象成对RDD的一系列粗粒度转换，如map、filter、reduce、group等，比MapReduce更加灵活。由前两个特性，Spark实现的RDD能够拥有很好的容错性能，Spark计算模型将对RDD的操作记录成lineage，即血缘关系，在真实计算任务时并不立即对数据进行转换操作。这样当出现中间计算结果缺失时，只需要根据lineage重新计算数据即可。此外，Spark是基于RDD的lineage形成的有向无环图DAG来进行任务调度的，要优于MapReduce的迭代执行机制，如图2.2所示，实线框为RDD，不同RDD之间的转换形成DAG，Spark通过这个DAG进行任务调度。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366311701/0\" alt></p>\n<p>针对流式数据处理场景，Spark在Spark Core的基础上实现了更高级的API–Spark Streaming。Spark Streaming是提供了表示连续数据流的、高度抽象的离散流（DStream）。DStream本质上是对RDD的一层封装，其中的每一个RDD都包含来自一个时间间隔的数据，如图2.3所示。DStream是一个没有边界的集合，没有大小限制，其代表了一个时空的概念。对DStream的操作，具体到每个时间段，就是空间的操作，也就是对时间间隔的对应批次数据的处理。Spark Streaming对于DStream的操作实质上就是对每个RDD应用批量操作。Spark Streaming将对流数据的处理抽象成对DStream的处理，而这种抽象是的开发人员能够像操作RDD一样操作DStream，所以说Spark Streaming提供了实时流处理的上下文。此外Spark Streaming支持从多个数据源创建DStream，如图2.4所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366347719/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366361643/0\" alt></p>\n<h3 id=\"2-6前后端分离\"><a href=\"#2-6前后端分离\" class=\"headerlink\" title=\"2.6前后端分离\"></a>2.6前后端分离</h3><p>前后端分离是指在网站开发阶段前端与后端各司其职，前端只要根据设计还原出页面的逻辑展示，后端只要处理根网页数据相关的业务逻辑，并向前端透出需要的接口即可。这种模式能够使前后端并行开发，大大提高效率，并将前后端一定程度上解耦。本文设计中网页技术部分采用前后端分离的模式，模拟用户页面行为的产生，并且进行分析数据结果的可视化。前端采用react技术栈，后端采用SpringBoot。</p>\n<p>React是Facebook公司开源的前端框架，是近年最火热的技术，国外Facebook、Instagram和国内知乎、蚂蚁金服等都在使用。React的思想是将前端所有元素组件化，将所有需要的UI控件甚至逻辑控件抽象成组件进行开发。每个组件维护自己的状态逻辑变化。React技术不断发展，社区中也不断贡献出特定场景下优秀的解决方案。</p>\n<p>SpringBoot是由Pivotal团队开发的框架，其能够简化新Spring应用的初始搭建以及开发过程。SpringBoot使用很多特定或默认的方式进行配置，简化了开发人员的准备工作时间，可以专注于真正业务的实现。SpringBoot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、产品级别的Spring应用，特别适合实现目前最火热的微服务架构。本文设计选择SpringBoot作为后端服务框架。</p>\n<h3 id=\"2-7-本章小结\"><a href=\"#2-7-本章小结\" class=\"headerlink\" title=\"2.7 本章小结\"></a>2.7 本章小结</h3><p>本章介绍了互联网中的闭环理论以及针对日志数据采集、传输、处理、分析、持久化以及可视化的可靠的开源工具，为本文设计的系统中的各个需求场景提供了最优的解决方案。</p>\n<h2 id=\"3系统需求分析与架构设计\"><a href=\"#3系统需求分析与架构设计\" class=\"headerlink\" title=\"3系统需求分析与架构设计\"></a>3系统需求分析与架构设计</h2><h3 id=\"3-1系统需求分析\"><a href=\"#3-1系统需求分析\" class=\"headerlink\" title=\"3.1系统需求分析\"></a>3.1系统需求分析</h3><h4 id=\"3-1-1系统概述\"><a href=\"#3-1-1系统概述\" class=\"headerlink\" title=\"3.1.1系统概述\"></a>3.1.1系统概述</h4><p>本文设计以一个通用音乐网站的日志为分析对象的流处理系统，完成对日志进行实时采集、传输、清洗、处理、分析、持久化和可视化，最终得到网站页面浏览和页面行为日志数据的相关数据分析指标结果等有价值的信息，展示实时流处理的能力。按照数据流处理流程可将本系统分为以下几个部分：日志数据产生与采集模块、日志数据传输模块、日志数据处理分析模块、分析结果持久化与可视化模块。数据流程如图3.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366469597/0\" alt></p>\n<p>音乐网站的日志分为两种类型，页面浏览日志和页面行为日志。对于两种类型的日志需要使用统一的日志收集工具进行收集，以便下游组件的使用；数据量庞大的日志数据需要使用消息队列进行传输，达到流量削峰的目的；从消息队列得到的日志数据进过分布式计算工具进行处理计算分析，持久化至存储设施中，最后将分析结果与网站业务相关的结构化数据进行更具体的连接操作完成分析结果的可视化展示。</p>\n<h4 id=\"3-1-2数据产生与采集需求分析\"><a href=\"#3-1-2数据产生与采集需求分析\" class=\"headerlink\" title=\"3.1.2数据产生与采集需求分析\"></a>3.1.2数据产生与采集需求分析</h4><p>两类不同的日志，页面浏览日志和页面行为日志，采用不同的方式产生。对于页面浏览日志中的每一条浏览日志需要是可配置的结构化数据，这样便于下游应用的清洗等操作,这些日志数据还需要存储在特定的位置，便于采集；对于页面行为日志，同样需要有一定的数据结构，便于在下游对不同的行为日志数据进行分流，但是页面行为日志可以有一定的定制化内容，因为不同的行为可能要采集的数据是不同的，因此行为日志的结构要保留一定的灵活性。行为日志数据的产生不能对用户体验产生影响，也就是说对于用户来说是透明的。</p>\n<p>日志数据的采集需要考虑到网站服务分布式的特点，也就是各类型日志都是分布在不同的服务器实体上，因此需要使用一个能够将分布的日志快速收集为全量日志数据的工具，同时为了与下游数据传输组件进行配合，采集工具需要具有一定的分流功能，即是能够将不同种类的日志信息先进行汇总，再向同一目的分发。</p>\n<h4 id=\"3-1-3数据传输与处理模块需求分析\"><a href=\"#3-1-3数据传输与处理模块需求分析\" class=\"headerlink\" title=\"3.1.3数据传输与处理模块需求分析\"></a>3.1.3数据传输与处理模块需求分析</h4><p>海量日志数据如果不加控制地发送至下游应用进行处理分析，会造成很多问题，最严重的就是超过应用的处理限度而不能正常服务，从而丢失数据。所以必须使用消息中间件解决大量日志数据传输的问题。消息中间件还需要有高可靠性和容错的能力。</p>\n<p>对于日志数据的处理分析模块，需要根据不同的日志类型和日志的种类进行不同的处理分析，也就是说页面浏览日志与页面行为日志分析的逻辑是不同的，对于不同行为的分析也是不同的。分析处理模块需要具有实时流处理的能力，以及一定的容错能力。同时还能分析复杂的数据场景，比如处理一定时间窗口内的所有数据。</p>\n<h4 id=\"3-1-4数据持久化与可视化模块需求分析\"><a href=\"#3-1-4数据持久化与可视化模块需求分析\" class=\"headerlink\" title=\"3.1.4数据持久化与可视化模块需求分析\"></a>3.1.4数据持久化与可视化模块需求分析</h4><p>分时结果数据的持久化需要考虑海量数据的特点，同时对于数据存储内容还要有一定的灵活性，这是指存储的分析结果内容应该能够根据业务的需求进行调整，能够动态地增加字段而不用修改表结构。</p>\n<p>可视化模块作为系统最终对外展示的部分，体现了整个系统的风格。优雅直观的界面和简洁直接的交互，不会让使用者产生疲倦感，同时有意愿继续使用和对产品提出改进意见。分析结果数据应该从各个维度进行数据的展示，维度由大到小，逐渐细化，帮助用户更好的观察数据。可视化模块是将一条条结构化的数据反映成直观形式表示的最后一步。</p>\n<p>可视化部分分为：今日数据概览、历史数据查询和网页行为分析。详细需求如下：</p>\n<ul>\n<li>今日数据概览</li>\n</ul>\n<p>展示网站各维度的今日实时数据，包括实时PageView、七日PageView走势、今日实时PageView走势、网站各类目访问量、各类目访问量走势、今日来源网站数据汇总和热搜词云。以供运营人员优化网站和优化搜索引擎结果等。</p>\n<ul>\n<li>历史数据查询</li>\n</ul>\n<p>可以选择所要查询的日期，查看所选日期的各项数据指标，同时增加以天为维度的30日访问量比较，可用于帮助网站各级别管理人员制定网站今后的内容发展方向，辅助决策。</p>\n<ul>\n<li>网页行为分析</li>\n</ul>\n<p>能够模拟音乐网站上的用户行为，如歌曲播放、收藏和评论等。从各个维度展现行为日志分析数据，如今日歌曲播放量排行，近一小时内热点歌曲排行、歌曲风格统计、热门歌曲标签云、热评歌曲和热门收藏歌曲等。各维度数据供网站运营人员优化网站歌曲内容。</p>\n<h3 id=\"3-2系统架构设计\"><a href=\"#3-2系统架构设计\" class=\"headerlink\" title=\"3.2系统架构设计\"></a>3.2系统架构设计</h3><h4 id=\"3-2-1总体架构\"><a href=\"#3-2-1总体架构\" class=\"headerlink\" title=\"3.2.1总体架构\"></a>3.2.1总体架构</h4><p>本文设计的网站日志实时流处理系统，采取分布式的架构，遵循低耦合的原则。总体架构分为四部分：日志数据产生、日志采集与传输、日志处理分析和持久化以及分析结果可视化。总体架构如图3.2所示，页面浏览日志与页面行为日志采用不同的方案产生；使用Flume与Kafka来进行日志数据的采集与传输；实时日志数据通过Spark Streaming进行处理与分析并将分析结果持久化至HBase中；最后采用前后端分离的模式，整个MySQL中的业务数据完成分析结果的可视化展示。总体架构如图3.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366982662/0\" alt></p>\n<h4 id=\"3-2-2-日志数据产生模块架构设计\"><a href=\"#3-2-2-日志数据产生模块架构设计\" class=\"headerlink\" title=\"3.2.2 日志数据产生模块架构设计\"></a>3.2.2 日志数据产生模块架构设计</h4><p>网站页面浏览日志是存储在服务器上access.log文件里面的结构化数据，并且这些浏览日志数据的结构是可配置的。本文设计使用Python脚本完成页面浏览日志的产生，并结合Linux的crontab功能完成周期性的调用。页面行为日志使用前端埋点的方式产生，在用户产生某种行为时调用js代码，向后端发出请求。后端使用SpringBoot搭建服务，通过解析请求和log4j产生行为日志数据。该模块架构如图3.3所示，产生的日志数据会被下游模块采集。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367132763/0\" alt></p>\n<h4 id=\"3-2-3-日志采集与传输模块架构设计\"><a href=\"#3-2-3-日志采集与传输模块架构设计\" class=\"headerlink\" title=\"3.2.3 日志采集与传输模块架构设计\"></a>3.2.3 日志采集与传输模块架构设计</h4><p>上一个模块已经产生了两类日志数据，本模块使用Flume和Kafka完成日志数据的采集与传输。页面浏览日志分布式的存储在网站服务器的access.log文件中，在每个服务器上需要部署一个Flume Agent完成access.log新增日志数据的采集，并将这些日志数据对接至下游Kafka生产者；页面行为日志是由服务器产生的log4j日志数据，通过配置会将这些数据发送至指定地址上的特定端口，在该地址上的服务器部署了Flume Agent，其会监听特定端口来采集日志数据。该Flume Agent会根据日志数据的内容进行分流，对接不同的Kafka生产者，使得不同的行为日志数据以不同的方法被实时处理。消息队列Kafka将Flume采集来的数据通过使用不同的topic传输至下游Spark Streaming。该模块架构如图3.4所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367178577/0\" alt></p>\n<h4 id=\"3-2-4-日志处理分析和持久化模块架构设计\"><a href=\"#3-2-4-日志处理分析和持久化模块架构设计\" class=\"headerlink\" title=\"3.2.4 日志处理分析和持久化模块架构设计\"></a>3.2.4 日志处理分析和持久化模块架构设计</h4><p>本模块使用Spark Streaming进行日志数据的处理与分析，针对不同的日志向Spark集群提交相应的Spark Streaming应用，这些应用会根据topic拉取Kafka消息队列中的消息，之后进行处理与分析，分析结果将持久化至HBase中。该模块架构如图3.5所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367225228/0\" alt></p>\n<h4 id=\"3-2-5-分析结果可视化模块架构设计\"><a href=\"#3-2-5-分析结果可视化模块架构设计\" class=\"headerlink\" title=\"3.2.5 分析结果可视化模块架构设计\"></a>3.2.5 分析结果可视化模块架构设计</h4><p>分析结果可视化模块采用react作为前端框架，并结合蚂蚁金服技术体验部的UI组件库ant design、ant design Pro以及商业场景下数据可视化解决方案Bizcharts完成数据的多维度展示。后端使用SpringBoot连接HBase，将日志分析结果透出至前端。此外由于网站具体业务的复杂性，前一阶段的分析结果是比较抽象的，还需要结合存有具体业务数据的结构化数据库MySQL来实现更细化的分析结果数据展示，本次设计音乐网站中较重要的结构化数据是歌曲信息的数据。该模块架构如图3.6所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367274523/0\" alt></p>\n<h3 id=\"3-3-本章小结\"><a href=\"#3-3-本章小结\" class=\"headerlink\" title=\"3.3 本章小结\"></a>3.3 本章小结</h3><p>本章首先对要设计的系统进行总体概括，并分模块进行了需求分析，然后给出了总体架构设计和详细的分模块架构设计。根据各模块的需求给出了可行且合适的解决方案，为下一章系统的具体实现提供了清晰的方向。</p>\n<h2 id=\"4设计与实现\"><a href=\"#4设计与实现\" class=\"headerlink\" title=\"4设计与实现\"></a>4设计与实现</h2><h3 id=\"4-1日志数据产生与采集模块\"><a href=\"#4-1日志数据产生与采集模块\" class=\"headerlink\" title=\"4.1日志数据产生与采集模块\"></a>4.1日志数据产生与采集模块</h3><h4 id=\"4-1-1日志数据产生与采集模块设计\"><a href=\"#4-1-1日志数据产生与采集模块设计\" class=\"headerlink\" title=\"4.1.1日志数据产生与采集模块设计\"></a>4.1.1日志数据产生与采集模块设计</h4><p>由于时间和成本等现实原因，本文设计不能采用真实线上的日志数据，而是采用模拟的方式产生。模拟方式产生的日志数据与真实线上日志完全相同，首先保证浏览日志的数据格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">98.29</span>.167.156 <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">14</span>:<span class=\"number\">31</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/145.html HTTP/1.1\"</span>  <span class=\"number\">200</span> http:<span class=\"comment\">//www.baidu.com/s?wd=新近发布</span></span><br></pre></td></tr></table></figure>\n<p>包括请求源地址、时间、方法、请求页面、HTTP协议、请求状态以及来源网站。所以需要设计脚本完成access.log的生成，为了防止日志文件过大，需要定时清理。对于页面行为日志，采用前端埋点的方式产生。首先根据产品设计的需求找到需要记录的用户行为，比如点击、滑动、停留、键入内容等，细化到音乐网站上，用户的行为可以是点击播放音乐、评论音乐、收藏音乐和切换音乐等。这些日志需要在产生这些行为时调用相关后端接口，并向后端传送打点的内容，由后端服务器来完成日志的产生。后端服务器通过log4j产生格式化的日志供下游采集。</p>\n<p>这两类日志的采集都使用Flume完成，但是应用不同配置的Flume agent来完成采集。首先两种agent都最重要将日志数据传送给kafka消息队列，但是应为不同的topic，此外对于不同的页面行为日志也要是不同的topic。对于浏览日志，Flume agent使用avro source读取access.log中新增的日志信息，并通过kafka sink传至下游。对于行为日志统一使用一个Flume agent进行采集，使用avro source监听特定的端口，由后端服务器配置log4j发送至指定端口。此外还需要Flume对所有因为日志按照行为的种类进行分流，使下游产生不同topic的消息。</p>\n<h4 id=\"4-1-2日志数据产生与采集模块实现\"><a href=\"#4-1-2日志数据产生与采集模块实现\" class=\"headerlink\" title=\"4.1.2日志数据产生与采集模块实现\"></a>4.1.2日志数据产生与采集模块实现</h4><ul>\n<li>页面浏览日志产生</li>\n</ul>\n<p>access.log使用python脚本产生，使用linux的crontab生成定时任务，每个一分钟产生一次access.log。关键代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">\"/home/hadoop/data/project/logs/access.log\"</span>,<span class=\"string\">\"w+\"</span>)  </span><br><span class=\"line\">    <span class=\"keyword\">while</span> count &gt;= <span class=\"number\">1</span>:   </span><br><span class=\"line\">        query_log = <span class=\"string\">\"&#123;ip&#125;\\t&#123;local_time&#125;\\t\\\"GET /&#123;url&#125; HTTP/1.1\\\"\\t&#123;status_code&#125;\\t&#123;referer&#125;\"</span>.format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)  </span><br><span class=\"line\">        f.write(query_log + <span class=\"string\">\"\\n\"</span>)  </span><br><span class=\"line\">        count = count - <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>最终access.log产生的数据结构如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">187.156</span>.143.55  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/128.html HTTP/1.1\"</span>  <span class=\"number\">404</span> -  </span><br><span class=\"line\"><span class=\"number\">87.187</span>.167.124  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/128.html HTTP/1.1\"</span>  <span class=\"number\">500</span> -  </span><br><span class=\"line\"><span class=\"number\">156.29</span>.187.168  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/112.html HTTP/1.1\"</span>  <span class=\"number\">200</span> http:<span class=\"comment\">//www.baidu.com/s?wd=新近发布</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>页面行为日志产生</li>\n</ul>\n<p>前端埋点的方式由于技术选型的不同有很多方式，本文设计使用的是前后端分离的开发模式，所以在前端要记录行为操作数据的部分编写相应的js代码，调用后端业务接口。后端服务器处理接口调用请求的同时使用log4j产生日志数据。具体实现如下：</p>\n<p>前端埋点js代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//播放歌曲记录  </span></span><br><span class=\"line\">    songplay(songInfo)&#123;  </span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123; </span><br><span class=\"line\">            params:&#123;  </span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songplay'</span>,<span class=\"comment\">//行为种类  </span></span><br><span class=\"line\">                songId: songInfo.songID<span class=\"comment\">//该行为日志需要记录的信息  </span></span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;).then(function (response) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;  </span><br><span class=\"line\">                message.success(<span class=\"string\">'添加歌曲播放记录成功'</span>);  </span><br><span class=\"line\">                console.log(<span class=\"string\">\"日志记录成功\"</span>)  </span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> console.log(<span class=\"string\">\"日志记录错误\"</span>)  </span><br><span class=\"line\">        &#125;).<span class=\"keyword\">catch</span>(function (error) &#123;  </span><br><span class=\"line\">            console.log(error)  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>后端接口日志产生：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span>  </span><br><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span>  </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ActionLogController</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Logger logger = Logger.getLogger(ActionLogController.class.getName());  </span><br><span class=\"line\">    <span class=\"meta\">@GetMapping</span>(<span class=\"string\">\"actionLogger\"</span>)  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">actionLogger</span><span class=\"params\">(HttpServletRequest request)</span> <span class=\"keyword\">throws</span> UnsupportedEncodingException </span>&#123;  </span><br><span class=\"line\">        <span class=\"keyword\">int</span> res = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        String k_topic = request.getParameter(<span class=\"string\">\"K_topic\"</span>);  </span><br><span class=\"line\">        String songID = request.getParameter(<span class=\"string\">\"songId\"</span>);  </span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;  </span><br><span class=\"line\">            logger.info(<span class=\"string\">\"topic:\"</span> + k_topic + <span class=\"string\">\" songID:\"</span> + songID);  </span><br><span class=\"line\">            res = <span class=\"number\">1</span>;  </span><br><span class=\"line\">        &#125;<span class=\"keyword\">catch</span> (Exception e)&#123;  </span><br><span class=\"line\">            logger.error(<span class=\"string\">\"error:\"</span> + e);  </span><br><span class=\"line\">            e.printStackTrace();  </span><br><span class=\"line\">            res = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> res;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>由于需要与Flume对接，log4j的配置如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender  </span><br><span class=\"line\">log4j.appender.flume.Hostname = hadoop000  </span><br><span class=\"line\">log4j.appender.flume.Port = <span class=\"number\">41415</span>  </span><br><span class=\"line\">log4j.appender.flume.UnsafeMode = <span class=\"keyword\">true</span>  </span><br><span class=\"line\">log4j.appender.flume.layout=org.apache.log4j.PatternLayout  </span><br><span class=\"line\">log4j.appender.flume.layout.ConversionPattern= %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure>\n<p>这样后端产生log4j日志则会向运行着Flume机器的41415端口发送。最终发送的日志数据格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">16</span>:<span class=\"number\">23</span>:<span class=\"number\">40</span>,<span class=\"number\">743</span> [http-nio-<span class=\"number\">8080</span>-exec-<span class=\"number\">6</span>] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songplay songID:<span class=\"number\">17</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Flume配置方案</li>\n</ul>\n<p>采集页面浏览日志的Flume agent使用命令从access.log文件中读取新增加的浏览日志数据。注意需要agent的channel选择memory-channel，需要将缓存的容量（capacity，默认值为100）设置一个较大的值，否则当日志数据过多时，会出现异常。本文设计memory-channel.capacity设置为10000。此外使用kafka-sink，将日志分发到指定topic的kafka消息队列中，本文设计页面浏览日志将被分发至topic为streamingtopic的消息队列中。关键配置项如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exec-memory-kafka.sources.exec-source.type = exec  </span><br><span class=\"line\">exec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log  </span><br><span class=\"line\">  </span><br><span class=\"line\">exec-memory-kafka.channels.memory-channel.capacity = <span class=\"number\">10000</span>  </span><br><span class=\"line\">exec-memory-kafka.channels.memory-channel.transactionCapacity = <span class=\"number\">10000</span> </span><br><span class=\"line\">  </span><br><span class=\"line\">exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class=\"line\">exec-memory-kafka.sinks.kafka-sink.topic = streamingtopic</span><br></pre></td></tr></table></figure>\n<p>页面行为大类的日志使用同一的Flume agent采集，使用avro-source监听特定端口发来的日志消息数据。为了能够将不同的行为分流，需要使用过滤器interceptors将日志数据中用于分流的标记提取出来，在交给kafka-sink分发至不同topic的消息队列中。interceptor使用正则表达式将日志数据中的标记提取出来命名为topic，在kafka-sink的配置中使用topic即可。interceptor配置如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#定义source中的过滤器  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors=i1  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.type=regex_extractor  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) songID:(.*?)  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers=s1 s2   </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic  </span><br><span class=\"line\">  </span><br><span class=\"line\">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink  </span><br><span class=\"line\">agent1.sinks.kafka-sink.topic = %&#123;topic&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2日志处理、分析与持久化模块\"><a href=\"#4-2日志处理、分析与持久化模块\" class=\"headerlink\" title=\"4.2日志处理、分析与持久化模块\"></a>4.2日志处理、分析与持久化模块</h3><h4 id=\"4-2-1日志处理、分析与持久化模块设计\"><a href=\"#4-2-1日志处理、分析与持久化模块设计\" class=\"headerlink\" title=\"4.2.1日志处理、分析与持久化模块设计\"></a>4.2.1日志处理、分析与持久化模块设计</h4><p>使用Spark Streaming和HBase完成日志数据的处理、分析和持久化。首先不同类型的日志需要用不同的Spark Streaming应用作业来处理，因为不同的日志分析的逻辑要求是不同的（如处理时间的要求），结构也是不同的，持久化的策略也是不同的。为了便于程序的可靠运行，减少不同类型日志之间处理逻辑的耦合情况，需要将日志处理分析的逻辑按照kafka topic进行划分。日志实时流数据可以根据日志数据结构的字段进行过滤，因为日志数据是结构化的，也是可以定制的。使用HBase进行持久化，一方面支持大量数据的存储，同时还能够动态的修改表结构，增加需要记录的字段十分方便。HBase的表根据时间、类目等主要维度进行设计。</p>\n<h4 id=\"4-2-2日志处理、分析与持久化模块实现\"><a href=\"#4-2-2日志处理、分析与持久化模块实现\" class=\"headerlink\" title=\"4.2.2日志处理、分析与持久化模块实现\"></a>4.2.2日志处理、分析与持久化模块实现</h4><ul>\n<li>应用初始化与流数据获取</li>\n</ul>\n<p>每个Spark Streaming应用作业与一个Kafka topic相对应。创建Spark Streaming处理上下文，指定实时流处理的时间间隔，并从Kafka消息队列中消费对应topic的日志数据，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">val <span class=\"title\">Array</span><span class=\"params\">(zkQuorum, group, topics, numThreads)</span> </span>= args  </span><br><span class=\"line\">val sparkConf = <span class=\"keyword\">new</span> SparkConf().setAppName(<span class=\"string\">\"MyStreamingApp\"</span>).setMaster(<span class=\"string\">\"local[5]\"</span>)  </span><br><span class=\"line\">val ssc = <span class=\"keyword\">new</span> StreamingContext(sparkConf, Seconds(<span class=\"number\">60</span>))  </span><br><span class=\"line\">val topicMap = topics.split(<span class=\"string\">\",\"</span>).map((_, numThreads.toInt)).toMap  </span><br><span class=\"line\">val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>日志数据模板化与清洗</li>\n</ul>\n<p>以上就根据日志分析的需求得到了实时的日志流数据，接下来就是对这些日志数据进行必要的过滤。首先对于浏览日志需要将所有不成功的请求清洗掉，保留成功的请求日志。由于浏览日志的数据结构是定制化的，所以我们可以根据特定字段进行清洗，本文设计首先要将每一条数据转换成Scala的模板类对象，后续对每一条数据的操作就是针对这个模板类对象完成的。其次还要将请求状态不成功的日志数据过滤掉。方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val logs = messages.map(_._2)  </span><br><span class=\"line\">val cleanData = logs.map(line =&gt; &#123;  </span><br><span class=\"line\">  val infos = line.split(<span class=\"string\">\"\\t\"</span>)<span class=\"comment\">//把每行日志根据\\t分隔符分割  </span></span><br><span class=\"line\">  val url = infos(<span class=\"number\">2</span>).split(<span class=\"string\">\" \"</span>)(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  <span class=\"comment\">// url为/music/128.html  </span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> itemID = <span class=\"number\">0</span>  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> (url.startsWith(<span class=\"string\">\"/music\"</span>))&#123;<span class=\"comment\">//music开头的把编号拿出来  </span></span><br><span class=\"line\">    val itemIdHTML = url.split(<span class=\"string\">\"/\"</span>)(<span class=\"number\">2</span>)  </span><br><span class=\"line\">    itemID = itemIdHTML.substring(<span class=\"number\">0</span>, itemIdHTML.lastIndexOf(<span class=\"string\">\".\"</span>)).toInt  </span><br><span class=\"line\">  &#125;  </span><br><span class=\"line\">  <span class=\"comment\">//将每一条日志数据转换成Scala中的模板类，并将请求状态为200的日志数据保留  </span></span><br><span class=\"line\">  ClickLog(infos(<span class=\"number\">0</span>), DateUtils.parseToMinute(infos(<span class=\"number\">1</span>)), itemID, infos(<span class=\"number\">3</span>).toInt, infos(<span class=\"number\">4</span>))  </span><br><span class=\"line\">&#125;).filter( clicklog =&gt; clicklog.itemId != <span class=\"number\">0</span> &amp;&amp; clicklog.statusCode == <span class=\"number\">200</span>)</span><br></pre></td></tr></table></figure>\n<p>最终得到的结构化数据为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ClickLog(<span class=\"number\">87.30</span>.187.55,<span class=\"number\">20190219090601</span>,<span class=\"number\">145</span>,<span class=\"number\">200</span>,https:<span class=\"comment\">//www.sogou.com/web?query=今日专辑)</span></span><br></pre></td></tr></table></figure>\n<p>从左到右分别表示请求源地址、时间、请求页面编号、请求状态、来源网站。</p>\n<ul>\n<li>实时统计访问量</li>\n</ul>\n<p>进行业务功能开发时需要兼顾到持久化和后续可视化的可行性。对于访问量的统计需要考虑维度的选择，因为本文设计需要以天为维度和以类目维度记录访问量，即今日到当前时间为止的各类目的访问量和所有页面的访问总量，所以HBase中关于访问量的表中rowkey设计要以日期和类目编号为维度，本文设计访问量表中的rowkey形如20190511_146，列族记录到当前时间为止的访问量。方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cleanData.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+<span class=\"string\">\"_\"</span>+x.itemId, <span class=\"number\">1</span>)<span class=\"comment\">//记录日期与类目编号  </span></span><br><span class=\"line\">    &#125;).reduceByKey(_+_).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[itemClickCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(itemClickCount(pair._1, pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        <span class=\"comment\">//将实时统计得到的类目访问量写入HBase  </span></span><br><span class=\"line\">        itemClickCountDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>HBase中行数据的更新操作使用incrementColumnValue方法完成，这样可以直接将当前时间段得到的访问总量直接与之前统计的访问总量相加，方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">def <span class=\"title\">save</span><span class=\"params\">(list: ListBuffer[itemClickCount])</span>:Unit </span>= &#123;  </span><br><span class=\"line\">    <span class=\"comment\">//单例模式获取对表操作的实例  </span></span><br><span class=\"line\">    val table = HBaseUtils.getInstance().getTable(tableName)  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ele &lt;- list)&#123;<span class=\"comment\">//每一行的统计信息进行写入  </span></span><br><span class=\"line\">     table.incrementColumnValue(Bytes.toBytes(ele.day_item),  </span><br><span class=\"line\">        Bytes.toBytes(cf),  </span><br><span class=\"line\">        Bytes.toBytes(qualifer),  </span><br><span class=\"line\">        ele.click_count)  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>以上就完成了在日期和类目编号维度下的实时访问量统计，HBase中记录的数据结构如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">20190511_130</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220479</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00\\x0B\\xDF </span><br><span class=\"line\"><span class=\"number\">20190511_131</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220479</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x0D          </span><br><span class=\"line\"><span class=\"number\">20190511_145</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220483</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00-\\xFD</span><br><span class=\"line\"><span class=\"number\">20190511_146</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220481</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x009\\x9C</span><br></pre></td></tr></table></figure>\n<ul>\n<li>实时统计窗口时间内访问量</li>\n</ul>\n<p>以上完成了实时统计访问量的需求，但是使用者还需要从更细化的维度观察使用数据。对于访问量，还需要在一天内以24小时为维度进行统计，也就是展示访问量在一天之内的趋势。可以预测网站在一天内的不同时间访问量是不同的，在某个时段可能有高峰，某个时段可能是低谷，这样网站是运营人员就可以选择在不同的时间完成不同是任务，比如在高峰时加大某项业务的投放力度，在访问比较少时完成新版本的发布等。本文设计使用Spark Streaming的window操作接口，分析窗口时间内所有流数据，完成每30分钟窗口期内类目访问量的统计。window的时间间隔和滑动时间均可自定义。实现方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cleanData.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+<span class=\"string\">\"_\"</span>+x.courseId, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKeyAndWindow((x: Int, y: Int) =&gt; x + y,  </span><br><span class=\"line\">      Seconds(<span class=\"number\">1800</span>), Seconds(<span class=\"number\">1800</span>)).foreachRDD(rdd =&gt; &#123;<span class=\"comment\">//设置窗口大小为1800秒（30分钟）、滑动时间间隔为30分钟  </span></span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[ClickCountTrend]  </span><br><span class=\"line\">        val time = FastDateFormat.getInstance(<span class=\"string\">\"HHmm\"</span>).format(<span class=\"keyword\">new</span> Date())<span class=\"comment\">//窗口时间戳  </span></span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(ClickCountTrend(  </span><br><span class=\"line\">            pair._1.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+time+pair._1.substring(<span class=\"number\">8</span>), </span><br><span class=\"line\">            pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        ClickCountTrendDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>以上就实现了每30分钟统计一次各类目在前30分钟内的访问量，对这个数据的积分即是当前时间的总访问量。HBase中持久化的rowkey结构为201905111337_146。这样就能够详细的观察访问量在一天中趋势，以及每个类目访问的趋势。但是目前这个实现方案有一个问题就是，当30分钟窗口时间跨越0点时，会丢失前一天的数据。比如，窗口期为5月10日23：45至11日0：15，由于rowkey设计的原因前15分钟的数据会计算到10日0：15的数据上。为了解决这个问题可以让应用任务在整点时启动。但是解决方案还是需要寻找的。</p>\n<p>访问量趋势的可视化展示参见可视化实现部分。</p>\n<ul>\n<li>来源网站实时统计</li>\n</ul>\n<p>针对从搜索引擎来的流量，需要统计来源网站和搜索关键词，这样网站可以进行SEO（搜索引擎优化），提高网站竞争力。对于来源网站的统计依然需要从清洗处理过后的模板数据中完成，这个维度的表中的rowkey以日期、来源网站和类目编号为维度。对于搜索关键词的统计也是以日期为维度，统计当前日期的热门搜索词。</p>\n<ul>\n<li>页面行为日志分析</li>\n</ul>\n<p>对于页面上各种不同的行为日志数据，为了便于后续的处理分析，都需要先将数据格式化为模板类对象。之后再进行日志数据的分析和持久化。以音乐播放为例，模板化方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"comment\">//messages为kafka产生的消息数据  </span></span><br><span class=\"line\">  val logs = messages.map(_._2)  </span><br><span class=\"line\">  val songPlayLog = logs.map(line =&gt; &#123;  </span><br><span class=\"line\">  \tval infos = line.split(<span class=\"string\">\" \"</span>)  </span><br><span class=\"line\">  \tval time = DateUtils.parseToMinute(infos(<span class=\"number\">0</span>)+<span class=\"string\">\" \"</span>+infos(<span class=\"number\">1</span>))  </span><br><span class=\"line\">  \tval UID = infos(<span class=\"number\">1</span>).split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).dropRight(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  \tval songID = infos.last.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).dropRight(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  \tSongPlayLog(time, UID, songID) <span class=\"comment\">//最终音乐播放行为模板类结构SongPlayLog(20190401125947, 00001, 15)  </span></span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>对于不同行为日志数据的分析需要根据实际业务需求进行个性化设计。同时还要注意使用实时流处理时要避免复杂的连接操作，实时流处理可以选择记录维度较简单、在原始数据上进行简单抽象的数据结果，后续复杂的分析和连接操作可以放在对实时性要求不高的流程中完成。一下以音乐播放行为为例，进行两个维度的数据分析与持久化。首先统计至当前时间的音乐播放量，方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">songPlayLog.map(x =&gt; &#123;  </span><br><span class=\"line\">    \t  <span class=\"comment\">//rowkey设计为日期与歌曲编号维度  </span></span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>) + <span class=\"string\">\"_\"</span> + x.songID, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKey(_+_).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[SongPlayDailyCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(SongPlayDailyCount(pair._1, pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        SongPlayDailyCountDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>同样地对于音乐播放是会有实时热点产生，所以需要实时统计窗口时间内的歌曲播放数据，并在较短时间内更新，以响应热点。首先需要进行HBase表设计，本文设计对于时间段内歌曲播放这类热点数据只保留最新时间的分析结果即可。其次，本文设计针对近一个小时内的歌曲播放进行统计，滑动间隔为10分钟，即每十分钟更新一次近一个小时的歌曲播放数据。在进行HBase表创建时需要设置表中数据的存活时间TTL，本文设计设置为600s（10分钟），即表中数据产生十分钟后失效，最新的数据计算后会被写入表中。分析数据方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">songPlayLog.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>) + <span class=\"string\">\"_\"</span> + x.songID, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKeyAndWindow((x: Int, y: Int) =&gt; x + y,  </span><br><span class=\"line\">      Seconds(<span class=\"number\">3600</span>), Seconds(<span class=\"number\">600</span>)).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[RecentlySongPlayCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(RecentlySongPlayCount(pair._1,  </span><br><span class=\"line\">            pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        RecentlyPlaySongDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>以上即得到了实时的歌曲播放统计数据，以及时间段内热点歌曲的播放统计。其他的行为日志的分析需要根据实际需求来实现，本文设计还有歌曲收藏和歌曲评论两个行为日志分析，分别从其他不同的角度反映网站内容数据。</p>\n<h4 id=\"4-2-3-Spark-Streaming应用提交集群机器执行\"><a href=\"#4-2-3-Spark-Streaming应用提交集群机器执行\" class=\"headerlink\" title=\"4.2.3 Spark Streaming应用提交集群机器执行\"></a>4.2.3 Spark Streaming应用提交集群机器执行</h4><p>将编写好的Spark Streaming作业程序编写好后使用mvn编译，将编译好的jar包拷贝至集群的机器上，在使用spark-submit指令进行任务的提交，指令如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-submit --master local[<span class=\"number\">5</span>] \\  </span><br><span class=\"line\">--jars $(echo /home/hadoop/app/hbase-<span class=\"number\">1.2</span>.0-cdh5.7.0/lib<span class=\"comment\">/*.jar | tr ' ' ',') \\ </span></span><br><span class=\"line\"><span class=\"comment\">--class com.chaoyue.spark.project.scala.MyStreamingApp \\  </span></span><br><span class=\"line\"><span class=\"comment\">--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \\  </span></span><br><span class=\"line\"><span class=\"comment\">/home/hadoop/lib/sparktrain-1.0.jar \\  </span></span><br><span class=\"line\"><span class=\"comment\">hadoop000:2181 test streamingtopic 1</span></span><br></pre></td></tr></table></figure>\n<p>此时Spark Streaming作业即运行在集群上了，可以再控制台查看运行日志如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO ShuffledDStream: Time <span class=\"number\">1557649980000</span> ms is invalid as zeroTime is <span class=\"number\">1557649260000</span> ms , slideDuration is <span class=\"number\">1800000</span> ms and difference is <span class=\"number\">720000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO JobScheduler: Starting job streaming job <span class=\"number\">1557649980000</span> ms.0 from job set of time <span class=\"number\">1557649980000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO SparkContext: Starting job: foreachPartition at MyStreamingApp.scala:<span class=\"number\">59</span>  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO JobScheduler: Added jobs <span class=\"keyword\">for</span> time <span class=\"number\">1557649980000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO DAGScheduler: Registering RDD <span class=\"number\">115</span> (map at MyStreamingApp.scala:<span class=\"number\">56</span>)  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO MemoryStore: Block broadcast_43 stored   </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on <span class=\"number\">192.168</span>.1.108:<span class=\"number\">42189</span> (size: <span class=\"number\">1869.0</span> B, free: <span class=\"number\">366.1</span> MB)  </span><br><span class=\"line\">...  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">05</span> INFO BlockGenerator: Pushed block input-<span class=\"number\">0</span>-<span class=\"number\">1557649985600</span>  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">08</span> INFO MemoryStore: Block input-<span class=\"number\">0</span>-<span class=\"number\">1557649988600</span> <span class=\"function\">stored as bytes in <span class=\"title\">memory</span> <span class=\"params\">(estimated size <span class=\"number\">305.0</span> B, free <span class=\"number\">365.9</span> MB)</span>  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 INFO BlockManagerInfo: Added input-0-1557649988600 in memory on 192.168.1.108:42189 <span class=\"params\">(size: <span class=\"number\">305.0</span> B, free: <span class=\"number\">366.0</span> MB)</span>  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 WARN BlockManager: Block input-0-1557649988600 replicated to only 0 <span class=\"title\">peer</span><span class=\"params\">(s)</span> instead of 1 peers  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 INFO BlockGenerator: Pushed block input-0-1557649988600</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3分析结果可视化模块\"><a href=\"#4-3分析结果可视化模块\" class=\"headerlink\" title=\"4.3分析结果可视化模块\"></a>4.3分析结果可视化模块</h3><h4 id=\"4-3-1分析结果可视化模块设计\"><a href=\"#4-3-1分析结果可视化模块设计\" class=\"headerlink\" title=\"4.3.1分析结果可视化模块设计\"></a>4.3.1分析结果可视化模块设计</h4><p>可视化部分采用前后端分离的开发模式，最终通过web网页的方式展示可视化结果。可视化模块不只是数据库中数据的展示，还要完成在实时流处理场景中不适合实时处理的其他维度的操作，即完成上个模块没有完成的复杂维度的分析。例如实时流处理中记录的歌曲播放信息只包括时间戳、用户ID和歌曲ID，但是歌曲本身的信息是丰富的，要展示的维度也是丰富的，所以在可视化模块进行复杂的连接操作，完成最终更丰富的分析结果展示。</p>\n<p>前端使用react为基础框架，react组件的思想非常适合复用，对于不同数据进行相同类型的展示时可以更换数据源复用组件。此外使用蚂蚁金服的开源react UI组件库ant-design和ant-design Pro，以及商业场景下的数据可视化解决方案Bizcharts，完成丰富的数据可视化展示。后端使用SpringBoot透出相关接口，给前端组件提供需要展示的数据，此外后端还负责实时流处理过程中没有完成的连接操作和其他处理数据的操作。</p>\n<h4 id=\"4-3-2分析结果可视化模块实现\"><a href=\"#4-3-2分析结果可视化模块实现\" class=\"headerlink\" title=\"4.3.2分析结果可视化模块实现\"></a>4.3.2分析结果可视化模块实现</h4><ul>\n<li>可视化web界面概览</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368878922/0\" alt></p>\n<p>如4.1所示，今日数据概览页面展示当日实时数据，包括：实时PageView、七日PV走势、今日PV走势，各类目实时访问统计、各类目访问趋势、今日来源网站统计和今日热搜词云。页面根据维度逐渐细化来布局，从上到下从左到右，首先是PV的总量，然后从一天中的时间为维度和类目为维度细化PV值，再到其他杂项指标。今日PV走势组件展示了网站访问在一天内的趋势，每30分钟统计30分钟内访问量，可以看出音乐网站的用户群体在中午和傍晚到晚上十分活跃。类目实时访问统计展示了用户对哪一个子类目更偏爱，哪些类目需要优化。七日PV比较给一周内的PV比较，可以大致看到某一天的网站浏览状况，需要历史日期更细化的数据可以到历史数据查询里查看。类目访问趋势展示了各类目的今日访问趋势，是前面两个维度的横向和纵向细化，帮助网站运营人员从细节查看网站数据。杂项数据包括来源网站统计和热搜词云，这些展示的是从搜索引擎过来的流量的分析，根据搜索引擎的热搜词，可以帮助网站进行SEO（搜索引擎优化），提高网站排名。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368954251/0\" alt></p>\n<p>如图4.2所示，历史数据查询提供选择日期的组件，可以查看所选日期当天的相关数据。提供30天PV比较，可以看到更长时间区间内网站浏览量的变化，通过这些变化的观察可以对网站的运营策略进行适当的调整。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369327550/0\" alt></p>\n<p>图4.3是音乐网站行为的模拟，具有典型的歌曲资源位组件，可以模拟的行为有歌曲播放、收藏和评论。点击按钮则会调用后端相关接口，后端通过log4j记录下行为日志数据。一个歌曲资源包括歌名、歌手、专辑、封面地址、音乐资源地址、风格和标签等，使用MySQL保存这些资源信息。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369422528/0\" alt></p>\n<p>图4.4是页面行为日志数据的实时处理分析展示页面。展示的内容包括，今日歌曲播放一览、近一小时最热播放歌曲、播放歌曲类型统计、今日加权热门歌曲、今日歌曲评论和收藏统计。今日歌曲播放一览展示今日实时的歌曲播放量统计。近一小时最热播放歌曲展示一小时窗口时间的歌曲播放量排行，是前一个统计维度的子集。歌曲播放类型统计使用南丁格尔玫瑰花环展示实时歌曲播放不同类型数量的统计，这个维度即是对实时流处理中得到的分析结果进行更高一级的抽象，在可视化的后端将歌曲播放量与歌曲资源进行连接操作得到歌曲类型统计。今日加权热门歌曲通过将三中行为的统计量进行加权，得到热门歌曲，加权权值为播放0.5：收藏1：评论0.8。另外两个展示的是实时歌曲收藏量与评论量。</p>\n<ul>\n<li>可视化部分详细展示</li>\n</ul>\n<p>下面将上节涉及的主要功能组件进行展示。</p>\n<ul>\n<li>页面实时浏览量</li>\n</ul>\n<p>实时展示网站今日总浏览量，周同比展示今日与前一周当日的浏览量百分比，日环比展示与前一日的浏览量百分比，日均展示一周七天的浏览量均值。如图4.5所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369535329/0\" alt></p>\n<ul>\n<li>七日浏览量走势</li>\n</ul>\n<p>展示七天前至当日的浏览量走势，如图4.6所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369649506/0\" alt></p>\n<ul>\n<li>今日实时浏览量走势</li>\n</ul>\n<p>以半小时为维度展示今日浏览量数据，每隔半小时统计半小时窗口期中的浏览量，能够清晰展示浏览量在一天中的变化。如图4.7所示，可以看出在凌晨时分访问量逐渐减少，到了早上访问量开始逐渐上升。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369685204/0\" alt></p>\n<ul>\n<li>各类目实时浏览量统计</li>\n</ul>\n<p>以网站不同类目页面为维度进行浏览量数据以及占比的实时展示，音乐网站下设6个大类目：瞩目艺人、最近播放、今日专辑、为你推荐、今日歌单和新近发布。如图4.8所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369719484/0\" alt></p>\n<ul>\n<li>今日各类目访问趋势</li>\n</ul>\n<p>以时间与类目为维度进行浏览量数据的展示，是前两个数据展示组件的细化。如图4.9所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369747929/0\" alt></p>\n<ul>\n<li>来源网站实时统计</li>\n</ul>\n<p>展示今日来源网站的数据统计，如从百度搜索进入主站则将百度搜索记录下来。如图4.10所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369788944/0\" alt></p>\n<ul>\n<li>今日热搜词云</li>\n</ul>\n<p>从搜索引擎进入主站时所进行的搜索词数据统计，如在搜索引擎中搜索“欧美音乐”进入主站，则对“欧美音乐”这个词条进行一次记录，用于搜索引擎优化。如图4.11所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369815950/0\" alt></p>\n<ul>\n<li>月度浏览量比较</li>\n</ul>\n<p>以月为维度对浏览量数据进行展示比较。如图4.12所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369855984/0\" alt></p>\n<ul>\n<li>页面行为模拟模块</li>\n</ul>\n<p>音乐网站页面用户行为的模拟组件，可模拟音乐播放、收藏、评论三种行为，在用户进行页面行为时则会进行日志的记录。如图4.13所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369965341/0\" alt></p>\n<ul>\n<li>今日歌曲播放一览</li>\n</ul>\n<p>今日实时歌曲播放量实时数据展示，如图4.14所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369987974/0\" alt></p>\n<ul>\n<li>近一小时播放Top歌曲</li>\n</ul>\n<p>近一小时窗口时间内歌曲播放量展示，每十分钟进行一小时窗口时间播放量的统计，展示当下实时热播歌曲。如图4.15所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370044063/0\" alt></p>\n<ul>\n<li>播放音乐类型统计</li>\n</ul>\n<p>以音乐风格为维度进行歌曲播放量数据实时展示，如图4.16所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370078824/0\" alt></p>\n<ul>\n<li>多权重今日热门歌曲</li>\n</ul>\n<p>从播放、收藏和评论三个维度进行不同权重的加权计算，得出今日热门歌曲进行展示，可以设计更复杂的加权算法得到热门歌曲，便于运营人员推送歌曲内容。如图4.17所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370164164/0\" alt></p>\n<ul>\n<li>今日歌曲收藏一览</li>\n</ul>\n<p>今日实时歌曲收藏量数据展示，如图4.18所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1559370195372/0\" alt></p>\n<ul>\n<li>今日歌曲评论一览</li>\n</ul>\n<p>今日实时歌曲评论量数据展示，如图4.19所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370221681/0\" alt></p>\n<h3 id=\"4-4-本章小结\"><a href=\"#4-4-本章小结\" class=\"headerlink\" title=\"4.4 本章小结\"></a>4.4 本章小结</h3><p>本章在系统需求分析与架构设计的基础上，分模块进行了详细的设计与实现，给出了每个部分的核心代码与解释，并在可视化模块介绍了各个分析数据的展现形式以及其中的意义。</p>\n<h2 id=\"5总结与展望\"><a href=\"#5总结与展望\" class=\"headerlink\" title=\"5总结与展望\"></a>5总结与展望</h2><h3 id=\"5-1-总结\"><a href=\"#5-1-总结\" class=\"headerlink\" title=\"5.1 总结\"></a>5.1 总结</h3><p>人类可以基于各种信息进行判断、决策，最终创造出价值，而数据作为信息的载体，正在被大量产生。然而数据的形式是多样的，而且是分散的，如何将分布的数据聚合并分析提取出有价值的信息，是大数据时代的挑战。海量数据分析作为互联网行业的闭环模式中的一环，其价值和重要性已经被广泛关注，越来越多的企业可是设立数据部门。一个网站的各个环节都会产生日志数据，这些数据形式多样且分散在不同机器实体上，如何高效采集、聚合、处理、分析和持久化给大数据分析带来了挑战。而现如今，更多的大数据分析需求还要加上“实时处理”这个需求。使用Spark作为分布式数据处理工具，整合海量日志采集工具Flume、可靠消息队列Kafka和海量数据存储HBase，满足了网站日志实时流处理的需求。Flume可定制化的从多种数据源分部式地采集日志，同时具备一定的数据处理能力，可以向多种下游数据目的地传输日志数据。消息队列Kafka保证了海量的日志数据能够有序、不丢失的被消费，同时运用topic机制使得在上游聚合的日志数据以不同的策略被处理和分析。Spark Streaming是整个系统的核心，其是在Spark Core之上的高级API，为整个数据处理流程提供了实时流处理的上下文。Spark的计算模型核心是弹性分布式数据集RDD，提供个基于内存的集群计算的容错性抽象。Spark Streaming的实时流处理能力是基于DStream，其是RDD的一个在时间和空间上的高层抽象，使得我们可以像处理RDD一样处理DStream。同时Spark Streaming支持从多种数据源创建DStream。可以说，Spark Streaming提供了基础实时流处理的解决方案。HBase提供了基于列的海量数据存储，不同于关系型数据库，HBase在大数据处理分析数据持久化方面有许多优点。本文基于Spark Streaming的音乐网站实时流处理，使用上述提及的几个工具和解决方案，同时使用react和SpringBoot进行了分析数据的可视化展示。</p>\n<p>本设计完成的主要成果如下：</p>\n<p>1）使用Flume实时采集服务器上access.log中的网页浏览日志和使用前端埋点采集到的页面行为日志。解决了分布式日志采集和聚合的问题，同时对日志进行了分流以供下游Kafka使用。</p>\n<p>2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源。使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力。在实时流处理中分析出基础业务指标数据，以供上层最这些数据进行其他复杂的操作，并将这些分析结果数据持久化至HBase中。</p>\n<p>3）使用React和SpringBoot构建前后端分离项目，同时在后端结合MySQL完成数据分析上层复杂的连接操作。最终从今日数据概览、历史数据查询和页面行为分析模拟三方面进行展示。分析数据展示包括：实时网站访问量、当日实时访问量走势、周和月维度访问量比较、各类目实时访问量及走势、来源网站统计、搜索引擎热搜词统计、歌曲实时播放量、近一小时热门歌曲播放量、歌曲播放类型比较、加权热门歌曲、实时歌曲收藏和评论统计。</p>\n<p>本文系统提供的数据可以帮助网站的管理、运营和开发维护人员更好地优化网站，比如优化网站类目内容、优化搜索引擎结果以及选择最合适的时间升级网站服务等。</p>\n<h3 id=\"5-2-展望\"><a href=\"#5-2-展望\" class=\"headerlink\" title=\"5.2 展望\"></a>5.2 展望</h3><p>实时流处理不仅仅是进行简单的数据分析，还有许多实时响应的场景下有更复杂的需求。比如，电商搜索推荐系统在搜索导购的各个场景都需要能够秒级召回个性化推荐数据，而这需要大量的离线和实时数据处理相结合和完成。Spark作为分布式数据处理框架，不仅提供了秒级实时流处理的高级抽象Spark Streaming，还提供了用于机器学习的MLlib、图处理的GraphX、交互式查询SparkSQL，对这些高级抽象进行整合应用可以处理更复杂场景下的需求。</p>\n","site":{"data":{"projects":[{"name":"AISmallTribe-weixinMiniProgram","url":"https://github.com/fangmiao97/AISmallTribe-weixinMiniProgram","desc":"一个展示型的微信小程序，旨在向更多对人工智能、机器学习感兴趣的非专业人士提供简单、可视化的算法展示"},{"name":"Minions-Real-time-data-streaming-processing-project","url":"https://github.com/fangmiao97/Minions-Real-time-data-streaming-processing-project","desc":"My Graduation Design Works🌟, using Flume/Kafka/Spark Streaming/React/Ant Design (Pro)."}]}},"excerpt":"","more":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>随着大数据时代的到来，数据总量不断增加，同时分布式存储与计算技术也在快速发展，如何从海量、分散的数据中提取出有价值的信息，已经成为各个行业的迫切需求，对数据的实时处理和响应变得越来越重要。</p>\n<p>本文设计了基于Spark Streaming的网站日志实时流处理系统，针对通用音乐网站两种类型的日志：网页浏览日志和网页行为日志，进行了实时采集、传输、处理、分析、持久化和可视化。本文完成了以下几个功能：1）使用Flume采集日志数据，解决了分布式日志采集和聚合的问题，同时对日志进行了分流；2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源；3）使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力，并将分析结果数据持久化至HBase中；4）使用前后端分离的开发模式完成分析结果的可视化展示。系统实现了将大量分散的日志数据进行实时整合，并完成分析计算，最后从多种维度进行信息展示。</p>\n<p>关键词：实时流处理；分布式计算；Spark Streaming；Flume；前后端分离</p>\n<h2 id=\"ABSTRACT\"><a href=\"#ABSTRACT\" class=\"headerlink\" title=\"ABSTRACT\"></a>ABSTRACT</h2><p>With the advent of the era of big data, the total amount of data continues to increase, and distributed storage and computing technologies are also developing rapidly. How to extract valuable information from massive and decentralized data has become an urgent need of various industries. Real-time processing and response of data is becoming increasingly important.</p>\n<p>This paper designs a real-time stream processing of website logs system based on Spark Streaming. It performs real-time collection, transmission, processing, analysis, persistence and visualization for two types of logs: universal web browsing logs and web page behavior logs. This paper has completed the following functions: 1) Using Flume to collect log data, solving the problem of distributed log collection and aggregation, and diverting the log at the same time; 2) Using message queue Kafka to solve a large number of log data transmission problems, which providing real-time log data source to the downstream application to process and analysis data; 3) using Spark Streaming to perform real-time processing and analysis of various types of log data, achieving the capability of second-level processing and phase data processing, and persisting the analysis result data to HBase; 4)Visualization of the results of the analysis using a development model with front and rear separation. The system realizes the real-time integration of a large number of scattered log data, completes the analysis and calculation, and finally displays information from various dimensions.</p>\n<p>KEYWORDS: Real-time stream processing; Distributed Computing; Spark Streaming; Flume; Front and rear separation</p>\n<h2 id=\"1绪论\"><a href=\"#1绪论\" class=\"headerlink\" title=\"1绪论\"></a>1绪论</h2><h3 id=\"1-1研究背景与意义\"><a href=\"#1-1研究背景与意义\" class=\"headerlink\" title=\"1.1研究背景与意义\"></a>1.1研究背景与意义</h3><p>随着DT（Data Technology）时代的到来，数据总量不断。预计到2020年，全球数据总量将超过44ZB，同时根据图灵奖获得者杰姆·格雷提出的“新摩尔定律”，每18个月，全球新增数据总量是自计算机诞生以来所有数据量之和。数据作为一种新的能源，被各行各业广为利用，为企业生产经营提供有价值的信息。同时，“爆炸式”增长的数据也对现有的数据采集、传输、分析和存储技术提出了挑战。</p>\n<p>互联网行业可以从多种多样的数据源采集信息，例如，服务器日志、网关日志、数据库日志和业务信息等。这些数据通过企业商业智能平台的挖掘分析，为企业内部以及各样相关者提供有价值的信息，例如，电商行业对于用户画像进行分析，进行精准营销和“千人千面”的召回数据，提高转化率；金融行业对于风险监控的要求较高，通过大数据分析，将提高企业的风控效率和准确率。而浏览器的页面型产品/服务的日志在众多数据源十分突出，一是其自然地提供了页面浏览（展现）的详细情况，以及具有定制化采集页面交互日志的能力，所以企业能够在这些日志数据中提取到大量可用有用的信息；二是部分日志信息具有很高的时效性，这是指采集到的日志数据需要实时分析、处理，并召回实时响应数据，为用户提供更好的体验。数据的离线和实时分析都是互联网行业“闭环”的重要部分。</p>\n<p>各大公司以及开源社区都提出了各种可用的计算模型和解决方案，为数据采集、传输、存储和分析提供了多种有用的工具。而目前最为火爆的工具之一就是Spark，它在Spark Core的基础上提供了多种高级实现，其中Spark Streaming使数据的实时处理成为可能。</p>\n<p>本文以一个通用音乐网站为背景，基于Spark Streaming对网站浏览日志和交互行为日志进行实时流处理，完成了从数据采集、传输、清洗、处理、持久化以及可视化的全流程。系统实现了网站服务器浏览日志的采集，以及通过前端埋点的方式采集可定制化的数据信息，使用Flume和Kafka向下游分流和传输数据。使用Spark Streaming清洗和处理数据，并将相关业务指标数据持久化到HBase中。可视化使用Springboot和React进行前后端分离的实现。</p>\n<p>本系统提供的数据有：</p>\n<ul>\n<li>页面浏览数据，包括Page View和网站各类目访问量，访问趋势等多维度的指标可视化数据，帮助相关人员进行网站的优化以及搜索引擎优化。</li>\n<li>页面交互信息数据，包括音乐网站情景下多种用户操作行为的记录与分析，并实时展示多维度的分析结果。</li>\n</ul>\n<h3 id=\"1-2研究现状\"><a href=\"#1-2研究现状\" class=\"headerlink\" title=\"1.2研究现状\"></a>1.2研究现状</h3><p>日志分析是在海量离散的日志数据中提取出有价值的，符合人们认知的信息。网站日志分析则是在页面浏览、页面行为等日志中得到多种维度的数据，供网站不同职能人员使用。例如，运营人员根据网站浏览量、各类目的访问情况等制定相应的运营方案；高层团队领导根据网站各业务转化率等指标灵活制定网站产品发展战略；开发人员根据网站服务性能数据优化产品性能，保障产品质量。如今已有许多商业上可行的方案，比如百度统计、神策数据等，但是它们要么关注仅仅关注访问量的统计，要么关注用户页面行为的统计。</p>\n<p>日志分析在国内外发展都十分迅速。最开始由于网站用户访问量不大，业务量规模也相对较小，整个网站的各个功能模块的日志都由专员进行分析。一般情况是远程登录各个节点来分析日志。随着业务量的不断增大，服务器以及功能模块的不断增多，日志分散在大量机器实体上，这给日志数据的采集、传输、分析和存储带来了挑战。</p>\n<p>Google用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。但是随着业务场景的复杂化，如互联网金融中的实时风控、电商中的个性化推荐数据召回等的出现，运用Hadoop MapReduce完成的离线数据分析已经不能满足企业发展的需求。Spark于2009年诞生于UC Berkeley的AMP实验室，并与2013年贡献至Apache社区，成为近年来最火热的大数据开源项目。</p>\n<p>Spark是基于内存的并行计算框架，也是基于MapReduce计算模型的分布式计算框架，其解决了Hadoop MapReduce的诸多问题，例如不能重用在迭代式机器学习和图计算中经常需要的中间结果，Hadoop MapReduce需要将这些中间数据进行复制、硬盘I/O和序列化。而Spark实现了弹性分布式数据集（RDD），一种基于内存的集群计算容错性抽象，能够很好地适应各种业务场景。Spark Streaming是在Spark Core之上的高级API。Spark Streaming提供了表示连续数据流的、高度抽象的被称为离散流的Dstream，可以通过多种数据源创建Dstream，为下游实时流处理使用。所以Spark Streaming为当前实时流处理的场景需求提供了可靠的上下文。</p>\n<p>此外，使用Flume与Kafka为海量日志的采集和传输提供了解决方案。Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，通过Flume可以将分散在各个服务器上的日志收集起来，以供下游日志数据处理分析的使用。Kafka是一种高吞吐量的分布式发布订阅消息系统，在大型网络中运用其对网站的服务性能进行扩展，能够保证所有服务请求不丢失，同时Kafka作为大型网站的基础设施，也为实时日志流处理提供消息队列的服务。</p>\n<p>目前对于实时流处理有许多解决方案，日志采集、传输、分析和存储的技术也有许多种选择。不同的解决方案/工具是由特定场景下提出的，寻找最合适的工具完成系统设计十分关键。</p>\n<h3 id=\"1-3论文结构\"><a href=\"#1-3论文结构\" class=\"headerlink\" title=\"1.3论文结构\"></a>1.3论文结构</h3><p>本文将全文内容划分为五个章节进行展开，其组织结构如下：</p>\n<p>第一章：绪论部分。简要介绍了网站日志实时流处理的研究背景和意义，并对现有大数据分析技术进行了简单的介绍，同时描述了本文设计的主要工作。</p>\n<p>第二章：系统相关理论和技术介绍。简要介绍了数据处理和日志分析的出发点和要点，针对日志采集、传输、处理、分析的难点进行介绍，并提供了现有可靠的解决方案或工具。</p>\n<p>第三章：系统需求分析与架构设计。基于本文设计的系统出了分模块的需求，并整合最佳的解决方案，设计了系统整体架构。</p>\n<p>第四章：从日志数据产生与采集、日志数据传输、处理、分析与持久化以及分析结果可视化三个方面进行了详细设计与实现。</p>\n<p>第五章：对本文设计的系统进行总结，并提出接下来要进一步完善的需求。</p>\n<h2 id=\"2相关理论与技术\"><a href=\"#2相关理论与技术\" class=\"headerlink\" title=\"2相关理论与技术\"></a>2相关理论与技术</h2><h3 id=\"2-1互联网中的闭环理论\"><a href=\"#2-1互联网中的闭环理论\" class=\"headerlink\" title=\"2.1互联网中的闭环理论\"></a>2.1互联网中的闭环理论</h3><h4 id=\"2-1-1闭环理论\"><a href=\"#2-1-1闭环理论\" class=\"headerlink\" title=\"2.1.1闭环理论\"></a>2.1.1闭环理论</h4><p>闭环理论是源于系统论和控制论的一种管理思想，其核心是通过有效的管理和控制手段，构成一个连续封闭的管理回路，使得系统中的每一个环节都能有效的衔接。最终达到提升管理水平，实现既定目标的目的。闭环系统的典型特征有：可控性强、首尾相连、循环往复。闭环式的组织能够高效地利用资源，灵活应对外界变化，达成组织目标，并在这个过程中不断自我提升。</p>\n<p>闭环理论强调“可控性”，在组织的运行流程中，尽可能的收集信息，并与原计划进行匹配，出现偏差则进行调整和修正。从而实现系统的高效演进与自我提升。</p>\n<h4 id=\"2-1-2互联网闭环\"><a href=\"#2-1-2互联网闭环\" class=\"headerlink\" title=\"2.1.2互联网闭环\"></a>2.1.2互联网闭环</h4><p>闭环理论最初由休哈特于1930年提出，现在已经衍生运用于各行各业。对于计算机互联网这样的强调系统和控制的行业，闭环理论运用十分广泛。在互联网行业中运用闭环理论的系统通常包括：验证、反馈、分析和控制这几个部分。直观的运用实例如下：</p>\n<ul>\n<li>产品开发、运营优化。如产品新特性发布前的灰度测试、电商及内容类产品的实时推荐系统等。</li>\n<li>互联网服务。如O2O线上到线下等各类分销平台、金融风控系统等。</li>\n</ul>\n<p>为了达到产品的不断提升，并对外提供更好地服务，互联网行业必须将整个系统的各部分形成完整闭环，而数据挖掘与分析是补完闭环的关键环节。网站日志实时分析作为网站系统闭环的一部分，通过分析来自用户的真实访问情况和行为情况反馈，为运营和决策人员提供有价值的信息，用以不断完善产品质量。此外通过收集数据为产品的其他功能，如推荐投放系统提供一手数据资源，完善相关功能，使得反馈结果实时触达用户。整个系统各个部分都为系统的提升做出贡献，并使之成为一个可提升的完整闭环。</p>\n<h3 id=\"2-2网站日志采集\"><a href=\"#2-2网站日志采集\" class=\"headerlink\" title=\"2.2网站日志采集\"></a>2.2网站日志采集</h3><p>浏览器的页面型产品/服务的日志采集可以分为如下两大类。</p>\n<h4 id=\"2-2-1页面浏览（展现）日志采集\"><a href=\"#2-2-1页面浏览（展现）日志采集\" class=\"headerlink\" title=\"2.2.1页面浏览（展现）日志采集\"></a>2.2.1页面浏览（展现）日志采集</h4><p>页面浏览日志是指当一个页面被浏览器加载呈现时多采集的日志。此类日志是最基础的互联网日志，也是目前所有互联网产品的两大基础指标：页面浏览量（Page View，PV）和访客数（Unique Visitors，UV）的统计基础。页面浏览日志是目前成熟度最高和完备度最高，同时也是最具挑战性的日志采集任务。</p>\n<p>页面浏览日志通常是记录在应用服务器上，如Nginx的access.log。</p>\n<h4 id=\"2-2-2页面交互日志采集\"><a href=\"#2-2-2页面交互日志采集\" class=\"headerlink\" title=\"2.2.2页面交互日志采集\"></a>2.2.2页面交互日志采集</h4><p>当页面加载和渲染完成之后，用户可以在页面上执行各种操作，随着前端技术的发展用户可以与页面上的元素完成很多互动操作，如启动、点击、拖拽和曝光等，这些行为信息价值巨大，设计者通常会要求采集用户的互动行为日志，以便通过量化获知用户的兴趣点或者优化体验。而行为数据属于低价值密度数据，其数量十分巨大，需要强大的数据处理能力和分析能力才能有效利用。</p>\n<p>行为日志的记录有多种方式，网页应用常用前端埋点的方式进行采集。</p>\n<h3 id=\"2-3-ETL过程与Flume\"><a href=\"#2-3-ETL过程与Flume\" class=\"headerlink\" title=\"2.3 ETL过程与Flume\"></a>2.3 ETL过程与Flume</h3><h4 id=\"2-3-1-ETL\"><a href=\"#2-3-1-ETL\" class=\"headerlink\" title=\"2.3.1 ETL\"></a>2.3.1 ETL</h4><p>ETL（Extract-Transform-Load）是一种数据仓库技术，用来描述将数据从数据源抽取（extract）、转换（transform）和加载（load）的过程。这一过程常用于数据仓库技术，但是在目前大数据应用中也十分重要。具体到日志分析类应用，由于服务器众多，同一类型的日志分散在不同的机器实体上。同时不同类型的日志有不同的结构，为了便于分析通常情况下需要同一结构。此外对于各种行为日志需要又不同的下游数据分析应用来处理，这就涉及到一定程度上的分流。因此在网站日志实时流处理中需要有一个ETL过程来进行分散数据的采集和一定程度的过滤清洗，使得下游应用能够高效运作。</p>\n<p>目前，Apache Flume十分适合这类ETL过程的业务场景。</p>\n<h4 id=\"2-3-2-Flume\"><a href=\"#2-3-2-Flume\" class=\"headerlink\" title=\"2.3.2 Flume\"></a>2.3.2 Flume</h4><p>Flume是一个高可用、高可靠的分布式海量日志收集、聚合和传输系统，其能够将不同源的日志数据汇聚到统一的数据存储中；Flume具有基于流式数据的简单且灵活的架构，同时具有优秀的容错机制和故障恢复机制；Flume运用简洁又灵活的数据模型，使得其十分适合在线分析应用。</p>\n<p>Flume不仅可用于日志数据的聚合。由于数据源是可定制的，Flume还可以传输海量的事件数据，包括但不限于，网络流数据、社交媒体产生的数据、电子邮件数据和各种可能形式的数据。</p>\n<p>Flume处理数据的最小单位是一个Flume Event。Flume Agent是一个常驻JVM的进程，用于处理从外部源传输来的event。Agent由source、channel和sink组成，如图2.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366186537/0\" alt></p>\n<p>用户可以选择多种数据源作为source，如avro source、thrift source和exec source等。在本文设计中，对于页面浏览日志使用exec source，执行系统读取指令采集log文件中新增的日志信息。</p>\n<p>对于页面行为日志，采用avro source监听特定的网络端口采集行为信息数据。</p>\n<p>对于采集信息的转换，Flume为source中提供了interceptor（过滤器）这个功能。在本文设计中，将运用过滤器对页面行为信息根据行为类型进行分流，以便下游不同数据处理应用使用。</p>\n<p>Flume的各个单位事件数据通过sink传递给下游应用使用。本文设计中使用kafka sink传递给下游kafka生产者使用。此外，为了防止事件数据过多，使用channel起到缓冲的作用，可以通过设置channel缓冲的大小来确定其承载力。</p>\n<h3 id=\"2-4-Kafka\"><a href=\"#2-4-Kafka\" class=\"headerlink\" title=\"2.4 Kafka\"></a>2.4 Kafka</h3><p>消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋、日志处理等问题，实现高性能，高可用，可伸缩和最终一致性架构。</p>\n<p>Kafka是经常用于日志处理的消息中间件，解决大量日志传输的问题。上游日志采集模块将采集到的数据写入到Kafka队列中，Kafka消息队列负责日志数据的接受、存储和转发，下游日志分析处理应用订阅kafka队列中的日志数据。</p>\n<p>本文设计中下游日志分析应用即是Spark Streaming作业。由于日志的数量很多，不同类型日志的处理分析逻辑不同，所以引入kafka消息队列。一方面防止日志数据过多淹没下游应用，另一方面按消息的topic进行分流消费。</p>\n<h3 id=\"2-5-Hadoop与Spark\"><a href=\"#2-5-Hadoop与Spark\" class=\"headerlink\" title=\"2.5 Hadoop与Spark\"></a>2.5 Hadoop与Spark</h3><p>Google用三篇论文提出了奠定大数据基础的GFS、Big Table和MapReduce，它们分别为分布式文件系统、海量数据存储和分布式计算提供了可行的方案。Apache的顶级项目Hadoop将它们进行了开源实现。因此，技术人员能够运用Hadoop对海量的日志数据进行分析，处理高吞吐、批量处理的业务场景。</p>\n<p>Hadoop是一个分布式系统基础框架，用户可以在不了解分布式底层细节的情况下，开发分布式程序，充分利用集群的威力进行告诉运算和存储。Hadoop诞生于2005年，在社区和众多技术人员的支持下，Hadoop生态圈逐步扩大，形成了一整套用于分布式数据处理分析存储等功能组件。其中最重要的组件有，分布式文件系统HDFS、分布式计算框架MapReduce、分布式列存储数据库HBase、分布式协作服务ZooKeeper、日志收集工具Flume、分布式资源管理器Yarn等。这些组件提供了各种业务场景下数据处理需求的解决方案。其中MapReduce是一种分布式计算模型，用以进行大数据量的计算。其屏蔽了分布式计算框架的细节，将计算抽象成map和reduce两个过程，具有高可靠性、高扩展性、成本低等特点。因此MapReduce被广泛运用与大规模数据分析。</p>\n<p>但是随着更多复杂数据分析处理场景的出现，如迭代式计算和交互式数据挖掘等，MapReduce计算模型的出现了缺陷。首先，MapReduce将数据处理的过程抽象成2个阶段，map和reduce，虽然高度抽象，技术人员只需要编写两个函数即可。但是高度抽象带来的问题是表达能力有限，当出现更复杂的计算要求时，不得不将map和reduce两个过程复杂化或者增加更多的MapReduce任务。同时，MapReduce过程中的计算结果都需要进行磁盘I/O，这样当需要多个MapReduce任务写作时，导致任务之间的衔接涉及I/O开销，不能直接重用中间结果。此外，在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务。</p>\n<p>MapReduce缺少对分布式内存的应用，而Spark在借鉴了Hadoop MapReduce有点的同时，很好地解决了MapReduce所面临的问题。首先Spark实现了一个分布式内存抽象的概念–弹性分布式数据集（以下称为RDD），其能够让开发人员以容错的方式在大规模集群上进行基于内存的计算。RDD是分布式内存的一个抽象概念，提供了高度受限的共享内存模型，这使得Spark计算任务能够在内存中重用中间计算结果。Spark的计算模式也属于MapReduce，但是不限于map和reduce操作，将数据操作抽象成对RDD的一系列粗粒度转换，如map、filter、reduce、group等，比MapReduce更加灵活。由前两个特性，Spark实现的RDD能够拥有很好的容错性能，Spark计算模型将对RDD的操作记录成lineage，即血缘关系，在真实计算任务时并不立即对数据进行转换操作。这样当出现中间计算结果缺失时，只需要根据lineage重新计算数据即可。此外，Spark是基于RDD的lineage形成的有向无环图DAG来进行任务调度的，要优于MapReduce的迭代执行机制，如图2.2所示，实线框为RDD，不同RDD之间的转换形成DAG，Spark通过这个DAG进行任务调度。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366311701/0\" alt></p>\n<p>针对流式数据处理场景，Spark在Spark Core的基础上实现了更高级的API–Spark Streaming。Spark Streaming是提供了表示连续数据流的、高度抽象的离散流（DStream）。DStream本质上是对RDD的一层封装，其中的每一个RDD都包含来自一个时间间隔的数据，如图2.3所示。DStream是一个没有边界的集合，没有大小限制，其代表了一个时空的概念。对DStream的操作，具体到每个时间段，就是空间的操作，也就是对时间间隔的对应批次数据的处理。Spark Streaming对于DStream的操作实质上就是对每个RDD应用批量操作。Spark Streaming将对流数据的处理抽象成对DStream的处理，而这种抽象是的开发人员能够像操作RDD一样操作DStream，所以说Spark Streaming提供了实时流处理的上下文。此外Spark Streaming支持从多个数据源创建DStream，如图2.4所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366347719/0\" alt></p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366361643/0\" alt></p>\n<h3 id=\"2-6前后端分离\"><a href=\"#2-6前后端分离\" class=\"headerlink\" title=\"2.6前后端分离\"></a>2.6前后端分离</h3><p>前后端分离是指在网站开发阶段前端与后端各司其职，前端只要根据设计还原出页面的逻辑展示，后端只要处理根网页数据相关的业务逻辑，并向前端透出需要的接口即可。这种模式能够使前后端并行开发，大大提高效率，并将前后端一定程度上解耦。本文设计中网页技术部分采用前后端分离的模式，模拟用户页面行为的产生，并且进行分析数据结果的可视化。前端采用react技术栈，后端采用SpringBoot。</p>\n<p>React是Facebook公司开源的前端框架，是近年最火热的技术，国外Facebook、Instagram和国内知乎、蚂蚁金服等都在使用。React的思想是将前端所有元素组件化，将所有需要的UI控件甚至逻辑控件抽象成组件进行开发。每个组件维护自己的状态逻辑变化。React技术不断发展，社区中也不断贡献出特定场景下优秀的解决方案。</p>\n<p>SpringBoot是由Pivotal团队开发的框架，其能够简化新Spring应用的初始搭建以及开发过程。SpringBoot使用很多特定或默认的方式进行配置，简化了开发人员的准备工作时间，可以专注于真正业务的实现。SpringBoot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、产品级别的Spring应用，特别适合实现目前最火热的微服务架构。本文设计选择SpringBoot作为后端服务框架。</p>\n<h3 id=\"2-7-本章小结\"><a href=\"#2-7-本章小结\" class=\"headerlink\" title=\"2.7 本章小结\"></a>2.7 本章小结</h3><p>本章介绍了互联网中的闭环理论以及针对日志数据采集、传输、处理、分析、持久化以及可视化的可靠的开源工具，为本文设计的系统中的各个需求场景提供了最优的解决方案。</p>\n<h2 id=\"3系统需求分析与架构设计\"><a href=\"#3系统需求分析与架构设计\" class=\"headerlink\" title=\"3系统需求分析与架构设计\"></a>3系统需求分析与架构设计</h2><h3 id=\"3-1系统需求分析\"><a href=\"#3-1系统需求分析\" class=\"headerlink\" title=\"3.1系统需求分析\"></a>3.1系统需求分析</h3><h4 id=\"3-1-1系统概述\"><a href=\"#3-1-1系统概述\" class=\"headerlink\" title=\"3.1.1系统概述\"></a>3.1.1系统概述</h4><p>本文设计以一个通用音乐网站的日志为分析对象的流处理系统，完成对日志进行实时采集、传输、清洗、处理、分析、持久化和可视化，最终得到网站页面浏览和页面行为日志数据的相关数据分析指标结果等有价值的信息，展示实时流处理的能力。按照数据流处理流程可将本系统分为以下几个部分：日志数据产生与采集模块、日志数据传输模块、日志数据处理分析模块、分析结果持久化与可视化模块。数据流程如图3.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366469597/0\" alt></p>\n<p>音乐网站的日志分为两种类型，页面浏览日志和页面行为日志。对于两种类型的日志需要使用统一的日志收集工具进行收集，以便下游组件的使用；数据量庞大的日志数据需要使用消息队列进行传输，达到流量削峰的目的；从消息队列得到的日志数据进过分布式计算工具进行处理计算分析，持久化至存储设施中，最后将分析结果与网站业务相关的结构化数据进行更具体的连接操作完成分析结果的可视化展示。</p>\n<h4 id=\"3-1-2数据产生与采集需求分析\"><a href=\"#3-1-2数据产生与采集需求分析\" class=\"headerlink\" title=\"3.1.2数据产生与采集需求分析\"></a>3.1.2数据产生与采集需求分析</h4><p>两类不同的日志，页面浏览日志和页面行为日志，采用不同的方式产生。对于页面浏览日志中的每一条浏览日志需要是可配置的结构化数据，这样便于下游应用的清洗等操作,这些日志数据还需要存储在特定的位置，便于采集；对于页面行为日志，同样需要有一定的数据结构，便于在下游对不同的行为日志数据进行分流，但是页面行为日志可以有一定的定制化内容，因为不同的行为可能要采集的数据是不同的，因此行为日志的结构要保留一定的灵活性。行为日志数据的产生不能对用户体验产生影响，也就是说对于用户来说是透明的。</p>\n<p>日志数据的采集需要考虑到网站服务分布式的特点，也就是各类型日志都是分布在不同的服务器实体上，因此需要使用一个能够将分布的日志快速收集为全量日志数据的工具，同时为了与下游数据传输组件进行配合，采集工具需要具有一定的分流功能，即是能够将不同种类的日志信息先进行汇总，再向同一目的分发。</p>\n<h4 id=\"3-1-3数据传输与处理模块需求分析\"><a href=\"#3-1-3数据传输与处理模块需求分析\" class=\"headerlink\" title=\"3.1.3数据传输与处理模块需求分析\"></a>3.1.3数据传输与处理模块需求分析</h4><p>海量日志数据如果不加控制地发送至下游应用进行处理分析，会造成很多问题，最严重的就是超过应用的处理限度而不能正常服务，从而丢失数据。所以必须使用消息中间件解决大量日志数据传输的问题。消息中间件还需要有高可靠性和容错的能力。</p>\n<p>对于日志数据的处理分析模块，需要根据不同的日志类型和日志的种类进行不同的处理分析，也就是说页面浏览日志与页面行为日志分析的逻辑是不同的，对于不同行为的分析也是不同的。分析处理模块需要具有实时流处理的能力，以及一定的容错能力。同时还能分析复杂的数据场景，比如处理一定时间窗口内的所有数据。</p>\n<h4 id=\"3-1-4数据持久化与可视化模块需求分析\"><a href=\"#3-1-4数据持久化与可视化模块需求分析\" class=\"headerlink\" title=\"3.1.4数据持久化与可视化模块需求分析\"></a>3.1.4数据持久化与可视化模块需求分析</h4><p>分时结果数据的持久化需要考虑海量数据的特点，同时对于数据存储内容还要有一定的灵活性，这是指存储的分析结果内容应该能够根据业务的需求进行调整，能够动态地增加字段而不用修改表结构。</p>\n<p>可视化模块作为系统最终对外展示的部分，体现了整个系统的风格。优雅直观的界面和简洁直接的交互，不会让使用者产生疲倦感，同时有意愿继续使用和对产品提出改进意见。分析结果数据应该从各个维度进行数据的展示，维度由大到小，逐渐细化，帮助用户更好的观察数据。可视化模块是将一条条结构化的数据反映成直观形式表示的最后一步。</p>\n<p>可视化部分分为：今日数据概览、历史数据查询和网页行为分析。详细需求如下：</p>\n<ul>\n<li>今日数据概览</li>\n</ul>\n<p>展示网站各维度的今日实时数据，包括实时PageView、七日PageView走势、今日实时PageView走势、网站各类目访问量、各类目访问量走势、今日来源网站数据汇总和热搜词云。以供运营人员优化网站和优化搜索引擎结果等。</p>\n<ul>\n<li>历史数据查询</li>\n</ul>\n<p>可以选择所要查询的日期，查看所选日期的各项数据指标，同时增加以天为维度的30日访问量比较，可用于帮助网站各级别管理人员制定网站今后的内容发展方向，辅助决策。</p>\n<ul>\n<li>网页行为分析</li>\n</ul>\n<p>能够模拟音乐网站上的用户行为，如歌曲播放、收藏和评论等。从各个维度展现行为日志分析数据，如今日歌曲播放量排行，近一小时内热点歌曲排行、歌曲风格统计、热门歌曲标签云、热评歌曲和热门收藏歌曲等。各维度数据供网站运营人员优化网站歌曲内容。</p>\n<h3 id=\"3-2系统架构设计\"><a href=\"#3-2系统架构设计\" class=\"headerlink\" title=\"3.2系统架构设计\"></a>3.2系统架构设计</h3><h4 id=\"3-2-1总体架构\"><a href=\"#3-2-1总体架构\" class=\"headerlink\" title=\"3.2.1总体架构\"></a>3.2.1总体架构</h4><p>本文设计的网站日志实时流处理系统，采取分布式的架构，遵循低耦合的原则。总体架构分为四部分：日志数据产生、日志采集与传输、日志处理分析和持久化以及分析结果可视化。总体架构如图3.2所示，页面浏览日志与页面行为日志采用不同的方案产生；使用Flume与Kafka来进行日志数据的采集与传输；实时日志数据通过Spark Streaming进行处理与分析并将分析结果持久化至HBase中；最后采用前后端分离的模式，整个MySQL中的业务数据完成分析结果的可视化展示。总体架构如图3.1所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559366982662/0\" alt></p>\n<h4 id=\"3-2-2-日志数据产生模块架构设计\"><a href=\"#3-2-2-日志数据产生模块架构设计\" class=\"headerlink\" title=\"3.2.2 日志数据产生模块架构设计\"></a>3.2.2 日志数据产生模块架构设计</h4><p>网站页面浏览日志是存储在服务器上access.log文件里面的结构化数据，并且这些浏览日志数据的结构是可配置的。本文设计使用Python脚本完成页面浏览日志的产生，并结合Linux的crontab功能完成周期性的调用。页面行为日志使用前端埋点的方式产生，在用户产生某种行为时调用js代码，向后端发出请求。后端使用SpringBoot搭建服务，通过解析请求和log4j产生行为日志数据。该模块架构如图3.3所示，产生的日志数据会被下游模块采集。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367132763/0\" alt></p>\n<h4 id=\"3-2-3-日志采集与传输模块架构设计\"><a href=\"#3-2-3-日志采集与传输模块架构设计\" class=\"headerlink\" title=\"3.2.3 日志采集与传输模块架构设计\"></a>3.2.3 日志采集与传输模块架构设计</h4><p>上一个模块已经产生了两类日志数据，本模块使用Flume和Kafka完成日志数据的采集与传输。页面浏览日志分布式的存储在网站服务器的access.log文件中，在每个服务器上需要部署一个Flume Agent完成access.log新增日志数据的采集，并将这些日志数据对接至下游Kafka生产者；页面行为日志是由服务器产生的log4j日志数据，通过配置会将这些数据发送至指定地址上的特定端口，在该地址上的服务器部署了Flume Agent，其会监听特定端口来采集日志数据。该Flume Agent会根据日志数据的内容进行分流，对接不同的Kafka生产者，使得不同的行为日志数据以不同的方法被实时处理。消息队列Kafka将Flume采集来的数据通过使用不同的topic传输至下游Spark Streaming。该模块架构如图3.4所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367178577/0\" alt></p>\n<h4 id=\"3-2-4-日志处理分析和持久化模块架构设计\"><a href=\"#3-2-4-日志处理分析和持久化模块架构设计\" class=\"headerlink\" title=\"3.2.4 日志处理分析和持久化模块架构设计\"></a>3.2.4 日志处理分析和持久化模块架构设计</h4><p>本模块使用Spark Streaming进行日志数据的处理与分析，针对不同的日志向Spark集群提交相应的Spark Streaming应用，这些应用会根据topic拉取Kafka消息队列中的消息，之后进行处理与分析，分析结果将持久化至HBase中。该模块架构如图3.5所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367225228/0\" alt></p>\n<h4 id=\"3-2-5-分析结果可视化模块架构设计\"><a href=\"#3-2-5-分析结果可视化模块架构设计\" class=\"headerlink\" title=\"3.2.5 分析结果可视化模块架构设计\"></a>3.2.5 分析结果可视化模块架构设计</h4><p>分析结果可视化模块采用react作为前端框架，并结合蚂蚁金服技术体验部的UI组件库ant design、ant design Pro以及商业场景下数据可视化解决方案Bizcharts完成数据的多维度展示。后端使用SpringBoot连接HBase，将日志分析结果透出至前端。此外由于网站具体业务的复杂性，前一阶段的分析结果是比较抽象的，还需要结合存有具体业务数据的结构化数据库MySQL来实现更细化的分析结果数据展示，本次设计音乐网站中较重要的结构化数据是歌曲信息的数据。该模块架构如图3.6所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559367274523/0\" alt></p>\n<h3 id=\"3-3-本章小结\"><a href=\"#3-3-本章小结\" class=\"headerlink\" title=\"3.3 本章小结\"></a>3.3 本章小结</h3><p>本章首先对要设计的系统进行总体概括，并分模块进行了需求分析，然后给出了总体架构设计和详细的分模块架构设计。根据各模块的需求给出了可行且合适的解决方案，为下一章系统的具体实现提供了清晰的方向。</p>\n<h2 id=\"4设计与实现\"><a href=\"#4设计与实现\" class=\"headerlink\" title=\"4设计与实现\"></a>4设计与实现</h2><h3 id=\"4-1日志数据产生与采集模块\"><a href=\"#4-1日志数据产生与采集模块\" class=\"headerlink\" title=\"4.1日志数据产生与采集模块\"></a>4.1日志数据产生与采集模块</h3><h4 id=\"4-1-1日志数据产生与采集模块设计\"><a href=\"#4-1-1日志数据产生与采集模块设计\" class=\"headerlink\" title=\"4.1.1日志数据产生与采集模块设计\"></a>4.1.1日志数据产生与采集模块设计</h4><p>由于时间和成本等现实原因，本文设计不能采用真实线上的日志数据，而是采用模拟的方式产生。模拟方式产生的日志数据与真实线上日志完全相同，首先保证浏览日志的数据格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">98.29</span>.167.156 <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">14</span>:<span class=\"number\">31</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/145.html HTTP/1.1\"</span>  <span class=\"number\">200</span> http:<span class=\"comment\">//www.baidu.com/s?wd=新近发布</span></span><br></pre></td></tr></table></figure>\n<p>包括请求源地址、时间、方法、请求页面、HTTP协议、请求状态以及来源网站。所以需要设计脚本完成access.log的生成，为了防止日志文件过大，需要定时清理。对于页面行为日志，采用前端埋点的方式产生。首先根据产品设计的需求找到需要记录的用户行为，比如点击、滑动、停留、键入内容等，细化到音乐网站上，用户的行为可以是点击播放音乐、评论音乐、收藏音乐和切换音乐等。这些日志需要在产生这些行为时调用相关后端接口，并向后端传送打点的内容，由后端服务器来完成日志的产生。后端服务器通过log4j产生格式化的日志供下游采集。</p>\n<p>这两类日志的采集都使用Flume完成，但是应用不同配置的Flume agent来完成采集。首先两种agent都最重要将日志数据传送给kafka消息队列，但是应为不同的topic，此外对于不同的页面行为日志也要是不同的topic。对于浏览日志，Flume agent使用avro source读取access.log中新增的日志信息，并通过kafka sink传至下游。对于行为日志统一使用一个Flume agent进行采集，使用avro source监听特定的端口，由后端服务器配置log4j发送至指定端口。此外还需要Flume对所有因为日志按照行为的种类进行分流，使下游产生不同topic的消息。</p>\n<h4 id=\"4-1-2日志数据产生与采集模块实现\"><a href=\"#4-1-2日志数据产生与采集模块实现\" class=\"headerlink\" title=\"4.1.2日志数据产生与采集模块实现\"></a>4.1.2日志数据产生与采集模块实现</h4><ul>\n<li>页面浏览日志产生</li>\n</ul>\n<p>access.log使用python脚本产生，使用linux的crontab生成定时任务，每个一分钟产生一次access.log。关键代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = open(<span class=\"string\">\"/home/hadoop/data/project/logs/access.log\"</span>,<span class=\"string\">\"w+\"</span>)  </span><br><span class=\"line\">    <span class=\"keyword\">while</span> count &gt;= <span class=\"number\">1</span>:   </span><br><span class=\"line\">        query_log = <span class=\"string\">\"&#123;ip&#125;\\t&#123;local_time&#125;\\t\\\"GET /&#123;url&#125; HTTP/1.1\\\"\\t&#123;status_code&#125;\\t&#123;referer&#125;\"</span>.format(url=sample_url(), ip=sample_ip(), referer=sample_referer(), status_code=sample_status_code(),local_time=time_str)  </span><br><span class=\"line\">        f.write(query_log + <span class=\"string\">\"\\n\"</span>)  </span><br><span class=\"line\">        count = count - <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>最终access.log产生的数据结构如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">187.156</span>.143.55  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/128.html HTTP/1.1\"</span>  <span class=\"number\">404</span> -  </span><br><span class=\"line\"><span class=\"number\">87.187</span>.167.124  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/128.html HTTP/1.1\"</span>  <span class=\"number\">500</span> -  </span><br><span class=\"line\"><span class=\"number\">156.29</span>.187.168  <span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">15</span>:<span class=\"number\">48</span>:<span class=\"number\">01</span> <span class=\"string\">\"GET /music/112.html HTTP/1.1\"</span>  <span class=\"number\">200</span> http:<span class=\"comment\">//www.baidu.com/s?wd=新近发布</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>页面行为日志产生</li>\n</ul>\n<p>前端埋点的方式由于技术选型的不同有很多方式，本文设计使用的是前后端分离的开发模式，所以在前端要记录行为操作数据的部分编写相应的js代码，调用后端业务接口。后端服务器处理接口调用请求的同时使用log4j产生日志数据。具体实现如下：</p>\n<p>前端埋点js代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//播放歌曲记录  </span></span><br><span class=\"line\">    songplay(songInfo)&#123;  </span><br><span class=\"line\">        axios.get(Utils.defaultURIdefaultURI+<span class=\"string\">\"/actionLogger\"</span>, &#123; </span><br><span class=\"line\">            params:&#123;  </span><br><span class=\"line\">                K_topic: <span class=\"string\">'minions_songplay'</span>,<span class=\"comment\">//行为种类  </span></span><br><span class=\"line\">                songId: songInfo.songID<span class=\"comment\">//该行为日志需要记录的信息  </span></span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;).then(function (response) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.data === <span class=\"number\">1</span>)&#123;  </span><br><span class=\"line\">                message.success(<span class=\"string\">'添加歌曲播放记录成功'</span>);  </span><br><span class=\"line\">                console.log(<span class=\"string\">\"日志记录成功\"</span>)  </span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> console.log(<span class=\"string\">\"日志记录错误\"</span>)  </span><br><span class=\"line\">        &#125;).<span class=\"keyword\">catch</span>(function (error) &#123;  </span><br><span class=\"line\">            console.log(error)  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>后端接口日志产生：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span>  </span><br><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span>  </span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ActionLogController</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Logger logger = Logger.getLogger(ActionLogController.class.getName());  </span><br><span class=\"line\">    <span class=\"meta\">@GetMapping</span>(<span class=\"string\">\"actionLogger\"</span>)  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">actionLogger</span><span class=\"params\">(HttpServletRequest request)</span> <span class=\"keyword\">throws</span> UnsupportedEncodingException </span>&#123;  </span><br><span class=\"line\">        <span class=\"keyword\">int</span> res = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        String k_topic = request.getParameter(<span class=\"string\">\"K_topic\"</span>);  </span><br><span class=\"line\">        String songID = request.getParameter(<span class=\"string\">\"songId\"</span>);  </span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;  </span><br><span class=\"line\">            logger.info(<span class=\"string\">\"topic:\"</span> + k_topic + <span class=\"string\">\" songID:\"</span> + songID);  </span><br><span class=\"line\">            res = <span class=\"number\">1</span>;  </span><br><span class=\"line\">        &#125;<span class=\"keyword\">catch</span> (Exception e)&#123;  </span><br><span class=\"line\">            logger.error(<span class=\"string\">\"error:\"</span> + e);  </span><br><span class=\"line\">            e.printStackTrace();  </span><br><span class=\"line\">            res = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> res;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>由于需要与Flume对接，log4j的配置如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender  </span><br><span class=\"line\">log4j.appender.flume.Hostname = hadoop000  </span><br><span class=\"line\">log4j.appender.flume.Port = <span class=\"number\">41415</span>  </span><br><span class=\"line\">log4j.appender.flume.UnsafeMode = <span class=\"keyword\">true</span>  </span><br><span class=\"line\">log4j.appender.flume.layout=org.apache.log4j.PatternLayout  </span><br><span class=\"line\">log4j.appender.flume.layout.ConversionPattern= %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure>\n<p>这样后端产生log4j日志则会向运行着Flume机器的41415端口发送。最终发送的日志数据格式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2019</span>-<span class=\"number\">05</span>-<span class=\"number\">10</span> <span class=\"number\">16</span>:<span class=\"number\">23</span>:<span class=\"number\">40</span>,<span class=\"number\">743</span> [http-nio-<span class=\"number\">8080</span>-exec-<span class=\"number\">6</span>] [com.chaoyue.minions.controller.ActionLogController] [INFO] - topic:minions_songplay songID:<span class=\"number\">17</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Flume配置方案</li>\n</ul>\n<p>采集页面浏览日志的Flume agent使用命令从access.log文件中读取新增加的浏览日志数据。注意需要agent的channel选择memory-channel，需要将缓存的容量（capacity，默认值为100）设置一个较大的值，否则当日志数据过多时，会出现异常。本文设计memory-channel.capacity设置为10000。此外使用kafka-sink，将日志分发到指定topic的kafka消息队列中，本文设计页面浏览日志将被分发至topic为streamingtopic的消息队列中。关键配置项如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exec-memory-kafka.sources.exec-source.type = exec  </span><br><span class=\"line\">exec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/data/project/logs/access.log  </span><br><span class=\"line\">  </span><br><span class=\"line\">exec-memory-kafka.channels.memory-channel.capacity = <span class=\"number\">10000</span>  </span><br><span class=\"line\">exec-memory-kafka.channels.memory-channel.transactionCapacity = <span class=\"number\">10000</span> </span><br><span class=\"line\">  </span><br><span class=\"line\">exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class=\"line\">exec-memory-kafka.sinks.kafka-sink.topic = streamingtopic</span><br></pre></td></tr></table></figure>\n<p>页面行为大类的日志使用同一的Flume agent采集，使用avro-source监听特定端口发来的日志消息数据。为了能够将不同的行为分流，需要使用过滤器interceptors将日志数据中用于分流的标记提取出来，在交给kafka-sink分发至不同topic的消息队列中。interceptor使用正则表达式将日志数据中的标记提取出来命名为topic，在kafka-sink的配置中使用topic即可。interceptor配置如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#定义source中的过滤器  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors=i1  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.type=regex_extractor  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.regex=topic:(.*?) songID:(.*?)  </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers=s1 s2   </span><br><span class=\"line\">agent1.sources.avro-source.interceptors.i1.serializers.s1.name=topic  </span><br><span class=\"line\">  </span><br><span class=\"line\">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink  </span><br><span class=\"line\">agent1.sinks.kafka-sink.topic = %&#123;topic&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-2日志处理、分析与持久化模块\"><a href=\"#4-2日志处理、分析与持久化模块\" class=\"headerlink\" title=\"4.2日志处理、分析与持久化模块\"></a>4.2日志处理、分析与持久化模块</h3><h4 id=\"4-2-1日志处理、分析与持久化模块设计\"><a href=\"#4-2-1日志处理、分析与持久化模块设计\" class=\"headerlink\" title=\"4.2.1日志处理、分析与持久化模块设计\"></a>4.2.1日志处理、分析与持久化模块设计</h4><p>使用Spark Streaming和HBase完成日志数据的处理、分析和持久化。首先不同类型的日志需要用不同的Spark Streaming应用作业来处理，因为不同的日志分析的逻辑要求是不同的（如处理时间的要求），结构也是不同的，持久化的策略也是不同的。为了便于程序的可靠运行，减少不同类型日志之间处理逻辑的耦合情况，需要将日志处理分析的逻辑按照kafka topic进行划分。日志实时流数据可以根据日志数据结构的字段进行过滤，因为日志数据是结构化的，也是可以定制的。使用HBase进行持久化，一方面支持大量数据的存储，同时还能够动态的修改表结构，增加需要记录的字段十分方便。HBase的表根据时间、类目等主要维度进行设计。</p>\n<h4 id=\"4-2-2日志处理、分析与持久化模块实现\"><a href=\"#4-2-2日志处理、分析与持久化模块实现\" class=\"headerlink\" title=\"4.2.2日志处理、分析与持久化模块实现\"></a>4.2.2日志处理、分析与持久化模块实现</h4><ul>\n<li>应用初始化与流数据获取</li>\n</ul>\n<p>每个Spark Streaming应用作业与一个Kafka topic相对应。创建Spark Streaming处理上下文，指定实时流处理的时间间隔，并从Kafka消息队列中消费对应topic的日志数据，如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">val <span class=\"title\">Array</span><span class=\"params\">(zkQuorum, group, topics, numThreads)</span> </span>= args  </span><br><span class=\"line\">val sparkConf = <span class=\"keyword\">new</span> SparkConf().setAppName(<span class=\"string\">\"MyStreamingApp\"</span>).setMaster(<span class=\"string\">\"local[5]\"</span>)  </span><br><span class=\"line\">val ssc = <span class=\"keyword\">new</span> StreamingContext(sparkConf, Seconds(<span class=\"number\">60</span>))  </span><br><span class=\"line\">val topicMap = topics.split(<span class=\"string\">\",\"</span>).map((_, numThreads.toInt)).toMap  </span><br><span class=\"line\">val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>日志数据模板化与清洗</li>\n</ul>\n<p>以上就根据日志分析的需求得到了实时的日志流数据，接下来就是对这些日志数据进行必要的过滤。首先对于浏览日志需要将所有不成功的请求清洗掉，保留成功的请求日志。由于浏览日志的数据结构是定制化的，所以我们可以根据特定字段进行清洗，本文设计首先要将每一条数据转换成Scala的模板类对象，后续对每一条数据的操作就是针对这个模板类对象完成的。其次还要将请求状态不成功的日志数据过滤掉。方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val logs = messages.map(_._2)  </span><br><span class=\"line\">val cleanData = logs.map(line =&gt; &#123;  </span><br><span class=\"line\">  val infos = line.split(<span class=\"string\">\"\\t\"</span>)<span class=\"comment\">//把每行日志根据\\t分隔符分割  </span></span><br><span class=\"line\">  val url = infos(<span class=\"number\">2</span>).split(<span class=\"string\">\" \"</span>)(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  <span class=\"comment\">// url为/music/128.html  </span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> itemID = <span class=\"number\">0</span>  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> (url.startsWith(<span class=\"string\">\"/music\"</span>))&#123;<span class=\"comment\">//music开头的把编号拿出来  </span></span><br><span class=\"line\">    val itemIdHTML = url.split(<span class=\"string\">\"/\"</span>)(<span class=\"number\">2</span>)  </span><br><span class=\"line\">    itemID = itemIdHTML.substring(<span class=\"number\">0</span>, itemIdHTML.lastIndexOf(<span class=\"string\">\".\"</span>)).toInt  </span><br><span class=\"line\">  &#125;  </span><br><span class=\"line\">  <span class=\"comment\">//将每一条日志数据转换成Scala中的模板类，并将请求状态为200的日志数据保留  </span></span><br><span class=\"line\">  ClickLog(infos(<span class=\"number\">0</span>), DateUtils.parseToMinute(infos(<span class=\"number\">1</span>)), itemID, infos(<span class=\"number\">3</span>).toInt, infos(<span class=\"number\">4</span>))  </span><br><span class=\"line\">&#125;).filter( clicklog =&gt; clicklog.itemId != <span class=\"number\">0</span> &amp;&amp; clicklog.statusCode == <span class=\"number\">200</span>)</span><br></pre></td></tr></table></figure>\n<p>最终得到的结构化数据为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ClickLog(<span class=\"number\">87.30</span>.187.55,<span class=\"number\">20190219090601</span>,<span class=\"number\">145</span>,<span class=\"number\">200</span>,https:<span class=\"comment\">//www.sogou.com/web?query=今日专辑)</span></span><br></pre></td></tr></table></figure>\n<p>从左到右分别表示请求源地址、时间、请求页面编号、请求状态、来源网站。</p>\n<ul>\n<li>实时统计访问量</li>\n</ul>\n<p>进行业务功能开发时需要兼顾到持久化和后续可视化的可行性。对于访问量的统计需要考虑维度的选择，因为本文设计需要以天为维度和以类目维度记录访问量，即今日到当前时间为止的各类目的访问量和所有页面的访问总量，所以HBase中关于访问量的表中rowkey设计要以日期和类目编号为维度，本文设计访问量表中的rowkey形如20190511_146，列族记录到当前时间为止的访问量。方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cleanData.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+<span class=\"string\">\"_\"</span>+x.itemId, <span class=\"number\">1</span>)<span class=\"comment\">//记录日期与类目编号  </span></span><br><span class=\"line\">    &#125;).reduceByKey(_+_).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[itemClickCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(itemClickCount(pair._1, pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        <span class=\"comment\">//将实时统计得到的类目访问量写入HBase  </span></span><br><span class=\"line\">        itemClickCountDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>HBase中行数据的更新操作使用incrementColumnValue方法完成，这样可以直接将当前时间段得到的访问总量直接与之前统计的访问总量相加，方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">def <span class=\"title\">save</span><span class=\"params\">(list: ListBuffer[itemClickCount])</span>:Unit </span>= &#123;  </span><br><span class=\"line\">    <span class=\"comment\">//单例模式获取对表操作的实例  </span></span><br><span class=\"line\">    val table = HBaseUtils.getInstance().getTable(tableName)  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ele &lt;- list)&#123;<span class=\"comment\">//每一行的统计信息进行写入  </span></span><br><span class=\"line\">     table.incrementColumnValue(Bytes.toBytes(ele.day_item),  </span><br><span class=\"line\">        Bytes.toBytes(cf),  </span><br><span class=\"line\">        Bytes.toBytes(qualifer),  </span><br><span class=\"line\">        ele.click_count)  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>以上就完成了在日期和类目编号维度下的实时访问量统计，HBase中记录的数据结构如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">20190511_130</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220479</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00\\x0B\\xDF </span><br><span class=\"line\"><span class=\"number\">20190511_131</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220479</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x0D          </span><br><span class=\"line\"><span class=\"number\">20190511_145</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220483</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x00-\\xFD</span><br><span class=\"line\"><span class=\"number\">20190511_146</span>              column=info:click_count, timestamp=<span class=\"number\">1557551220481</span>, </span><br><span class=\"line\">value=\\x00\\x00\\x00\\x00\\x00\\x009\\x9C</span><br></pre></td></tr></table></figure>\n<ul>\n<li>实时统计窗口时间内访问量</li>\n</ul>\n<p>以上完成了实时统计访问量的需求，但是使用者还需要从更细化的维度观察使用数据。对于访问量，还需要在一天内以24小时为维度进行统计，也就是展示访问量在一天之内的趋势。可以预测网站在一天内的不同时间访问量是不同的，在某个时段可能有高峰，某个时段可能是低谷，这样网站是运营人员就可以选择在不同的时间完成不同是任务，比如在高峰时加大某项业务的投放力度，在访问比较少时完成新版本的发布等。本文设计使用Spark Streaming的window操作接口，分析窗口时间内所有流数据，完成每30分钟窗口期内类目访问量的统计。window的时间间隔和滑动时间均可自定义。实现方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cleanData.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+<span class=\"string\">\"_\"</span>+x.courseId, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKeyAndWindow((x: Int, y: Int) =&gt; x + y,  </span><br><span class=\"line\">      Seconds(<span class=\"number\">1800</span>), Seconds(<span class=\"number\">1800</span>)).foreachRDD(rdd =&gt; &#123;<span class=\"comment\">//设置窗口大小为1800秒（30分钟）、滑动时间间隔为30分钟  </span></span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[ClickCountTrend]  </span><br><span class=\"line\">        val time = FastDateFormat.getInstance(<span class=\"string\">\"HHmm\"</span>).format(<span class=\"keyword\">new</span> Date())<span class=\"comment\">//窗口时间戳  </span></span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(ClickCountTrend(  </span><br><span class=\"line\">            pair._1.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>)+time+pair._1.substring(<span class=\"number\">8</span>), </span><br><span class=\"line\">            pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        ClickCountTrendDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>以上就实现了每30分钟统计一次各类目在前30分钟内的访问量，对这个数据的积分即是当前时间的总访问量。HBase中持久化的rowkey结构为201905111337_146。这样就能够详细的观察访问量在一天中趋势，以及每个类目访问的趋势。但是目前这个实现方案有一个问题就是，当30分钟窗口时间跨越0点时，会丢失前一天的数据。比如，窗口期为5月10日23：45至11日0：15，由于rowkey设计的原因前15分钟的数据会计算到10日0：15的数据上。为了解决这个问题可以让应用任务在整点时启动。但是解决方案还是需要寻找的。</p>\n<p>访问量趋势的可视化展示参见可视化实现部分。</p>\n<ul>\n<li>来源网站实时统计</li>\n</ul>\n<p>针对从搜索引擎来的流量，需要统计来源网站和搜索关键词，这样网站可以进行SEO（搜索引擎优化），提高网站竞争力。对于来源网站的统计依然需要从清洗处理过后的模板数据中完成，这个维度的表中的rowkey以日期、来源网站和类目编号为维度。对于搜索关键词的统计也是以日期为维度，统计当前日期的热门搜索词。</p>\n<ul>\n<li>页面行为日志分析</li>\n</ul>\n<p>对于页面上各种不同的行为日志数据，为了便于后续的处理分析，都需要先将数据格式化为模板类对象。之后再进行日志数据的分析和持久化。以音乐播放为例，模板化方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"comment\">//messages为kafka产生的消息数据  </span></span><br><span class=\"line\">  val logs = messages.map(_._2)  </span><br><span class=\"line\">  val songPlayLog = logs.map(line =&gt; &#123;  </span><br><span class=\"line\">  \tval infos = line.split(<span class=\"string\">\" \"</span>)  </span><br><span class=\"line\">  \tval time = DateUtils.parseToMinute(infos(<span class=\"number\">0</span>)+<span class=\"string\">\" \"</span>+infos(<span class=\"number\">1</span>))  </span><br><span class=\"line\">  \tval UID = infos(<span class=\"number\">1</span>).split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).dropRight(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  \tval songID = infos.last.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).dropRight(<span class=\"number\">1</span>)  </span><br><span class=\"line\">  \tSongPlayLog(time, UID, songID) <span class=\"comment\">//最终音乐播放行为模板类结构SongPlayLog(20190401125947, 00001, 15)  </span></span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>对于不同行为日志数据的分析需要根据实际业务需求进行个性化设计。同时还要注意使用实时流处理时要避免复杂的连接操作，实时流处理可以选择记录维度较简单、在原始数据上进行简单抽象的数据结果，后续复杂的分析和连接操作可以放在对实时性要求不高的流程中完成。一下以音乐播放行为为例，进行两个维度的数据分析与持久化。首先统计至当前时间的音乐播放量，方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">songPlayLog.map(x =&gt; &#123;  </span><br><span class=\"line\">    \t  <span class=\"comment\">//rowkey设计为日期与歌曲编号维度  </span></span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>) + <span class=\"string\">\"_\"</span> + x.songID, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKey(_+_).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[SongPlayDailyCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(SongPlayDailyCount(pair._1, pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        SongPlayDailyCountDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>同样地对于音乐播放是会有实时热点产生，所以需要实时统计窗口时间内的歌曲播放数据，并在较短时间内更新，以响应热点。首先需要进行HBase表设计，本文设计对于时间段内歌曲播放这类热点数据只保留最新时间的分析结果即可。其次，本文设计针对近一个小时内的歌曲播放进行统计，滑动间隔为10分钟，即每十分钟更新一次近一个小时的歌曲播放数据。在进行HBase表创建时需要设置表中数据的存活时间TTL，本文设计设置为600s（10分钟），即表中数据产生十分钟后失效，最新的数据计算后会被写入表中。分析数据方法如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">songPlayLog.map(x =&gt; &#123;  </span><br><span class=\"line\">      (x.time.substring(<span class=\"number\">0</span>,<span class=\"number\">8</span>) + <span class=\"string\">\"_\"</span> + x.songID, <span class=\"number\">1</span>)  </span><br><span class=\"line\">    &#125;).reduceByKeyAndWindow((x: Int, y: Int) =&gt; x + y,  </span><br><span class=\"line\">      Seconds(<span class=\"number\">3600</span>), Seconds(<span class=\"number\">600</span>)).foreachRDD(rdd =&gt; &#123;  </span><br><span class=\"line\">      rdd.foreachPartition(partitionRecords =&gt; &#123;  </span><br><span class=\"line\">        val list = <span class=\"keyword\">new</span> ListBuffer[RecentlySongPlayCount]  </span><br><span class=\"line\">        partitionRecords.foreach(pair =&gt; &#123;  </span><br><span class=\"line\">          list.append(RecentlySongPlayCount(pair._1,  </span><br><span class=\"line\">            pair._2))  </span><br><span class=\"line\">        &#125;)  </span><br><span class=\"line\">        RecentlyPlaySongDAO.save(list)  </span><br><span class=\"line\">      &#125;)  </span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>以上即得到了实时的歌曲播放统计数据，以及时间段内热点歌曲的播放统计。其他的行为日志的分析需要根据实际需求来实现，本文设计还有歌曲收藏和歌曲评论两个行为日志分析，分别从其他不同的角度反映网站内容数据。</p>\n<h4 id=\"4-2-3-Spark-Streaming应用提交集群机器执行\"><a href=\"#4-2-3-Spark-Streaming应用提交集群机器执行\" class=\"headerlink\" title=\"4.2.3 Spark Streaming应用提交集群机器执行\"></a>4.2.3 Spark Streaming应用提交集群机器执行</h4><p>将编写好的Spark Streaming作业程序编写好后使用mvn编译，将编译好的jar包拷贝至集群的机器上，在使用spark-submit指令进行任务的提交，指令如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-submit --master local[<span class=\"number\">5</span>] \\  </span><br><span class=\"line\">--jars $(echo /home/hadoop/app/hbase-<span class=\"number\">1.2</span>.0-cdh5.7.0/lib<span class=\"comment\">/*.jar | tr ' ' ',') \\ </span></span><br><span class=\"line\"><span class=\"comment\">--class com.chaoyue.spark.project.scala.MyStreamingApp \\  </span></span><br><span class=\"line\"><span class=\"comment\">--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \\  </span></span><br><span class=\"line\"><span class=\"comment\">/home/hadoop/lib/sparktrain-1.0.jar \\  </span></span><br><span class=\"line\"><span class=\"comment\">hadoop000:2181 test streamingtopic 1</span></span><br></pre></td></tr></table></figure>\n<p>此时Spark Streaming作业即运行在集群上了，可以再控制台查看运行日志如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO ShuffledDStream: Time <span class=\"number\">1557649980000</span> ms is invalid as zeroTime is <span class=\"number\">1557649260000</span> ms , slideDuration is <span class=\"number\">1800000</span> ms and difference is <span class=\"number\">720000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO JobScheduler: Starting job streaming job <span class=\"number\">1557649980000</span> ms.0 from job set of time <span class=\"number\">1557649980000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO SparkContext: Starting job: foreachPartition at MyStreamingApp.scala:<span class=\"number\">59</span>  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO JobScheduler: Added jobs <span class=\"keyword\">for</span> time <span class=\"number\">1557649980000</span> ms  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO DAGScheduler: Registering RDD <span class=\"number\">115</span> (map at MyStreamingApp.scala:<span class=\"number\">56</span>)  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO MemoryStore: Block broadcast_43 stored   </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">00</span> INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on <span class=\"number\">192.168</span>.1.108:<span class=\"number\">42189</span> (size: <span class=\"number\">1869.0</span> B, free: <span class=\"number\">366.1</span> MB)  </span><br><span class=\"line\">...  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">05</span> INFO BlockGenerator: Pushed block input-<span class=\"number\">0</span>-<span class=\"number\">1557649985600</span>  </span><br><span class=\"line\"><span class=\"number\">19</span>/<span class=\"number\">05</span>/<span class=\"number\">12</span> <span class=\"number\">16</span>:<span class=\"number\">33</span>:<span class=\"number\">08</span> INFO MemoryStore: Block input-<span class=\"number\">0</span>-<span class=\"number\">1557649988600</span> <span class=\"function\">stored as bytes in <span class=\"title\">memory</span> <span class=\"params\">(estimated size <span class=\"number\">305.0</span> B, free <span class=\"number\">365.9</span> MB)</span>  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 INFO BlockManagerInfo: Added input-0-1557649988600 in memory on 192.168.1.108:42189 <span class=\"params\">(size: <span class=\"number\">305.0</span> B, free: <span class=\"number\">366.0</span> MB)</span>  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 WARN BlockManager: Block input-0-1557649988600 replicated to only 0 <span class=\"title\">peer</span><span class=\"params\">(s)</span> instead of 1 peers  </span></span><br><span class=\"line\"><span class=\"function\">19/05/12 16:33:08 INFO BlockGenerator: Pushed block input-0-1557649988600</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3分析结果可视化模块\"><a href=\"#4-3分析结果可视化模块\" class=\"headerlink\" title=\"4.3分析结果可视化模块\"></a>4.3分析结果可视化模块</h3><h4 id=\"4-3-1分析结果可视化模块设计\"><a href=\"#4-3-1分析结果可视化模块设计\" class=\"headerlink\" title=\"4.3.1分析结果可视化模块设计\"></a>4.3.1分析结果可视化模块设计</h4><p>可视化部分采用前后端分离的开发模式，最终通过web网页的方式展示可视化结果。可视化模块不只是数据库中数据的展示，还要完成在实时流处理场景中不适合实时处理的其他维度的操作，即完成上个模块没有完成的复杂维度的分析。例如实时流处理中记录的歌曲播放信息只包括时间戳、用户ID和歌曲ID，但是歌曲本身的信息是丰富的，要展示的维度也是丰富的，所以在可视化模块进行复杂的连接操作，完成最终更丰富的分析结果展示。</p>\n<p>前端使用react为基础框架，react组件的思想非常适合复用，对于不同数据进行相同类型的展示时可以更换数据源复用组件。此外使用蚂蚁金服的开源react UI组件库ant-design和ant-design Pro，以及商业场景下的数据可视化解决方案Bizcharts，完成丰富的数据可视化展示。后端使用SpringBoot透出相关接口，给前端组件提供需要展示的数据，此外后端还负责实时流处理过程中没有完成的连接操作和其他处理数据的操作。</p>\n<h4 id=\"4-3-2分析结果可视化模块实现\"><a href=\"#4-3-2分析结果可视化模块实现\" class=\"headerlink\" title=\"4.3.2分析结果可视化模块实现\"></a>4.3.2分析结果可视化模块实现</h4><ul>\n<li>可视化web界面概览</li>\n</ul>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368878922/0\" alt></p>\n<p>如4.1所示，今日数据概览页面展示当日实时数据，包括：实时PageView、七日PV走势、今日PV走势，各类目实时访问统计、各类目访问趋势、今日来源网站统计和今日热搜词云。页面根据维度逐渐细化来布局，从上到下从左到右，首先是PV的总量，然后从一天中的时间为维度和类目为维度细化PV值，再到其他杂项指标。今日PV走势组件展示了网站访问在一天内的趋势，每30分钟统计30分钟内访问量，可以看出音乐网站的用户群体在中午和傍晚到晚上十分活跃。类目实时访问统计展示了用户对哪一个子类目更偏爱，哪些类目需要优化。七日PV比较给一周内的PV比较，可以大致看到某一天的网站浏览状况，需要历史日期更细化的数据可以到历史数据查询里查看。类目访问趋势展示了各类目的今日访问趋势，是前面两个维度的横向和纵向细化，帮助网站运营人员从细节查看网站数据。杂项数据包括来源网站统计和热搜词云，这些展示的是从搜索引擎过来的流量的分析，根据搜索引擎的热搜词，可以帮助网站进行SEO（搜索引擎优化），提高网站排名。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559368954251/0\" alt></p>\n<p>如图4.2所示，历史数据查询提供选择日期的组件，可以查看所选日期当天的相关数据。提供30天PV比较，可以看到更长时间区间内网站浏览量的变化，通过这些变化的观察可以对网站的运营策略进行适当的调整。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369327550/0\" alt></p>\n<p>图4.3是音乐网站行为的模拟，具有典型的歌曲资源位组件，可以模拟的行为有歌曲播放、收藏和评论。点击按钮则会调用后端相关接口，后端通过log4j记录下行为日志数据。一个歌曲资源包括歌名、歌手、专辑、封面地址、音乐资源地址、风格和标签等，使用MySQL保存这些资源信息。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369422528/0\" alt></p>\n<p>图4.4是页面行为日志数据的实时处理分析展示页面。展示的内容包括，今日歌曲播放一览、近一小时最热播放歌曲、播放歌曲类型统计、今日加权热门歌曲、今日歌曲评论和收藏统计。今日歌曲播放一览展示今日实时的歌曲播放量统计。近一小时最热播放歌曲展示一小时窗口时间的歌曲播放量排行，是前一个统计维度的子集。歌曲播放类型统计使用南丁格尔玫瑰花环展示实时歌曲播放不同类型数量的统计，这个维度即是对实时流处理中得到的分析结果进行更高一级的抽象，在可视化的后端将歌曲播放量与歌曲资源进行连接操作得到歌曲类型统计。今日加权热门歌曲通过将三中行为的统计量进行加权，得到热门歌曲，加权权值为播放0.5：收藏1：评论0.8。另外两个展示的是实时歌曲收藏量与评论量。</p>\n<ul>\n<li>可视化部分详细展示</li>\n</ul>\n<p>下面将上节涉及的主要功能组件进行展示。</p>\n<ul>\n<li>页面实时浏览量</li>\n</ul>\n<p>实时展示网站今日总浏览量，周同比展示今日与前一周当日的浏览量百分比，日环比展示与前一日的浏览量百分比，日均展示一周七天的浏览量均值。如图4.5所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369535329/0\" alt></p>\n<ul>\n<li>七日浏览量走势</li>\n</ul>\n<p>展示七天前至当日的浏览量走势，如图4.6所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369649506/0\" alt></p>\n<ul>\n<li>今日实时浏览量走势</li>\n</ul>\n<p>以半小时为维度展示今日浏览量数据，每隔半小时统计半小时窗口期中的浏览量，能够清晰展示浏览量在一天中的变化。如图4.7所示，可以看出在凌晨时分访问量逐渐减少，到了早上访问量开始逐渐上升。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369685204/0\" alt></p>\n<ul>\n<li>各类目实时浏览量统计</li>\n</ul>\n<p>以网站不同类目页面为维度进行浏览量数据以及占比的实时展示，音乐网站下设6个大类目：瞩目艺人、最近播放、今日专辑、为你推荐、今日歌单和新近发布。如图4.8所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369719484/0\" alt></p>\n<ul>\n<li>今日各类目访问趋势</li>\n</ul>\n<p>以时间与类目为维度进行浏览量数据的展示，是前两个数据展示组件的细化。如图4.9所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369747929/0\" alt></p>\n<ul>\n<li>来源网站实时统计</li>\n</ul>\n<p>展示今日来源网站的数据统计，如从百度搜索进入主站则将百度搜索记录下来。如图4.10所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369788944/0\" alt></p>\n<ul>\n<li>今日热搜词云</li>\n</ul>\n<p>从搜索引擎进入主站时所进行的搜索词数据统计，如在搜索引擎中搜索“欧美音乐”进入主站，则对“欧美音乐”这个词条进行一次记录，用于搜索引擎优化。如图4.11所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369815950/0\" alt></p>\n<ul>\n<li>月度浏览量比较</li>\n</ul>\n<p>以月为维度对浏览量数据进行展示比较。如图4.12所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369855984/0\" alt></p>\n<ul>\n<li>页面行为模拟模块</li>\n</ul>\n<p>音乐网站页面用户行为的模拟组件，可模拟音乐播放、收藏、评论三种行为，在用户进行页面行为时则会进行日志的记录。如图4.13所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369965341/0\" alt></p>\n<ul>\n<li>今日歌曲播放一览</li>\n</ul>\n<p>今日实时歌曲播放量实时数据展示，如图4.14所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559369987974/0\" alt></p>\n<ul>\n<li>近一小时播放Top歌曲</li>\n</ul>\n<p>近一小时窗口时间内歌曲播放量展示，每十分钟进行一小时窗口时间播放量的统计，展示当下实时热播歌曲。如图4.15所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370044063/0\" alt></p>\n<ul>\n<li>播放音乐类型统计</li>\n</ul>\n<p>以音乐风格为维度进行歌曲播放量数据实时展示，如图4.16所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370078824/0\" alt></p>\n<ul>\n<li>多权重今日热门歌曲</li>\n</ul>\n<p>从播放、收藏和评论三个维度进行不同权重的加权计算，得出今日热门歌曲进行展示，可以设计更复杂的加权算法得到热门歌曲，便于运营人员推送歌曲内容。如图4.17所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370164164/0\" alt></p>\n<ul>\n<li>今日歌曲收藏一览</li>\n</ul>\n<p>今日实时歌曲收藏量数据展示，如图4.18所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1359855289_1559370195372/0\" alt></p>\n<ul>\n<li>今日歌曲评论一览</li>\n</ul>\n<p>今日实时歌曲评论量数据展示，如图4.19所示。</p>\n<p><img src=\"https://puui.qpic.cn/fans_admin/0/3_1599777765_1559370221681/0\" alt></p>\n<h3 id=\"4-4-本章小结\"><a href=\"#4-4-本章小结\" class=\"headerlink\" title=\"4.4 本章小结\"></a>4.4 本章小结</h3><p>本章在系统需求分析与架构设计的基础上，分模块进行了详细的设计与实现，给出了每个部分的核心代码与解释，并在可视化模块介绍了各个分析数据的展现形式以及其中的意义。</p>\n<h2 id=\"5总结与展望\"><a href=\"#5总结与展望\" class=\"headerlink\" title=\"5总结与展望\"></a>5总结与展望</h2><h3 id=\"5-1-总结\"><a href=\"#5-1-总结\" class=\"headerlink\" title=\"5.1 总结\"></a>5.1 总结</h3><p>人类可以基于各种信息进行判断、决策，最终创造出价值，而数据作为信息的载体，正在被大量产生。然而数据的形式是多样的，而且是分散的，如何将分布的数据聚合并分析提取出有价值的信息，是大数据时代的挑战。海量数据分析作为互联网行业的闭环模式中的一环，其价值和重要性已经被广泛关注，越来越多的企业可是设立数据部门。一个网站的各个环节都会产生日志数据，这些数据形式多样且分散在不同机器实体上，如何高效采集、聚合、处理、分析和持久化给大数据分析带来了挑战。而现如今，更多的大数据分析需求还要加上“实时处理”这个需求。使用Spark作为分布式数据处理工具，整合海量日志采集工具Flume、可靠消息队列Kafka和海量数据存储HBase，满足了网站日志实时流处理的需求。Flume可定制化的从多种数据源分部式地采集日志，同时具备一定的数据处理能力，可以向多种下游数据目的地传输日志数据。消息队列Kafka保证了海量的日志数据能够有序、不丢失的被消费，同时运用topic机制使得在上游聚合的日志数据以不同的策略被处理和分析。Spark Streaming是整个系统的核心，其是在Spark Core之上的高级API，为整个数据处理流程提供了实时流处理的上下文。Spark的计算模型核心是弹性分布式数据集RDD，提供个基于内存的集群计算的容错性抽象。Spark Streaming的实时流处理能力是基于DStream，其是RDD的一个在时间和空间上的高层抽象，使得我们可以像处理RDD一样处理DStream。同时Spark Streaming支持从多种数据源创建DStream。可以说，Spark Streaming提供了基础实时流处理的解决方案。HBase提供了基于列的海量数据存储，不同于关系型数据库，HBase在大数据处理分析数据持久化方面有许多优点。本文基于Spark Streaming的音乐网站实时流处理，使用上述提及的几个工具和解决方案，同时使用react和SpringBoot进行了分析数据的可视化展示。</p>\n<p>本设计完成的主要成果如下：</p>\n<p>1）使用Flume实时采集服务器上access.log中的网页浏览日志和使用前端埋点采集到的页面行为日志。解决了分布式日志采集和聚合的问题，同时对日志进行了分流以供下游Kafka使用。</p>\n<p>2）使用消息队列Kafka解决大量日志数据传输问题，并为下游数据处理分析Spark Streaming提供实时日志数据源。使用Spark Streaming进行多种类型日志数据的实时处理分析，达到秒级处理和阶段数据处理的能力。在实时流处理中分析出基础业务指标数据，以供上层最这些数据进行其他复杂的操作，并将这些分析结果数据持久化至HBase中。</p>\n<p>3）使用React和SpringBoot构建前后端分离项目，同时在后端结合MySQL完成数据分析上层复杂的连接操作。最终从今日数据概览、历史数据查询和页面行为分析模拟三方面进行展示。分析数据展示包括：实时网站访问量、当日实时访问量走势、周和月维度访问量比较、各类目实时访问量及走势、来源网站统计、搜索引擎热搜词统计、歌曲实时播放量、近一小时热门歌曲播放量、歌曲播放类型比较、加权热门歌曲、实时歌曲收藏和评论统计。</p>\n<p>本文系统提供的数据可以帮助网站的管理、运营和开发维护人员更好地优化网站，比如优化网站类目内容、优化搜索引擎结果以及选择最合适的时间升级网站服务等。</p>\n<h3 id=\"5-2-展望\"><a href=\"#5-2-展望\" class=\"headerlink\" title=\"5.2 展望\"></a>5.2 展望</h3><p>实时流处理不仅仅是进行简单的数据分析，还有许多实时响应的场景下有更复杂的需求。比如，电商搜索推荐系统在搜索导购的各个场景都需要能够秒级召回个性化推荐数据，而这需要大量的离线和实时数据处理相结合和完成。Spark作为分布式数据处理框架，不仅提供了秒级实时流处理的高级抽象Spark Streaming，还提供了用于机器学习的MLlib、图处理的GraphX、交互式查询SparkSQL，对这些高级抽象进行整合应用可以处理更复杂场景下的需求。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjy1diwa5000048ups53lj72k","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwar000748up3ffkbyjc"},{"post_id":"cjy1diwad000148up1ktf88xv","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwas000948up7pzg5c51"},{"post_id":"cjy1diwd6000j48up0kt0aij4","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwdv000q48updjwxlgsn"},{"post_id":"cjy1diwch000c48uphen7v88y","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwe4000t48uptp8et8b8"},{"post_id":"cjy1diwdi000l48upmu5ixpaa","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwe9000w48up4ppulgux"},{"post_id":"cjy1diwcq000e48up4fimq95r","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diweh000z48up4u47ocmb"},{"post_id":"cjy1diwds000p48upxsp2bzec","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwel001148upehagwviw"},{"post_id":"cjy1diwe2000s48up35q1gm6o","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwex001548upfg8cjbnv"},{"post_id":"cjy1diwcx000h48upxz1if1ym","category_id":"cjy1diwdy000r48uposf4iov0","_id":"cjy1diwf6001848uptv6mn520"},{"post_id":"cjy1diwe6000u48upww8b8t15","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwff001b48upb8rl9bbj"},{"post_id":"cjy1diwdo000n48upze803e9e","category_id":"cjy1diwdy000r48uposf4iov0","_id":"cjy1diwfk001e48upr6s9cird"},{"post_id":"cjy1diwej001048upllvs4fby","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwft001i48upgcbdugry"},{"post_id":"cjy1diwer001348upa10cchxy","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwg0001l48upj0wlnq8s"},{"post_id":"cjy1diwf3001648upcidqoanj","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwg8001p48up6c9q62qz"},{"post_id":"cjy1diwfc001a48upuevklzch","category_id":"cjy1diwfn001f48upqbwwl1h8","_id":"cjy1diwgi001u48up6c7lblt5"},{"post_id":"cjy1diwg2001n48up6zwpnkvl","category_id":"cjy1diwfn001f48upqbwwl1h8","_id":"cjy1diwgm001v48upqdj5hgty"},{"post_id":"cjy1diwge001r48up7bo3mxlv","category_id":"cjy1diwfn001f48upqbwwl1h8","_id":"cjy1diwgp001w48upnu65cjch"},{"post_id":"cjy1diwfh001d48upj0lvysn3","category_id":"cjy1diwfn001f48upqbwwl1h8","_id":"cjy1diwgr001x48upzdjucsgf"},{"post_id":"cjy1diwfp001g48up6vnjz5hy","category_id":"cjy1diwfn001f48upqbwwl1h8","_id":"cjy1diwgt001y48up7bez3qfe"},{"post_id":"cjy1diwi7002048upmdlwgy0i","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwib002348upwwa80jp2"},{"post_id":"cjy1diwi5001z48upx46yqewc","category_id":"cjy1diwi8002148upe1vmo6bq","_id":"cjy1diwid002548upgtarhhfl"},{"post_id":"cjy1diwj4002848upzw07sxip","category_id":"cjy1diwah000248updve23zlm","_id":"cjy1diwj7002a48upezan9njr"},{"post_id":"cjy1diwk8002c48upz0se2o00","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwkd002e48up1o6go57e"},{"post_id":"cjy1diwkr002i48upomlhg5cp","category_id":"cjy1diwcv000g48upvqmtx0um","_id":"cjy1diwkt002j48upuc1r0dg8"}],"PostTag":[{"post_id":"cjy1diwa5000048ups53lj72k","tag_id":"cjy1diwak000348upmcbpzc06","_id":"cjy1diwaq000648uptxdpfqcx"},{"post_id":"cjy1diwad000148up1ktf88xv","tag_id":"cjy1diwan000548upf3kw5fj3","_id":"cjy1diwat000a48up25atmacz"},{"post_id":"cjy1diwad000148up1ktf88xv","tag_id":"cjy1diwar000848up6kdtsgc4","_id":"cjy1diwau000b48upkh7id033"},{"post_id":"cjy1diwdi000l48upmu5ixpaa","tag_id":"cjy1diwdr000o48upn66qaqzp","_id":"cjy1diwex001448upu15ipkvg"},{"post_id":"cjy1diwdi000l48upmu5ixpaa","tag_id":"cjy1diwe8000v48upc3bqacjt","_id":"cjy1diwf5001748up7q3q9whp"},{"post_id":"cjy1diwe2000s48up35q1gm6o","tag_id":"cjy1diwem001248upozhzkx4e","_id":"cjy1diwfg001c48upqsnw8dxk"},{"post_id":"cjy1diwer001348upa10cchxy","tag_id":"cjy1diwan000548upf3kw5fj3","_id":"cjy1diwfx001j48up0q82u8xj"},{"post_id":"cjy1diwer001348upa10cchxy","tag_id":"cjy1diwf8001948up2zwrfk5e","_id":"cjy1diwg1001m48upnul503w0"},{"post_id":"cjy1diwfy001k48upjawxi9d7","tag_id":"cjy1diwan000548upf3kw5fj3","_id":"cjy1diwgd001q48upoca1xjug"},{"post_id":"cjy1diwf3001648upcidqoanj","tag_id":"cjy1diwf8001948up2zwrfk5e","_id":"cjy1diwgg001s48uphj30a3xa"},{"post_id":"cjy1diwi7002048upmdlwgy0i","tag_id":"cjy1diwi9002248uppyvy42ks","_id":"cjy1diwif002648upbqi25knn"},{"post_id":"cjy1diwi7002048upmdlwgy0i","tag_id":"cjy1diwic002448up98wua86x","_id":"cjy1diwig002748upsl6jdwb4"},{"post_id":"cjy1diwj4002848upzw07sxip","tag_id":"cjy1diwj6002948upagbpq9af","_id":"cjy1diwjb002b48upaep0qmwz"},{"post_id":"cjy1diwk8002c48upz0se2o00","tag_id":"cjy1diwkb002d48upr7bomana","_id":"cjy1diwkg002g48up64dyiczz"},{"post_id":"cjy1diwk8002c48upz0se2o00","tag_id":"cjy1diwke002f48uplxq55c2w","_id":"cjy1diwkh002h48upf73r5vjb"}],"Tag":[{"name":"Java","_id":"cjy1diwak000348upmcbpzc06"},{"name":"开发相关","_id":"cjy1diwan000548upf3kw5fj3"},{"name":"Flume","_id":"cjy1diwar000848up6kdtsgc4"},{"name":"axios","_id":"cjy1diwdr000o48upn66qaqzp"},{"name":"CORS","_id":"cjy1diwe8000v48upc3bqacjt"},{"name":"react ES6","_id":"cjy1diwem001248upozhzkx4e"},{"name":"毕业设计","_id":"cjy1diwf8001948up2zwrfk5e"},{"name":"翻译","_id":"cjy1diwi9002248uppyvy42ks"},{"name":"大型网络扩展","_id":"cjy1diwic002448up98wua86x"},{"name":"LeetCode","_id":"cjy1diwj6002948upagbpq9af"},{"name":"Spark","_id":"cjy1diwkb002d48upr7bomana"},{"name":"RDD","_id":"cjy1diwke002f48uplxq55c2w"}]}}