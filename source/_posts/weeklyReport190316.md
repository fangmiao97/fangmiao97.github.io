---
title: 每周记录📝（190316）
categories:
    - 每周记录
---
![学校的傍晚](https://wx1.sinaimg.cn/mw1024/6a49516fly1g14vxty5rbj21400u0e82.jpg)

这一周将毕业设计的开头部分完成了链路打通，具体就是在前端埋点得到的数据能够在kafka中分topic消费。

其实这之间走了很多弯路，自己反复查资料才发现一个正确的方法。现在才意识到做项目需要一个指导老师的重要性，同样企业需要技术的布道师也是这个道理。

我的毕设是使用spark streaming进行网站流量的分析。其实对于毕设很尴尬的一点事，没有真实的业务数据，只能自己凭空制造。另外我觉得高校中的毕业设计，真的越来越水，就像是一个大一点的课程设计，自己独立完成的那种。

这一周我首先想完成通过自己在前端埋点，收集到数据，并能够给后面的业务流程使用。我的前端使用react，埋点的数据收集打算用flume收集。最开始的思路是使用flume的HTTPSource直接收集来自前端POST请求中的数据，可是通过查阅文档发现，无法完成。因为HTTPSource接受的需要是flume的Event对象，前端直接发来的数据不符合要求。所以说前端的任务只要完成原始数据收集的工作就可以了，其他必要的转换由后端完成。

还有一点就是，经别人提醒，如果说用flume直接收集网络上的数据还有安全的问题（见[V2ex帖子](https://www.v2ex.com/t/543592#reply4)）。所以有后端转发的必要。所以在这里我就有个疑惑，关于HTTPSource的使用场景到底是什么？

进过反复的尝试，确定了方案是：前端埋点采集到的数据由后端接受，并发送log4j日志给flume，再由flume的interceptor进行过滤（分流），分成不同的topic给kafka的消费者使用，之后就可以给spark streaming使用啦。(解决方案[见文章](https://fangmiao97.github.io/2019/03/16/FluemToKafkaBaseOnDifferntTopic/))

不过现在遗留下的问题就是，前端使用axios出现跨域访问的问题。这个问题下周开始解决。下周要完成的任务就是把前端的框架搭好，开始学习spark streaming深入一点的东西，然后找需求。目前的需求太单薄了，不怎么丰富。

本周还做了点其他的事，总之效率没有那么高吧。这两天刷LeetCode也刷不起来，不知道为啥。所以这两天休息了一下，没刷。

感觉自己有的时候总被一些东西打扰，自己的控制力不够。另外看书的速度也挺慢的。看了名人传记，感觉厉害的人为何有那么多精力呢？

最近看到好玩的图，取名我的毕设，哈哈哈🤣
![我的毕设](https://wx2.sinaimg.cn/mw1024/6a49516fly1g14w7kwjt8j20j60n4dks.jpg)

